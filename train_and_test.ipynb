{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_and_test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/007vasy/CNN_MNIST_FLORIAN/blob/master/train_and_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-adhRe9CyxXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CNN(object):\n",
        "    def __init__(self, patch_size, num_filters_fist_layer, num_filters_second_layer,\n",
        "                 size_fully_connected_layer, num_classes=10, image_size=784):\n",
        "        # Placeholders for input of images, labels and dropout rate\n",
        "        self.x = tf.placeholder(tf.float32, shape=[None, image_size])\n",
        "        self.y_ = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "        # creates and returns a weight variable with given shape initialized with\n",
        "        # a truncated normal distribution with stddev of 0.1\n",
        "        def weight_variable(shape, nameVar):\n",
        "            initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "            return tf.Variable(initial, name=nameVar)\n",
        "\n",
        "        # creates and returns a bias variable with given shape initialized with\n",
        "        # a constant of 0.1\n",
        "        def bias_variable(shape, nameVar):\n",
        "            initial = tf.constant(0.1, shape=shape)\n",
        "            return tf.Variable(initial, name=nameVar)\n",
        "\n",
        "        # computes a 2D convolution for the input data x and the filter W\n",
        "        # uses a stride of one and is zero padded so the output is the same size as the input\n",
        "        # input shapes:\n",
        "        # x is the input tensor - should be a 4-D tensor of shape [batch_size, in_height, in_width, in_channels]\n",
        "        # W is the filter tensor - should be a 4-D tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
        "        # strides is a 4-D tensor that defines how the filter slides over the input tensor in each of the 4 dimensions\n",
        "        # padding - if it is set to \"SAME\" it means that zero padding on every side of the input is introduced to\n",
        "        # make the shapes match if needed such that the filter is centered at all the pixels of the image according\n",
        "        # to the strides.\n",
        "        # ex. if strides=[1, 1, 1, 1] and padding='SAME' the filter is centered at every pixel from the image\n",
        "        # padding - if it is set to \"VALID\" it means that there is no padding.\n",
        "        def conv2d(x, W):\n",
        "            return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "        # performs max pooling over 2x2 blocks\n",
        "        # input shapes:\n",
        "        # x is the input tensor - should be a 4-D tensor of shape [batch_size, in_height, in_width, in_channels]\n",
        "        # ksize has the same dimensionality as the input tensor. It defines the patch size. It extracts the max\n",
        "        # value out of each such patch. Here the patch we define is a 2x2 block\n",
        "        # strides is a 4-D tensor that defines how the patch slides over the input tensor\n",
        "        # if the padding is \"SAME\" there is padding, if it is \"VALID\" there is no padding\n",
        "        # For the SAME padding, the output height and width are computed as:\n",
        "        #     out_height = ceil(float(in_height) / float(strides1))\n",
        "        #     out_width = ceil(float(in_width) / float(strides[2]))\n",
        "        # For the VALID padding, the output height and width are computed as:\n",
        "        #     out_height = ceil(float(in_height - filter_height + 1) / float(strides1))\n",
        "        #     out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))\n",
        "        # example: if x is an image of shape [2,3] and has 1 channel (so the input shape is [1, 2, 3, 1])\n",
        "        # , we max pool with 2x2 kernel and the stride is 2\n",
        "        # if the pad is VALID (valid_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID'))\n",
        "        #  the output is of shape [1, 1, 1, 1]\n",
        "        # if the pad is SAME we pad the image to the shape [2, 4];\n",
        "        # (same_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')) the output is of shape [1, 1, 2, 1]\n",
        "        def max_pool_2x2(x):\n",
        "            return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "        # Create a convolution + maxpool layer for the first layer\n",
        "\n",
        "        # define the filter\n",
        "        self.W_conv1 = weight_variable([patch_size, patch_size, 1, num_filters_fist_layer], \"filter_layer1\")\n",
        "        b_conv1 = bias_variable([num_filters_fist_layer], \"bias_layer1\")\n",
        "        # reshape the data to a 4D tensor to fit into the convolution\n",
        "        # the second and third dimensions correspond to image width and height,\n",
        "        #  and the final dimension corresponds to the number of color channels\n",
        "        # the first dimension is for the batch size; when we have -1 for one dimension when reshaping\n",
        "        # it will dynamically calculate that dimension\n",
        "        # example: if x is of shape [a, b*c, d] and we run tf.reshape([-1, b, c, d]), the first dimension will be \"a\"\n",
        "        # this is useful when the batch size varies\n",
        "        x_image = tf.reshape(self.x, [-1, 28, 28, 1])\n",
        "        # apply convolution, add the bias, apply relu, and then max pooling\n",
        "        h_conv1 = tf.nn.relu(conv2d(x_image, self.W_conv1) + b_conv1)\n",
        "        # print h_conv1.get_shape() # the shape is [-1, 28, 28, 32]\n",
        "        h_pool1 = max_pool_2x2(h_conv1)\n",
        "        # print h_pool1.get_shape() # the shape is [-1, 14, 14, 32]\n",
        "\n",
        "        # Create a densely connected layer with relu\n",
        "        W_fc1 = weight_variable([14 * 14 * 32, size_fully_connected_layer], \"W_fc1\")\n",
        "        b_fc1 = bias_variable([size_fully_connected_layer], \"b_fc1\")\n",
        "        h_pool1_flat = tf.reshape(h_pool1, [-1, 14 * 14 * 32])\n",
        "        # the shape of h_fc1 is [-1, size_fully_connected_layer]\n",
        "        h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
        "\n",
        "        # TODO: Add second conv layer here. Use the num_filters_second_layer parameter\n",
        "\n",
        "        # TODO: Add dropout here\n",
        "\n",
        "        W_fc2 = weight_variable([size_fully_connected_layer, num_classes], \"W_fc2\")\n",
        "        b_fc2 = bias_variable([num_classes], \"b_fc2\")\n",
        "\n",
        "        self.y = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
        "\n",
        "        # TODO: Add regularizer here\n",
        "\n",
        "        self.cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(self.y), reduction_indices=[1]))\n",
        "        self.correct_prediction = tf.equal(tf.argmax(self.y, 1), tf.argmax(self.y_, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q1dKybkwzHuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50051
        },
        "outputId": "605250f1-0fa0-456d-9ec0-b033364f195e"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "# from TwoLayerCNN import CNN\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ==================== Parameters =====================================\n",
        "\n",
        "# Model Hyperparameters\n",
        "tf.flags.DEFINE_integer(\"num_filters_fist_layer\", 32, \"Number of filters per filter size for first layer(default: 32)\")\n",
        "tf.flags.DEFINE_integer(\"num_filters_second_layer\", 64, \"Number of filters per filter size for 2nd layer (default: 64)\")\n",
        "tf.flags.DEFINE_integer(\"patch_size\", 5, \"Size of the filter (default: 5)\")\n",
        "tf.flags.DEFINE_float(\"size_fully_connected_layer\", 512, \"Size of the fully connected layer (default: 1024)\")\n",
        "# Training parameters\n",
        "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch Size (default: 50)\")\n",
        "# tf.flags.DEFINE_integer(\"num_epochs\", 20, \"Number of training epochs (default: 2000)\")\n",
        "tf.flags.DEFINE_integer(\"num_epochs\", 2000, \"Number of training epochs (default: 2000)\")\n",
        "tf.flags.DEFINE_integer(\"evaluate_every\", 10, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
        "tf.flags.DEFINE_integer(\"checkpoint_every\", 50, \"Save model after this many steps (default: 100)\")\n",
        "# Misc Parameters\n",
        "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
        "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
        "tf.flags.DEFINE_string(\"run_name\", None, \"Suffix for output directory. If None, a timestamp is used instead\")\n",
        "\n",
        "\n",
        "FLAGS = tf.flags.FLAGS\n",
        "# FLAGS._parse_flags() # Handling MAC -> Linux change\n",
        "FLAGS(sys.argv, known_only=True)\n",
        "print(\"\\nParameters:\")\n",
        "for attr, value in sorted(FLAGS.__flags.items()):\n",
        "    print(\"{}={}\".format(attr.upper(), value))\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "#==================== Data Loading and Preparation ===================\n",
        "\n",
        "# downloads the MNIST data if it doesn't exist\n",
        "# each image is of size 28x28, and is stored in a flattened version (size 784)\n",
        "# the label for each image is a one-hot vector (size 10)\n",
        "# the data is divided in training set (mnist.train) of size 55,000, validation set\n",
        "# (mnist.validation) of size 5,000 and test set (mnist.test) of size 10,000\n",
        "# for each set the images and labels are given (e.g. mnist.train.images of size\n",
        "# [55,000, 784] and mnist.train.labels of size [55,000, 10])\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    session_conf = tf.ConfigProto(\n",
        "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
        "      log_device_placement=FLAGS.log_device_placement)\n",
        "    # Create a session\n",
        "    sess = tf.Session(config=session_conf)\n",
        "    with sess.as_default():\n",
        "        # Build the graph\n",
        "        cnn = CNN(patch_size=FLAGS.patch_size, num_filters_fist_layer=FLAGS.num_filters_fist_layer,\n",
        "                  num_filters_second_layer=int(FLAGS.num_filters_second_layer), size_fully_connected_layer=int(FLAGS.size_fully_connected_layer))\n",
        "\n",
        "        # Define Training procedure\n",
        "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "        optimizer = tf.train.AdamOptimizer(1e-4)\n",
        "        grads_and_vars = optimizer.compute_gradients(cnn.cross_entropy)\n",
        "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
        "\n",
        "        # Keep track of gradient values and sparsity (optional)\n",
        "        grad_summaries = []\n",
        "        for g, v in grads_and_vars:\n",
        "            if g is not None:\n",
        "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
        "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
        "                grad_summaries.append(grad_hist_summary)\n",
        "                grad_summaries.append(sparsity_summary)\n",
        "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
        "\n",
        "        # Output directory for models and summaries\n",
        "        if FLAGS.run_name is None:\n",
        "          timestamp = str(int(time.time()))\n",
        "          out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
        "        else:\n",
        "          out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", FLAGS.run_name))\n",
        "        print(\"Writing to {}\\n\".format(out_dir))\n",
        "\n",
        "        # Summaries for loss and accuracy\n",
        "        loss_summary = tf.summary.scalar(\"loss\", cnn.cross_entropy)\n",
        "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
        "        # Since we have many channels, we will get filters only for one channel\n",
        "        V = tf.slice(cnn.W_conv1, (0, 0, 0, 0), (-1, -1, -1, 1))\n",
        "        # Bring into shape expected by image_summary\n",
        "        V = tf.reshape(V, (-1, 5, 5, 1))\n",
        "        image_summary_op = tf.summary.image(\"kernel_layer1\", V, 5)\n",
        "        # TODO: Add a summary for visualizing the filters from the second layer\n",
        "        # Train Summaries\n",
        "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, image_summary_op, grad_summaries_merged])\n",
        "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
        "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
        "\n",
        "        # Dev summaries\n",
        "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
        "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
        "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
        "\n",
        "        # Checkpoint directory. TF assumes this directory already exists so we need to create it\n",
        "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
        "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        # Initialize all the variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "        def train_step(x_batch, y_batch):\n",
        "            \"\"\"\n",
        "                A single training step\n",
        "                \"\"\"\n",
        "            feed_dict = {\n",
        "                cnn.x: x_batch,\n",
        "                cnn.y_: y_batch\n",
        "            }\n",
        "            _, step, summaries, loss, accuracy = sess.run(\n",
        "                [train_op, global_step, train_summary_op, cnn.cross_entropy, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            train_summary_writer.add_summary(summaries, step)\n",
        "            train_summary_writer.flush()\n",
        "\n",
        "\n",
        "        def dev_step(x_batch, y_batch, writer=None):\n",
        "            \"\"\"\n",
        "            Evaluates model on a dev set\n",
        "            \"\"\"\n",
        "            feed_dict = {\n",
        "              cnn.x: x_batch,\n",
        "              cnn.y_: y_batch\n",
        "            }\n",
        "            step, summaries, loss, accuracy = sess.run(\n",
        "                [global_step, dev_summary_op, cnn.cross_entropy, cnn.accuracy],\n",
        "                feed_dict)\n",
        "            time_str = datetime.datetime.now().isoformat()\n",
        "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
        "            if writer:\n",
        "                writer.add_summary(summaries, step)\n",
        "\n",
        "\n",
        "        for i in range(FLAGS.num_epochs):\n",
        "            batch = mnist.train.next_batch(FLAGS.batch_size)\n",
        "            train_step(batch[0], batch[1])\n",
        "            current_step = tf.train.global_step(sess, global_step)\n",
        "            if current_step % FLAGS.evaluate_every == 0:\n",
        "                print(\"\\nEvaluation:\")\n",
        "                dev_step(mnist.validation.images, mnist.validation.labels, writer=dev_summary_writer)\n",
        "                print(\"\")\n",
        "            if current_step % FLAGS.checkpoint_every == 0:\n",
        "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
        "                print(\"Saved model checkpoint to {}\\n\".format(path))\n",
        "\n",
        "        print(\"test accuracy %g\"%cnn.accuracy.eval(feed_dict={\n",
        "            cnn.x: mnist.test.images, cnn.y_: mnist.test.labels}))\n",
        "\n",
        "print(loss_summary)\n",
        "print(acc_summary)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Parameters:\n",
            "ALLOW_SOFT_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f1e543c31d0>\n",
            "BATCH_SIZE=<absl.flags._flag.Flag object at 0x7f1e5d133518>\n",
            "CHECKPOINT_EVERY=<absl.flags._flag.Flag object at 0x7f1e543c3128>\n",
            "EVALUATE_EVERY=<absl.flags._flag.Flag object at 0x7f1e5d1337b8>\n",
            "LOG_DEVICE_PLACEMENT=<absl.flags._flag.BooleanFlag object at 0x7f1e543c3278>\n",
            "NUM_EPOCHS=<absl.flags._flag.Flag object at 0x7f1e5d133748>\n",
            "NUM_FILTERS_FIST_LAYER=<absl.flags._flag.Flag object at 0x7f1e5d133160>\n",
            "NUM_FILTERS_SECOND_LAYER=<absl.flags._flag.Flag object at 0x7f1e5d133198>\n",
            "PATCH_SIZE=<absl.flags._flag.Flag object at 0x7f1e5d133128>\n",
            "RUN_NAME=<absl.flags._flag.Flag object at 0x7f1e543c32b0>\n",
            "SIZE_FULLY_CONNECTED_LAYER=<absl.flags._flag.Flag object at 0x7f1e5d1336d8>\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-2-167b2053307e>:46: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "INFO:tensorflow:Summary name filter_layer1:0/grad/hist is illegal; using filter_layer1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name filter_layer1:0/grad/sparsity is illegal; using filter_layer1_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name bias_layer1:0/grad/hist is illegal; using bias_layer1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name bias_layer1:0/grad/sparsity is illegal; using bias_layer1_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name W_fc1:0/grad/hist is illegal; using W_fc1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name W_fc1:0/grad/sparsity is illegal; using W_fc1_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name b_fc1:0/grad/hist is illegal; using b_fc1_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name b_fc1:0/grad/sparsity is illegal; using b_fc1_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name W_fc2:0/grad/hist is illegal; using W_fc2_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name W_fc2:0/grad/sparsity is illegal; using W_fc2_0/grad/sparsity instead.\n",
            "INFO:tensorflow:Summary name b_fc2:0/grad/hist is illegal; using b_fc2_0/grad/hist instead.\n",
            "INFO:tensorflow:Summary name b_fc2:0/grad/sparsity is illegal; using b_fc2_0/grad/sparsity instead.\n",
            "Writing to /content/runs/1554976753\n",
            "\n",
            "2019-04-11T09:59:14.236783: step 1, loss 2.8086, acc 0.15625\n",
            "2019-04-11T09:59:14.460886: step 2, loss 2.63001, acc 0.125\n",
            "2019-04-11T09:59:14.679570: step 3, loss 2.38067, acc 0.09375\n",
            "2019-04-11T09:59:14.898801: step 4, loss 2.35357, acc 0.1875\n",
            "2019-04-11T09:59:15.106986: step 5, loss 2.05198, acc 0.34375\n",
            "2019-04-11T09:59:15.320391: step 6, loss 2.27323, acc 0.25\n",
            "2019-04-11T09:59:15.531700: step 7, loss 2.33075, acc 0.09375\n",
            "2019-04-11T09:59:15.738075: step 8, loss 1.82522, acc 0.3125\n",
            "2019-04-11T09:59:15.950617: step 9, loss 2.03093, acc 0.15625\n",
            "2019-04-11T09:59:16.161204: step 10, loss 1.77873, acc 0.3125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:18.649643: step 10, loss 1.97043, acc 0.2766\n",
            "\n",
            "2019-04-11T09:59:18.866486: step 11, loss 1.95134, acc 0.375\n",
            "2019-04-11T09:59:19.085547: step 12, loss 1.95046, acc 0.28125\n",
            "2019-04-11T09:59:19.300949: step 13, loss 1.84365, acc 0.40625\n",
            "2019-04-11T09:59:19.508046: step 14, loss 1.74935, acc 0.375\n",
            "2019-04-11T09:59:19.714724: step 15, loss 1.76554, acc 0.46875\n",
            "2019-04-11T09:59:19.926424: step 16, loss 1.64136, acc 0.375\n",
            "2019-04-11T09:59:20.129944: step 17, loss 1.51094, acc 0.5625\n",
            "2019-04-11T09:59:20.339613: step 18, loss 1.8034, acc 0.40625\n",
            "2019-04-11T09:59:20.546726: step 19, loss 1.83544, acc 0.40625\n",
            "2019-04-11T09:59:20.759981: step 20, loss 1.64554, acc 0.53125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:23.003532: step 20, loss 1.50847, acc 0.5228\n",
            "\n",
            "2019-04-11T09:59:23.212448: step 21, loss 1.69472, acc 0.34375\n",
            "2019-04-11T09:59:23.423060: step 22, loss 1.52222, acc 0.625\n",
            "2019-04-11T09:59:23.630621: step 23, loss 1.4232, acc 0.53125\n",
            "2019-04-11T09:59:23.839479: step 24, loss 1.45152, acc 0.5625\n",
            "2019-04-11T09:59:24.054577: step 25, loss 1.25513, acc 0.53125\n",
            "2019-04-11T09:59:24.261967: step 26, loss 1.36371, acc 0.6875\n",
            "2019-04-11T09:59:24.467779: step 27, loss 1.29108, acc 0.5625\n",
            "2019-04-11T09:59:24.667665: step 28, loss 1.266, acc 0.65625\n",
            "2019-04-11T09:59:24.886478: step 29, loss 1.39649, acc 0.59375\n",
            "2019-04-11T09:59:25.094652: step 30, loss 1.50964, acc 0.5625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:27.331022: step 30, loss 1.23429, acc 0.6382\n",
            "\n",
            "2019-04-11T09:59:27.532382: step 31, loss 1.07397, acc 0.6875\n",
            "2019-04-11T09:59:27.745352: step 32, loss 1.46933, acc 0.53125\n",
            "2019-04-11T09:59:27.955045: step 33, loss 1.24818, acc 0.59375\n",
            "2019-04-11T09:59:28.158983: step 34, loss 1.1614, acc 0.65625\n",
            "2019-04-11T09:59:28.369589: step 35, loss 1.15475, acc 0.65625\n",
            "2019-04-11T09:59:28.573369: step 36, loss 1.02113, acc 0.71875\n",
            "2019-04-11T09:59:28.788648: step 37, loss 1.05022, acc 0.71875\n",
            "2019-04-11T09:59:28.988689: step 38, loss 0.936366, acc 0.78125\n",
            "2019-04-11T09:59:29.204096: step 39, loss 0.986378, acc 0.71875\n",
            "2019-04-11T09:59:29.416844: step 40, loss 1.12135, acc 0.6875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:31.719444: step 40, loss 1.02302, acc 0.7166\n",
            "\n",
            "2019-04-11T09:59:31.941018: step 41, loss 1.27797, acc 0.5625\n",
            "2019-04-11T09:59:32.155701: step 42, loss 0.98688, acc 0.625\n",
            "2019-04-11T09:59:32.366724: step 43, loss 0.977545, acc 0.78125\n",
            "2019-04-11T09:59:32.585049: step 44, loss 0.845755, acc 0.875\n",
            "2019-04-11T09:59:32.807844: step 45, loss 0.96072, acc 0.6875\n",
            "2019-04-11T09:59:33.033861: step 46, loss 0.866329, acc 0.84375\n",
            "2019-04-11T09:59:33.259687: step 47, loss 0.932148, acc 0.6875\n",
            "2019-04-11T09:59:33.479600: step 48, loss 0.947501, acc 0.78125\n",
            "2019-04-11T09:59:33.700536: step 49, loss 0.998655, acc 0.71875\n",
            "2019-04-11T09:59:33.922729: step 50, loss 0.831802, acc 0.78125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:36.242892: step 50, loss 0.907287, acc 0.7446\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-50\n",
            "\n",
            "2019-04-11T09:59:36.564330: step 51, loss 0.938663, acc 0.78125\n",
            "2019-04-11T09:59:36.775997: step 52, loss 1.03146, acc 0.59375\n",
            "2019-04-11T09:59:36.993841: step 53, loss 0.923757, acc 0.6875\n",
            "2019-04-11T09:59:37.212327: step 54, loss 0.817712, acc 0.8125\n",
            "2019-04-11T09:59:37.426100: step 55, loss 0.763538, acc 0.8125\n",
            "2019-04-11T09:59:37.644196: step 56, loss 0.882811, acc 0.78125\n",
            "2019-04-11T09:59:37.857583: step 57, loss 1.07212, acc 0.6875\n",
            "2019-04-11T09:59:38.082475: step 58, loss 0.823286, acc 0.75\n",
            "2019-04-11T09:59:38.304374: step 59, loss 0.885075, acc 0.71875\n",
            "2019-04-11T09:59:38.519161: step 60, loss 1.04579, acc 0.625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:40.828711: step 60, loss 0.831459, acc 0.7612\n",
            "\n",
            "2019-04-11T09:59:41.029562: step 61, loss 0.738657, acc 0.84375\n",
            "2019-04-11T09:59:41.246276: step 62, loss 0.914813, acc 0.75\n",
            "2019-04-11T09:59:41.448075: step 63, loss 0.739467, acc 0.8125\n",
            "2019-04-11T09:59:41.665458: step 64, loss 0.709136, acc 0.90625\n",
            "2019-04-11T09:59:41.874698: step 65, loss 0.744549, acc 0.8125\n",
            "2019-04-11T09:59:42.076140: step 66, loss 1.0728, acc 0.71875\n",
            "2019-04-11T09:59:42.286289: step 67, loss 0.750601, acc 0.8125\n",
            "2019-04-11T09:59:42.486553: step 68, loss 1.06937, acc 0.6875\n",
            "2019-04-11T09:59:42.697241: step 69, loss 0.714368, acc 0.8125\n",
            "2019-04-11T09:59:42.906822: step 70, loss 0.678875, acc 0.8125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:45.160797: step 70, loss 0.733714, acc 0.8002\n",
            "\n",
            "2019-04-11T09:59:45.372461: step 71, loss 0.651838, acc 0.8125\n",
            "2019-04-11T09:59:45.570436: step 72, loss 0.727345, acc 0.8125\n",
            "2019-04-11T09:59:45.787595: step 73, loss 0.665808, acc 0.8125\n",
            "2019-04-11T09:59:45.997265: step 74, loss 0.638103, acc 0.8125\n",
            "2019-04-11T09:59:46.200802: step 75, loss 0.961231, acc 0.71875\n",
            "2019-04-11T09:59:46.415901: step 76, loss 0.886938, acc 0.75\n",
            "2019-04-11T09:59:46.629526: step 77, loss 0.880964, acc 0.71875\n",
            "2019-04-11T09:59:46.842999: step 78, loss 0.856446, acc 0.75\n",
            "2019-04-11T09:59:47.049583: step 79, loss 0.613344, acc 0.875\n",
            "2019-04-11T09:59:47.262664: step 80, loss 0.873417, acc 0.75\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:49.501923: step 80, loss 0.688877, acc 0.8032\n",
            "\n",
            "2019-04-11T09:59:49.716547: step 81, loss 0.984862, acc 0.6875\n",
            "2019-04-11T09:59:49.911534: step 82, loss 0.815323, acc 0.75\n",
            "2019-04-11T09:59:50.125964: step 83, loss 0.731478, acc 0.84375\n",
            "2019-04-11T09:59:50.323660: step 84, loss 0.471529, acc 0.875\n",
            "2019-04-11T09:59:50.525826: step 85, loss 0.638533, acc 0.8125\n",
            "2019-04-11T09:59:50.737191: step 86, loss 0.719416, acc 0.71875\n",
            "2019-04-11T09:59:50.944564: step 87, loss 0.736052, acc 0.84375\n",
            "2019-04-11T09:59:51.151097: step 88, loss 0.855417, acc 0.71875\n",
            "2019-04-11T09:59:51.360427: step 89, loss 0.48819, acc 0.875\n",
            "2019-04-11T09:59:51.568446: step 90, loss 0.739647, acc 0.75\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:53.811427: step 90, loss 0.660878, acc 0.8138\n",
            "\n",
            "2019-04-11T09:59:54.011672: step 91, loss 0.607748, acc 0.84375\n",
            "2019-04-11T09:59:54.225236: step 92, loss 0.809824, acc 0.75\n",
            "2019-04-11T09:59:54.438036: step 93, loss 0.575696, acc 0.875\n",
            "2019-04-11T09:59:54.648498: step 94, loss 0.666917, acc 0.78125\n",
            "2019-04-11T09:59:54.861887: step 95, loss 0.504419, acc 0.96875\n",
            "2019-04-11T09:59:55.061276: step 96, loss 0.753211, acc 0.78125\n",
            "2019-04-11T09:59:55.273631: step 97, loss 0.668724, acc 0.84375\n",
            "2019-04-11T09:59:55.474375: step 98, loss 0.717038, acc 0.78125\n",
            "2019-04-11T09:59:55.683972: step 99, loss 0.414083, acc 0.9375\n",
            "2019-04-11T09:59:55.892412: step 100, loss 0.542743, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T09:59:58.130658: step 100, loss 0.610316, acc 0.8224\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-100\n",
            "\n",
            "2019-04-11T09:59:58.419411: step 101, loss 0.584349, acc 0.84375\n",
            "2019-04-11T09:59:58.629988: step 102, loss 0.621401, acc 0.78125\n",
            "2019-04-11T09:59:58.836443: step 103, loss 0.927065, acc 0.71875\n",
            "2019-04-11T09:59:59.047783: step 104, loss 0.684976, acc 0.75\n",
            "2019-04-11T09:59:59.258585: step 105, loss 0.588175, acc 0.8125\n",
            "2019-04-11T09:59:59.469019: step 106, loss 0.566118, acc 0.875\n",
            "2019-04-11T09:59:59.682116: step 107, loss 0.614132, acc 0.8125\n",
            "2019-04-11T09:59:59.890015: step 108, loss 0.93297, acc 0.75\n",
            "2019-04-11T10:00:00.097155: step 109, loss 0.809975, acc 0.84375\n",
            "2019-04-11T10:00:00.308810: step 110, loss 0.848814, acc 0.75\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:02.548608: step 110, loss 0.586189, acc 0.8426\n",
            "\n",
            "2019-04-11T10:00:02.764864: step 111, loss 0.542423, acc 0.84375\n",
            "2019-04-11T10:00:02.976834: step 112, loss 0.395887, acc 0.9375\n",
            "2019-04-11T10:00:03.190788: step 113, loss 0.923075, acc 0.65625\n",
            "2019-04-11T10:00:03.405317: step 114, loss 0.739298, acc 0.71875\n",
            "2019-04-11T10:00:03.616491: step 115, loss 0.782663, acc 0.78125\n",
            "2019-04-11T10:00:03.820989: step 116, loss 0.61037, acc 0.84375\n",
            "2019-04-11T10:00:04.031546: step 117, loss 0.745939, acc 0.75\n",
            "2019-04-11T10:00:04.229497: step 118, loss 0.629436, acc 0.90625\n",
            "2019-04-11T10:00:04.439848: step 119, loss 0.330708, acc 0.96875\n",
            "2019-04-11T10:00:04.643264: step 120, loss 0.614369, acc 0.8125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:06.890391: step 120, loss 0.534235, acc 0.8498\n",
            "\n",
            "2019-04-11T10:00:07.100958: step 121, loss 0.537811, acc 0.84375\n",
            "2019-04-11T10:00:07.304704: step 122, loss 0.686754, acc 0.8125\n",
            "2019-04-11T10:00:07.516589: step 123, loss 0.694873, acc 0.78125\n",
            "2019-04-11T10:00:07.728415: step 124, loss 0.576714, acc 0.8125\n",
            "2019-04-11T10:00:07.933535: step 125, loss 0.640547, acc 0.8125\n",
            "2019-04-11T10:00:08.137541: step 126, loss 0.465818, acc 0.84375\n",
            "2019-04-11T10:00:08.353629: step 127, loss 0.529338, acc 0.90625\n",
            "2019-04-11T10:00:08.566870: step 128, loss 0.542413, acc 0.78125\n",
            "2019-04-11T10:00:08.786515: step 129, loss 0.656745, acc 0.78125\n",
            "2019-04-11T10:00:09.000473: step 130, loss 0.704241, acc 0.71875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:11.269509: step 130, loss 0.518088, acc 0.8548\n",
            "\n",
            "2019-04-11T10:00:11.473252: step 131, loss 0.409273, acc 0.90625\n",
            "2019-04-11T10:00:11.683879: step 132, loss 0.677593, acc 0.90625\n",
            "2019-04-11T10:00:11.890083: step 133, loss 0.812724, acc 0.71875\n",
            "2019-04-11T10:00:12.095911: step 134, loss 0.41134, acc 0.90625\n",
            "2019-04-11T10:00:12.306573: step 135, loss 0.626392, acc 0.8125\n",
            "2019-04-11T10:00:12.519387: step 136, loss 0.456998, acc 0.875\n",
            "2019-04-11T10:00:12.732278: step 137, loss 0.708369, acc 0.78125\n",
            "2019-04-11T10:00:12.934096: step 138, loss 0.364202, acc 0.9375\n",
            "2019-04-11T10:00:13.147471: step 139, loss 0.426799, acc 0.8125\n",
            "2019-04-11T10:00:13.355014: step 140, loss 0.386102, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:15.596641: step 140, loss 0.497369, acc 0.8682\n",
            "\n",
            "2019-04-11T10:00:15.805913: step 141, loss 0.283462, acc 1\n",
            "2019-04-11T10:00:16.012174: step 142, loss 0.391683, acc 0.90625\n",
            "2019-04-11T10:00:16.221652: step 143, loss 0.558867, acc 0.84375\n",
            "2019-04-11T10:00:16.430649: step 144, loss 0.326443, acc 0.96875\n",
            "2019-04-11T10:00:16.642908: step 145, loss 0.48893, acc 0.9375\n",
            "2019-04-11T10:00:16.856136: step 146, loss 0.511565, acc 0.78125\n",
            "2019-04-11T10:00:17.070005: step 147, loss 0.57838, acc 0.875\n",
            "2019-04-11T10:00:17.280236: step 148, loss 0.465435, acc 0.90625\n",
            "2019-04-11T10:00:17.494431: step 149, loss 0.357279, acc 0.9375\n",
            "2019-04-11T10:00:17.702003: step 150, loss 0.503802, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:19.935348: step 150, loss 0.499569, acc 0.8514\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-150\n",
            "\n",
            "2019-04-11T10:00:20.219745: step 151, loss 0.709043, acc 0.8125\n",
            "2019-04-11T10:00:20.429636: step 152, loss 0.590434, acc 0.75\n",
            "2019-04-11T10:00:20.633645: step 153, loss 0.444485, acc 0.875\n",
            "2019-04-11T10:00:20.885024: step 154, loss 0.541422, acc 0.84375\n",
            "2019-04-11T10:00:21.098185: step 155, loss 0.765116, acc 0.8125\n",
            "2019-04-11T10:00:21.302860: step 156, loss 0.492069, acc 0.84375\n",
            "2019-04-11T10:00:21.509698: step 157, loss 0.496176, acc 0.75\n",
            "2019-04-11T10:00:21.715273: step 158, loss 0.422281, acc 0.84375\n",
            "2019-04-11T10:00:21.938595: step 159, loss 0.641728, acc 0.75\n",
            "2019-04-11T10:00:22.155442: step 160, loss 0.371629, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:24.388869: step 160, loss 0.483638, acc 0.864\n",
            "\n",
            "2019-04-11T10:00:24.599078: step 161, loss 0.755122, acc 0.8125\n",
            "2019-04-11T10:00:24.807845: step 162, loss 0.519837, acc 0.84375\n",
            "2019-04-11T10:00:25.027485: step 163, loss 0.520314, acc 0.8125\n",
            "2019-04-11T10:00:25.238945: step 164, loss 0.473356, acc 0.875\n",
            "2019-04-11T10:00:25.445696: step 165, loss 0.469159, acc 0.90625\n",
            "2019-04-11T10:00:25.658816: step 166, loss 0.495505, acc 0.875\n",
            "2019-04-11T10:00:25.872037: step 167, loss 0.441705, acc 0.84375\n",
            "2019-04-11T10:00:26.079463: step 168, loss 0.575173, acc 0.8125\n",
            "2019-04-11T10:00:26.288268: step 169, loss 0.464478, acc 0.875\n",
            "2019-04-11T10:00:26.501607: step 170, loss 0.375582, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:28.746495: step 170, loss 0.471538, acc 0.8612\n",
            "\n",
            "2019-04-11T10:00:28.959116: step 171, loss 0.619478, acc 0.8125\n",
            "2019-04-11T10:00:29.167020: step 172, loss 0.270236, acc 0.9375\n",
            "2019-04-11T10:00:29.380350: step 173, loss 0.782154, acc 0.8125\n",
            "2019-04-11T10:00:29.594083: step 174, loss 0.33303, acc 0.9375\n",
            "2019-04-11T10:00:29.806623: step 175, loss 0.419126, acc 0.875\n",
            "2019-04-11T10:00:30.015306: step 176, loss 0.491852, acc 0.9375\n",
            "2019-04-11T10:00:30.223366: step 177, loss 0.381147, acc 0.9375\n",
            "2019-04-11T10:00:30.421438: step 178, loss 0.494372, acc 0.875\n",
            "2019-04-11T10:00:30.641229: step 179, loss 0.413776, acc 0.84375\n",
            "2019-04-11T10:00:30.857743: step 180, loss 0.656988, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:33.098721: step 180, loss 0.458871, acc 0.8702\n",
            "\n",
            "2019-04-11T10:00:33.303014: step 181, loss 0.258819, acc 0.9375\n",
            "2019-04-11T10:00:33.518791: step 182, loss 0.343867, acc 0.90625\n",
            "2019-04-11T10:00:33.729039: step 183, loss 0.391749, acc 0.90625\n",
            "2019-04-11T10:00:33.940160: step 184, loss 0.527286, acc 0.71875\n",
            "2019-04-11T10:00:34.149041: step 185, loss 0.587507, acc 0.71875\n",
            "2019-04-11T10:00:34.357430: step 186, loss 0.477698, acc 0.875\n",
            "2019-04-11T10:00:34.571348: step 187, loss 0.533566, acc 0.875\n",
            "2019-04-11T10:00:34.787040: step 188, loss 0.297231, acc 0.875\n",
            "2019-04-11T10:00:34.999432: step 189, loss 0.534753, acc 0.90625\n",
            "2019-04-11T10:00:35.209446: step 190, loss 0.378812, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:37.446486: step 190, loss 0.42965, acc 0.8742\n",
            "\n",
            "2019-04-11T10:00:37.656377: step 191, loss 0.396188, acc 0.875\n",
            "2019-04-11T10:00:37.869656: step 192, loss 0.658953, acc 0.78125\n",
            "2019-04-11T10:00:38.077958: step 193, loss 0.665368, acc 0.71875\n",
            "2019-04-11T10:00:38.286371: step 194, loss 0.603526, acc 0.8125\n",
            "2019-04-11T10:00:38.494636: step 195, loss 0.369761, acc 0.84375\n",
            "2019-04-11T10:00:38.709717: step 196, loss 0.408429, acc 0.875\n",
            "2019-04-11T10:00:38.921467: step 197, loss 0.440324, acc 0.84375\n",
            "2019-04-11T10:00:39.128466: step 198, loss 0.419498, acc 0.875\n",
            "2019-04-11T10:00:39.338963: step 199, loss 0.539759, acc 0.84375\n",
            "2019-04-11T10:00:39.565072: step 200, loss 0.343487, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:41.812630: step 200, loss 0.423631, acc 0.8852\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-200\n",
            "\n",
            "2019-04-11T10:00:42.101723: step 201, loss 0.501648, acc 0.875\n",
            "2019-04-11T10:00:42.312227: step 202, loss 0.388971, acc 0.9375\n",
            "2019-04-11T10:00:42.525869: step 203, loss 0.428514, acc 0.84375\n",
            "2019-04-11T10:00:42.739370: step 204, loss 0.509434, acc 0.8125\n",
            "2019-04-11T10:00:42.954050: step 205, loss 0.497834, acc 0.8125\n",
            "2019-04-11T10:00:43.167162: step 206, loss 0.432093, acc 0.9375\n",
            "2019-04-11T10:00:43.382101: step 207, loss 0.399692, acc 0.875\n",
            "2019-04-11T10:00:43.586109: step 208, loss 0.499535, acc 0.90625\n",
            "2019-04-11T10:00:43.796968: step 209, loss 0.276481, acc 0.90625\n",
            "2019-04-11T10:00:44.015152: step 210, loss 0.325411, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:46.249679: step 210, loss 0.408929, acc 0.8856\n",
            "\n",
            "2019-04-11T10:00:46.457331: step 211, loss 0.283004, acc 0.96875\n",
            "2019-04-11T10:00:46.669910: step 212, loss 0.441898, acc 0.84375\n",
            "2019-04-11T10:00:46.887925: step 213, loss 0.256299, acc 0.9375\n",
            "2019-04-11T10:00:47.097264: step 214, loss 0.388723, acc 0.84375\n",
            "2019-04-11T10:00:47.308745: step 215, loss 0.706164, acc 0.8125\n",
            "2019-04-11T10:00:47.519694: step 216, loss 0.674158, acc 0.8125\n",
            "2019-04-11T10:00:47.732669: step 217, loss 0.425269, acc 0.90625\n",
            "2019-04-11T10:00:47.942527: step 218, loss 0.481912, acc 0.9375\n",
            "2019-04-11T10:00:48.144906: step 219, loss 0.57555, acc 0.75\n",
            "2019-04-11T10:00:48.362035: step 220, loss 0.469594, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:50.597589: step 220, loss 0.394577, acc 0.8934\n",
            "\n",
            "2019-04-11T10:00:50.806126: step 221, loss 0.39851, acc 0.875\n",
            "2019-04-11T10:00:51.016970: step 222, loss 0.450245, acc 0.8125\n",
            "2019-04-11T10:00:51.222452: step 223, loss 0.569344, acc 0.84375\n",
            "2019-04-11T10:00:51.439590: step 224, loss 0.327144, acc 0.90625\n",
            "2019-04-11T10:00:51.655153: step 225, loss 0.185757, acc 0.96875\n",
            "2019-04-11T10:00:51.869534: step 226, loss 0.311146, acc 0.9375\n",
            "2019-04-11T10:00:52.090026: step 227, loss 0.261976, acc 0.96875\n",
            "2019-04-11T10:00:52.304685: step 228, loss 0.365116, acc 0.875\n",
            "2019-04-11T10:00:52.516443: step 229, loss 0.793584, acc 0.8125\n",
            "2019-04-11T10:00:52.729974: step 230, loss 0.356305, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:54.969374: step 230, loss 0.370557, acc 0.9016\n",
            "\n",
            "2019-04-11T10:00:55.176330: step 231, loss 0.450673, acc 0.84375\n",
            "2019-04-11T10:00:55.389948: step 232, loss 0.325959, acc 0.90625\n",
            "2019-04-11T10:00:55.602325: step 233, loss 0.671911, acc 0.75\n",
            "2019-04-11T10:00:55.814502: step 234, loss 0.475266, acc 0.84375\n",
            "2019-04-11T10:00:56.027221: step 235, loss 0.475222, acc 0.8125\n",
            "2019-04-11T10:00:56.239200: step 236, loss 0.336275, acc 0.90625\n",
            "2019-04-11T10:00:56.450254: step 237, loss 0.346182, acc 0.90625\n",
            "2019-04-11T10:00:56.673565: step 238, loss 0.367454, acc 0.90625\n",
            "2019-04-11T10:00:56.892350: step 239, loss 0.602394, acc 0.8125\n",
            "2019-04-11T10:00:57.103906: step 240, loss 0.585503, acc 0.78125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:00:59.341438: step 240, loss 0.370595, acc 0.9022\n",
            "\n",
            "2019-04-11T10:00:59.544148: step 241, loss 0.362564, acc 0.875\n",
            "2019-04-11T10:00:59.764434: step 242, loss 0.225061, acc 0.96875\n",
            "2019-04-11T10:00:59.976941: step 243, loss 0.32485, acc 0.90625\n",
            "2019-04-11T10:01:00.188228: step 244, loss 0.475501, acc 0.78125\n",
            "2019-04-11T10:01:00.399532: step 245, loss 0.319174, acc 0.875\n",
            "2019-04-11T10:01:00.614111: step 246, loss 0.310085, acc 0.9375\n",
            "2019-04-11T10:01:00.826585: step 247, loss 0.447518, acc 0.875\n",
            "2019-04-11T10:01:01.046416: step 248, loss 0.365474, acc 0.875\n",
            "2019-04-11T10:01:01.259594: step 249, loss 0.282883, acc 0.96875\n",
            "2019-04-11T10:01:01.473865: step 250, loss 0.521271, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:03.724850: step 250, loss 0.377763, acc 0.8984\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-250\n",
            "\n",
            "2019-04-11T10:01:04.014081: step 251, loss 0.463178, acc 0.84375\n",
            "2019-04-11T10:01:04.221811: step 252, loss 0.607818, acc 0.84375\n",
            "2019-04-11T10:01:04.433090: step 253, loss 0.290756, acc 0.875\n",
            "2019-04-11T10:01:04.649865: step 254, loss 0.611713, acc 0.78125\n",
            "2019-04-11T10:01:04.856256: step 255, loss 0.448369, acc 0.84375\n",
            "2019-04-11T10:01:05.071994: step 256, loss 0.530626, acc 0.84375\n",
            "2019-04-11T10:01:05.283491: step 257, loss 0.27555, acc 0.9375\n",
            "2019-04-11T10:01:05.494959: step 258, loss 0.323498, acc 0.9375\n",
            "2019-04-11T10:01:05.700007: step 259, loss 0.52697, acc 0.84375\n",
            "2019-04-11T10:01:05.907919: step 260, loss 0.331317, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:08.150403: step 260, loss 0.369283, acc 0.8964\n",
            "\n",
            "2019-04-11T10:01:08.356742: step 261, loss 0.515956, acc 0.875\n",
            "2019-04-11T10:01:08.562357: step 262, loss 0.344742, acc 0.875\n",
            "2019-04-11T10:01:08.775712: step 263, loss 0.390745, acc 0.84375\n",
            "2019-04-11T10:01:08.987526: step 264, loss 0.397323, acc 0.875\n",
            "2019-04-11T10:01:09.201038: step 265, loss 0.240104, acc 0.875\n",
            "2019-04-11T10:01:09.410443: step 266, loss 0.327478, acc 0.9375\n",
            "2019-04-11T10:01:09.618808: step 267, loss 0.397699, acc 0.84375\n",
            "2019-04-11T10:01:09.821732: step 268, loss 0.602475, acc 0.84375\n",
            "2019-04-11T10:01:10.037717: step 269, loss 0.403382, acc 0.84375\n",
            "2019-04-11T10:01:10.252859: step 270, loss 0.227301, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:12.522353: step 270, loss 0.345718, acc 0.905\n",
            "\n",
            "2019-04-11T10:01:12.738365: step 271, loss 0.580714, acc 0.8125\n",
            "2019-04-11T10:01:12.951411: step 272, loss 0.411299, acc 0.78125\n",
            "2019-04-11T10:01:13.166830: step 273, loss 0.346922, acc 0.90625\n",
            "2019-04-11T10:01:13.378555: step 274, loss 0.50371, acc 0.90625\n",
            "2019-04-11T10:01:13.590185: step 275, loss 0.378398, acc 0.90625\n",
            "2019-04-11T10:01:13.805218: step 276, loss 0.440047, acc 0.84375\n",
            "2019-04-11T10:01:14.023502: step 277, loss 0.352188, acc 0.875\n",
            "2019-04-11T10:01:14.235128: step 278, loss 0.337091, acc 0.96875\n",
            "2019-04-11T10:01:14.443808: step 279, loss 0.351445, acc 0.90625\n",
            "2019-04-11T10:01:14.654525: step 280, loss 0.466051, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:16.902524: step 280, loss 0.336328, acc 0.9078\n",
            "\n",
            "2019-04-11T10:01:17.111669: step 281, loss 0.458804, acc 0.90625\n",
            "2019-04-11T10:01:17.325772: step 282, loss 0.420502, acc 0.875\n",
            "2019-04-11T10:01:17.538461: step 283, loss 0.373069, acc 0.84375\n",
            "2019-04-11T10:01:17.748479: step 284, loss 0.707766, acc 0.78125\n",
            "2019-04-11T10:01:17.964616: step 285, loss 0.280221, acc 0.9375\n",
            "2019-04-11T10:01:18.172566: step 286, loss 0.362565, acc 0.875\n",
            "2019-04-11T10:01:18.386006: step 287, loss 0.556768, acc 0.84375\n",
            "2019-04-11T10:01:18.590050: step 288, loss 0.356646, acc 0.875\n",
            "2019-04-11T10:01:18.807738: step 289, loss 0.317952, acc 0.875\n",
            "2019-04-11T10:01:19.023675: step 290, loss 0.347603, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:21.256094: step 290, loss 0.339548, acc 0.9058\n",
            "\n",
            "2019-04-11T10:01:21.471618: step 291, loss 0.457308, acc 0.875\n",
            "2019-04-11T10:01:21.688351: step 292, loss 0.282592, acc 0.90625\n",
            "2019-04-11T10:01:21.901871: step 293, loss 0.494003, acc 0.8125\n",
            "2019-04-11T10:01:22.128924: step 294, loss 0.196791, acc 1\n",
            "2019-04-11T10:01:22.333455: step 295, loss 0.25283, acc 0.90625\n",
            "2019-04-11T10:01:22.549076: step 296, loss 0.212513, acc 0.96875\n",
            "2019-04-11T10:01:22.762974: step 297, loss 0.375118, acc 0.9375\n",
            "2019-04-11T10:01:22.965843: step 298, loss 0.314159, acc 0.84375\n",
            "2019-04-11T10:01:23.178898: step 299, loss 0.271847, acc 0.9375\n",
            "2019-04-11T10:01:23.389696: step 300, loss 0.341142, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:25.634314: step 300, loss 0.32664, acc 0.9102\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-300\n",
            "\n",
            "2019-04-11T10:01:25.935262: step 301, loss 0.254143, acc 0.9375\n",
            "2019-04-11T10:01:26.154085: step 302, loss 0.258658, acc 0.90625\n",
            "2019-04-11T10:01:26.365440: step 303, loss 0.37342, acc 0.90625\n",
            "2019-04-11T10:01:26.587685: step 304, loss 0.200697, acc 0.90625\n",
            "2019-04-11T10:01:26.800771: step 305, loss 0.424136, acc 0.84375\n",
            "2019-04-11T10:01:27.018512: step 306, loss 0.427059, acc 0.9375\n",
            "2019-04-11T10:01:27.232780: step 307, loss 0.219247, acc 0.9375\n",
            "2019-04-11T10:01:27.442062: step 308, loss 0.378159, acc 0.90625\n",
            "2019-04-11T10:01:27.659017: step 309, loss 0.343095, acc 0.875\n",
            "2019-04-11T10:01:27.873289: step 310, loss 0.260657, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:30.108259: step 310, loss 0.370473, acc 0.891\n",
            "\n",
            "2019-04-11T10:01:30.322014: step 311, loss 0.258141, acc 0.90625\n",
            "2019-04-11T10:01:30.531108: step 312, loss 0.420585, acc 0.90625\n",
            "2019-04-11T10:01:30.748906: step 313, loss 0.381204, acc 0.875\n",
            "2019-04-11T10:01:30.962990: step 314, loss 0.416441, acc 0.875\n",
            "2019-04-11T10:01:31.179512: step 315, loss 0.366256, acc 0.90625\n",
            "2019-04-11T10:01:31.389665: step 316, loss 0.259162, acc 0.90625\n",
            "2019-04-11T10:01:31.595729: step 317, loss 0.277573, acc 0.96875\n",
            "2019-04-11T10:01:31.811353: step 318, loss 0.319741, acc 0.875\n",
            "2019-04-11T10:01:32.025262: step 319, loss 0.119714, acc 0.96875\n",
            "2019-04-11T10:01:32.233636: step 320, loss 0.408484, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:34.468727: step 320, loss 0.329377, acc 0.9112\n",
            "\n",
            "2019-04-11T10:01:34.681783: step 321, loss 0.384882, acc 0.90625\n",
            "2019-04-11T10:01:34.898133: step 322, loss 0.277181, acc 0.9375\n",
            "2019-04-11T10:01:35.109478: step 323, loss 0.350316, acc 0.84375\n",
            "2019-04-11T10:01:35.321537: step 324, loss 0.427973, acc 0.84375\n",
            "2019-04-11T10:01:35.532809: step 325, loss 0.329233, acc 0.9375\n",
            "2019-04-11T10:01:35.737971: step 326, loss 0.379637, acc 0.84375\n",
            "2019-04-11T10:01:35.956110: step 327, loss 0.351442, acc 0.875\n",
            "2019-04-11T10:01:36.173912: step 328, loss 0.211259, acc 0.9375\n",
            "2019-04-11T10:01:36.386274: step 329, loss 0.354616, acc 0.90625\n",
            "2019-04-11T10:01:36.602698: step 330, loss 0.265908, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:38.842643: step 330, loss 0.314338, acc 0.9128\n",
            "\n",
            "2019-04-11T10:01:39.055564: step 331, loss 0.322859, acc 0.90625\n",
            "2019-04-11T10:01:39.266551: step 332, loss 0.171084, acc 0.9375\n",
            "2019-04-11T10:01:39.480080: step 333, loss 0.205362, acc 0.9375\n",
            "2019-04-11T10:01:39.691976: step 334, loss 0.101578, acc 0.96875\n",
            "2019-04-11T10:01:39.902420: step 335, loss 0.490416, acc 0.875\n",
            "2019-04-11T10:01:40.112523: step 336, loss 0.356736, acc 0.875\n",
            "2019-04-11T10:01:40.328972: step 337, loss 0.196009, acc 0.96875\n",
            "2019-04-11T10:01:40.539205: step 338, loss 0.388909, acc 0.875\n",
            "2019-04-11T10:01:40.755625: step 339, loss 0.479613, acc 0.875\n",
            "2019-04-11T10:01:40.971668: step 340, loss 0.281876, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:43.206238: step 340, loss 0.329758, acc 0.9054\n",
            "\n",
            "2019-04-11T10:01:43.417926: step 341, loss 0.474258, acc 0.90625\n",
            "2019-04-11T10:01:43.629868: step 342, loss 0.271595, acc 0.875\n",
            "2019-04-11T10:01:43.842686: step 343, loss 0.55364, acc 0.78125\n",
            "2019-04-11T10:01:44.049447: step 344, loss 0.285552, acc 0.90625\n",
            "2019-04-11T10:01:44.268672: step 345, loss 0.259225, acc 0.90625\n",
            "2019-04-11T10:01:44.484902: step 346, loss 0.39099, acc 0.875\n",
            "2019-04-11T10:01:44.699274: step 347, loss 0.396516, acc 0.84375\n",
            "2019-04-11T10:01:44.918624: step 348, loss 0.233634, acc 0.90625\n",
            "2019-04-11T10:01:45.135096: step 349, loss 0.300621, acc 0.90625\n",
            "2019-04-11T10:01:45.351313: step 350, loss 0.441919, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:47.589418: step 350, loss 0.335318, acc 0.907\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-350\n",
            "\n",
            "2019-04-11T10:01:47.926525: step 351, loss 0.352552, acc 0.9375\n",
            "2019-04-11T10:01:48.138480: step 352, loss 0.39415, acc 0.875\n",
            "2019-04-11T10:01:48.354647: step 353, loss 0.391474, acc 0.875\n",
            "2019-04-11T10:01:48.573092: step 354, loss 0.308777, acc 0.90625\n",
            "2019-04-11T10:01:48.786839: step 355, loss 0.301852, acc 0.9375\n",
            "2019-04-11T10:01:48.996161: step 356, loss 0.285355, acc 0.9375\n",
            "2019-04-11T10:01:49.201956: step 357, loss 0.315364, acc 0.9375\n",
            "2019-04-11T10:01:49.427556: step 358, loss 0.253406, acc 0.90625\n",
            "2019-04-11T10:01:49.640063: step 359, loss 0.283794, acc 0.90625\n",
            "2019-04-11T10:01:49.853882: step 360, loss 0.432105, acc 0.8125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:52.091049: step 360, loss 0.303146, acc 0.9166\n",
            "\n",
            "2019-04-11T10:01:52.300500: step 361, loss 0.478232, acc 0.84375\n",
            "2019-04-11T10:01:52.513817: step 362, loss 0.263094, acc 0.90625\n",
            "2019-04-11T10:01:52.727323: step 363, loss 0.481052, acc 0.84375\n",
            "2019-04-11T10:01:52.946345: step 364, loss 0.222256, acc 0.96875\n",
            "2019-04-11T10:01:53.160802: step 365, loss 0.308816, acc 0.90625\n",
            "2019-04-11T10:01:53.374718: step 366, loss 0.336445, acc 0.90625\n",
            "2019-04-11T10:01:53.591437: step 367, loss 0.429722, acc 0.90625\n",
            "2019-04-11T10:01:53.808427: step 368, loss 0.43904, acc 0.90625\n",
            "2019-04-11T10:01:54.021424: step 369, loss 0.283345, acc 0.90625\n",
            "2019-04-11T10:01:54.234107: step 370, loss 0.3153, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:01:56.471174: step 370, loss 0.300429, acc 0.9178\n",
            "\n",
            "2019-04-11T10:01:56.684112: step 371, loss 0.404123, acc 0.9375\n",
            "2019-04-11T10:01:56.895133: step 372, loss 0.299661, acc 0.875\n",
            "2019-04-11T10:01:57.105791: step 373, loss 0.213255, acc 0.90625\n",
            "2019-04-11T10:01:57.326379: step 374, loss 0.175173, acc 0.9375\n",
            "2019-04-11T10:01:57.535406: step 375, loss 0.446698, acc 0.84375\n",
            "2019-04-11T10:01:57.751005: step 376, loss 0.312476, acc 0.9375\n",
            "2019-04-11T10:01:57.968428: step 377, loss 0.345109, acc 0.90625\n",
            "2019-04-11T10:01:58.180160: step 378, loss 0.169998, acc 1\n",
            "2019-04-11T10:01:58.393000: step 379, loss 0.399852, acc 0.8125\n",
            "2019-04-11T10:01:58.598604: step 380, loss 0.232476, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:00.839270: step 380, loss 0.320243, acc 0.9106\n",
            "\n",
            "2019-04-11T10:02:01.055419: step 381, loss 0.171858, acc 0.9375\n",
            "2019-04-11T10:02:01.264975: step 382, loss 0.306182, acc 0.90625\n",
            "2019-04-11T10:02:01.479093: step 383, loss 0.262556, acc 0.96875\n",
            "2019-04-11T10:02:01.695515: step 384, loss 0.228596, acc 0.9375\n",
            "2019-04-11T10:02:01.908324: step 385, loss 0.77783, acc 0.84375\n",
            "2019-04-11T10:02:02.125452: step 386, loss 0.203157, acc 0.9375\n",
            "2019-04-11T10:02:02.338226: step 387, loss 0.27459, acc 0.90625\n",
            "2019-04-11T10:02:02.556546: step 388, loss 0.207452, acc 0.90625\n",
            "2019-04-11T10:02:02.774836: step 389, loss 0.199821, acc 0.96875\n",
            "2019-04-11T10:02:02.991595: step 390, loss 0.233344, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:05.240031: step 390, loss 0.327276, acc 0.9074\n",
            "\n",
            "2019-04-11T10:02:05.453537: step 391, loss 0.3539, acc 0.875\n",
            "2019-04-11T10:02:05.669366: step 392, loss 0.429898, acc 0.875\n",
            "2019-04-11T10:02:05.882607: step 393, loss 0.712202, acc 0.8125\n",
            "2019-04-11T10:02:06.100036: step 394, loss 0.472055, acc 0.84375\n",
            "2019-04-11T10:02:06.314003: step 395, loss 0.352313, acc 0.875\n",
            "2019-04-11T10:02:06.531614: step 396, loss 0.283696, acc 0.84375\n",
            "2019-04-11T10:02:06.738493: step 397, loss 0.316991, acc 0.875\n",
            "2019-04-11T10:02:06.953940: step 398, loss 0.374621, acc 0.90625\n",
            "2019-04-11T10:02:07.168290: step 399, loss 0.301378, acc 0.9375\n",
            "2019-04-11T10:02:07.384703: step 400, loss 0.24922, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:09.621251: step 400, loss 0.296899, acc 0.9152\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-400\n",
            "\n",
            "2019-04-11T10:02:09.929994: step 401, loss 0.321721, acc 0.90625\n",
            "2019-04-11T10:02:10.133248: step 402, loss 0.419975, acc 0.8125\n",
            "2019-04-11T10:02:10.349071: step 403, loss 0.388929, acc 0.90625\n",
            "2019-04-11T10:02:10.558339: step 404, loss 0.2376, acc 0.9375\n",
            "2019-04-11T10:02:10.775222: step 405, loss 0.563847, acc 0.8125\n",
            "2019-04-11T10:02:10.993945: step 406, loss 0.478514, acc 0.84375\n",
            "2019-04-11T10:02:11.207690: step 407, loss 0.256831, acc 0.9375\n",
            "2019-04-11T10:02:11.421487: step 408, loss 0.141615, acc 1\n",
            "2019-04-11T10:02:11.645332: step 409, loss 0.36234, acc 0.90625\n",
            "2019-04-11T10:02:11.861547: step 410, loss 0.14885, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:14.098632: step 410, loss 0.307969, acc 0.9106\n",
            "\n",
            "2019-04-11T10:02:14.313154: step 411, loss 0.236549, acc 0.90625\n",
            "2019-04-11T10:02:14.513719: step 412, loss 0.216084, acc 0.96875\n",
            "2019-04-11T10:02:14.736472: step 413, loss 0.225351, acc 0.9375\n",
            "2019-04-11T10:02:14.954555: step 414, loss 0.208235, acc 0.9375\n",
            "2019-04-11T10:02:15.173872: step 415, loss 0.410508, acc 0.84375\n",
            "2019-04-11T10:02:15.390010: step 416, loss 0.284468, acc 0.90625\n",
            "2019-04-11T10:02:15.601124: step 417, loss 0.343778, acc 0.90625\n",
            "2019-04-11T10:02:15.813000: step 418, loss 0.431466, acc 0.90625\n",
            "2019-04-11T10:02:16.025423: step 419, loss 0.518358, acc 0.8125\n",
            "2019-04-11T10:02:16.237924: step 420, loss 0.319932, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:18.476949: step 420, loss 0.285325, acc 0.9168\n",
            "\n",
            "2019-04-11T10:02:18.682979: step 421, loss 0.299298, acc 0.90625\n",
            "2019-04-11T10:02:18.904723: step 422, loss 0.375502, acc 0.84375\n",
            "2019-04-11T10:02:19.123220: step 423, loss 0.0955743, acc 1\n",
            "2019-04-11T10:02:19.330646: step 424, loss 0.1468, acc 0.96875\n",
            "2019-04-11T10:02:19.537128: step 425, loss 0.196106, acc 0.96875\n",
            "2019-04-11T10:02:19.748708: step 426, loss 0.446618, acc 0.9375\n",
            "2019-04-11T10:02:19.958594: step 427, loss 0.194205, acc 0.90625\n",
            "2019-04-11T10:02:20.163127: step 428, loss 0.321705, acc 0.90625\n",
            "2019-04-11T10:02:20.384435: step 429, loss 0.184805, acc 0.90625\n",
            "2019-04-11T10:02:20.600368: step 430, loss 0.364329, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:22.841647: step 430, loss 0.294671, acc 0.9182\n",
            "\n",
            "2019-04-11T10:02:23.060125: step 431, loss 0.370432, acc 0.9375\n",
            "2019-04-11T10:02:23.276351: step 432, loss 0.247196, acc 0.90625\n",
            "2019-04-11T10:02:23.492574: step 433, loss 0.422322, acc 0.875\n",
            "2019-04-11T10:02:23.726835: step 434, loss 0.19598, acc 1\n",
            "2019-04-11T10:02:23.938801: step 435, loss 0.316099, acc 0.875\n",
            "2019-04-11T10:02:24.152744: step 436, loss 0.538686, acc 0.8125\n",
            "2019-04-11T10:02:24.365015: step 437, loss 0.406403, acc 0.875\n",
            "2019-04-11T10:02:24.581114: step 438, loss 0.131383, acc 0.96875\n",
            "2019-04-11T10:02:24.795421: step 439, loss 0.443668, acc 0.84375\n",
            "2019-04-11T10:02:25.013922: step 440, loss 0.181708, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:27.255435: step 440, loss 0.302881, acc 0.9098\n",
            "\n",
            "2019-04-11T10:02:27.470379: step 441, loss 0.363657, acc 0.90625\n",
            "2019-04-11T10:02:27.684336: step 442, loss 0.289403, acc 0.90625\n",
            "2019-04-11T10:02:27.899050: step 443, loss 0.621899, acc 0.84375\n",
            "2019-04-11T10:02:28.108838: step 444, loss 0.14298, acc 0.96875\n",
            "2019-04-11T10:02:28.323314: step 445, loss 0.109743, acc 1\n",
            "2019-04-11T10:02:28.537841: step 446, loss 0.280168, acc 0.90625\n",
            "2019-04-11T10:02:28.755660: step 447, loss 0.186092, acc 0.96875\n",
            "2019-04-11T10:02:28.971120: step 448, loss 0.413047, acc 0.90625\n",
            "2019-04-11T10:02:29.192129: step 449, loss 0.823326, acc 0.78125\n",
            "2019-04-11T10:02:29.404391: step 450, loss 0.140365, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:31.648372: step 450, loss 0.280333, acc 0.921\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-450\n",
            "\n",
            "2019-04-11T10:02:31.956042: step 451, loss 0.322992, acc 0.90625\n",
            "2019-04-11T10:02:32.171819: step 452, loss 0.295222, acc 0.90625\n",
            "2019-04-11T10:02:32.394492: step 453, loss 0.12817, acc 0.96875\n",
            "2019-04-11T10:02:32.613625: step 454, loss 0.315682, acc 0.90625\n",
            "2019-04-11T10:02:32.829960: step 455, loss 0.191679, acc 0.96875\n",
            "2019-04-11T10:02:33.049740: step 456, loss 0.234374, acc 0.9375\n",
            "2019-04-11T10:02:33.261305: step 457, loss 0.464681, acc 0.90625\n",
            "2019-04-11T10:02:33.476822: step 458, loss 0.406494, acc 0.875\n",
            "2019-04-11T10:02:33.688385: step 459, loss 0.419245, acc 0.875\n",
            "2019-04-11T10:02:33.903077: step 460, loss 0.555663, acc 0.8125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:36.145121: step 460, loss 0.29262, acc 0.916\n",
            "\n",
            "2019-04-11T10:02:36.363294: step 461, loss 0.147003, acc 0.96875\n",
            "2019-04-11T10:02:36.578448: step 462, loss 0.14328, acc 1\n",
            "2019-04-11T10:02:36.795005: step 463, loss 0.215835, acc 1\n",
            "2019-04-11T10:02:37.010980: step 464, loss 0.297229, acc 0.90625\n",
            "2019-04-11T10:02:37.227098: step 465, loss 0.14326, acc 0.96875\n",
            "2019-04-11T10:02:37.442455: step 466, loss 0.267353, acc 0.90625\n",
            "2019-04-11T10:02:37.657088: step 467, loss 0.298365, acc 0.9375\n",
            "2019-04-11T10:02:37.869313: step 468, loss 0.247117, acc 0.9375\n",
            "2019-04-11T10:02:38.080734: step 469, loss 0.216805, acc 0.9375\n",
            "2019-04-11T10:02:38.296082: step 470, loss 0.361029, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:40.543376: step 470, loss 0.27316, acc 0.9216\n",
            "\n",
            "2019-04-11T10:02:40.757886: step 471, loss 0.233697, acc 0.9375\n",
            "2019-04-11T10:02:40.978503: step 472, loss 0.344898, acc 0.875\n",
            "2019-04-11T10:02:41.185444: step 473, loss 0.329809, acc 0.9375\n",
            "2019-04-11T10:02:41.403611: step 474, loss 0.627838, acc 0.875\n",
            "2019-04-11T10:02:41.617107: step 475, loss 0.247333, acc 0.90625\n",
            "2019-04-11T10:02:41.825158: step 476, loss 0.136901, acc 1\n",
            "2019-04-11T10:02:42.042279: step 477, loss 0.290399, acc 0.9375\n",
            "2019-04-11T10:02:42.256505: step 478, loss 0.461821, acc 0.8125\n",
            "2019-04-11T10:02:42.475125: step 479, loss 0.174795, acc 0.96875\n",
            "2019-04-11T10:02:42.697811: step 480, loss 0.0894735, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:44.937505: step 480, loss 0.270678, acc 0.9254\n",
            "\n",
            "2019-04-11T10:02:45.153070: step 481, loss 0.191342, acc 0.9375\n",
            "2019-04-11T10:02:45.354384: step 482, loss 0.7714, acc 0.8125\n",
            "2019-04-11T10:02:45.572240: step 483, loss 0.277934, acc 0.9375\n",
            "2019-04-11T10:02:45.786309: step 484, loss 0.607353, acc 0.84375\n",
            "2019-04-11T10:02:45.992804: step 485, loss 0.2763, acc 0.90625\n",
            "2019-04-11T10:02:46.211103: step 486, loss 0.114656, acc 0.96875\n",
            "2019-04-11T10:02:46.425329: step 487, loss 0.291037, acc 0.875\n",
            "2019-04-11T10:02:46.634460: step 488, loss 0.173269, acc 0.9375\n",
            "2019-04-11T10:02:46.845789: step 489, loss 0.33461, acc 0.9375\n",
            "2019-04-11T10:02:47.056221: step 490, loss 0.291699, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:49.290264: step 490, loss 0.282992, acc 0.9164\n",
            "\n",
            "2019-04-11T10:02:49.503070: step 491, loss 0.354308, acc 0.90625\n",
            "2019-04-11T10:02:49.720099: step 492, loss 0.242552, acc 0.90625\n",
            "2019-04-11T10:02:49.924903: step 493, loss 0.673041, acc 0.84375\n",
            "2019-04-11T10:02:50.135528: step 494, loss 0.244319, acc 0.9375\n",
            "2019-04-11T10:02:50.348326: step 495, loss 0.299118, acc 0.90625\n",
            "2019-04-11T10:02:50.561410: step 496, loss 0.268755, acc 0.9375\n",
            "2019-04-11T10:02:50.784208: step 497, loss 0.124874, acc 0.96875\n",
            "2019-04-11T10:02:51.001265: step 498, loss 0.0948827, acc 1\n",
            "2019-04-11T10:02:51.214528: step 499, loss 0.395665, acc 0.9375\n",
            "2019-04-11T10:02:51.429004: step 500, loss 0.312881, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:53.664923: step 500, loss 0.278923, acc 0.9208\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-500\n",
            "\n",
            "2019-04-11T10:02:53.969161: step 501, loss 0.564131, acc 0.84375\n",
            "2019-04-11T10:02:54.187338: step 502, loss 0.214356, acc 0.96875\n",
            "2019-04-11T10:02:54.405000: step 503, loss 0.269332, acc 0.90625\n",
            "2019-04-11T10:02:54.607996: step 504, loss 0.350118, acc 0.90625\n",
            "2019-04-11T10:02:54.842453: step 505, loss 0.131627, acc 0.96875\n",
            "2019-04-11T10:02:55.057209: step 506, loss 0.231429, acc 0.90625\n",
            "2019-04-11T10:02:55.264448: step 507, loss 0.16038, acc 0.96875\n",
            "2019-04-11T10:02:55.477931: step 508, loss 0.160229, acc 0.9375\n",
            "2019-04-11T10:02:55.693269: step 509, loss 0.268085, acc 0.9375\n",
            "2019-04-11T10:02:55.912567: step 510, loss 0.184818, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:02:58.149286: step 510, loss 0.292943, acc 0.9138\n",
            "\n",
            "2019-04-11T10:02:58.367117: step 511, loss 0.233838, acc 0.96875\n",
            "2019-04-11T10:02:58.583718: step 512, loss 0.504349, acc 0.84375\n",
            "2019-04-11T10:02:58.801294: step 513, loss 0.758673, acc 0.75\n",
            "2019-04-11T10:02:59.019267: step 514, loss 0.388054, acc 0.875\n",
            "2019-04-11T10:02:59.233170: step 515, loss 0.171685, acc 0.9375\n",
            "2019-04-11T10:02:59.456180: step 516, loss 0.445658, acc 0.84375\n",
            "2019-04-11T10:02:59.673262: step 517, loss 0.266815, acc 0.90625\n",
            "2019-04-11T10:02:59.888702: step 518, loss 0.24872, acc 0.9375\n",
            "2019-04-11T10:03:00.102254: step 519, loss 0.215885, acc 0.9375\n",
            "2019-04-11T10:03:00.309634: step 520, loss 0.348226, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:02.577364: step 520, loss 0.270691, acc 0.9236\n",
            "\n",
            "2019-04-11T10:03:02.795910: step 521, loss 0.201481, acc 0.9375\n",
            "2019-04-11T10:03:03.019408: step 522, loss 0.46622, acc 0.8125\n",
            "2019-04-11T10:03:03.232358: step 523, loss 0.179413, acc 0.875\n",
            "2019-04-11T10:03:03.450388: step 524, loss 0.307265, acc 0.9375\n",
            "2019-04-11T10:03:03.671895: step 525, loss 0.16493, acc 1\n",
            "2019-04-11T10:03:03.891200: step 526, loss 0.293012, acc 0.90625\n",
            "2019-04-11T10:03:04.111857: step 527, loss 0.133492, acc 1\n",
            "2019-04-11T10:03:04.325437: step 528, loss 0.452908, acc 0.9375\n",
            "2019-04-11T10:03:04.537385: step 529, loss 0.25427, acc 0.90625\n",
            "2019-04-11T10:03:04.751531: step 530, loss 0.359705, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:06.995587: step 530, loss 0.272415, acc 0.9178\n",
            "\n",
            "2019-04-11T10:03:07.214626: step 531, loss 0.275028, acc 0.9375\n",
            "2019-04-11T10:03:07.430090: step 532, loss 0.466036, acc 0.875\n",
            "2019-04-11T10:03:07.649864: step 533, loss 0.321462, acc 0.90625\n",
            "2019-04-11T10:03:07.863534: step 534, loss 0.426426, acc 0.875\n",
            "2019-04-11T10:03:08.077793: step 535, loss 0.518736, acc 0.8125\n",
            "2019-04-11T10:03:08.292999: step 536, loss 0.491223, acc 0.84375\n",
            "2019-04-11T10:03:08.512164: step 537, loss 0.458267, acc 0.8125\n",
            "2019-04-11T10:03:08.729130: step 538, loss 0.447875, acc 0.84375\n",
            "2019-04-11T10:03:08.941094: step 539, loss 0.363053, acc 0.875\n",
            "2019-04-11T10:03:09.160185: step 540, loss 0.436941, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:11.398685: step 540, loss 0.24633, acc 0.931\n",
            "\n",
            "2019-04-11T10:03:11.613647: step 541, loss 0.296817, acc 0.9375\n",
            "2019-04-11T10:03:11.832054: step 542, loss 0.156512, acc 1\n",
            "2019-04-11T10:03:12.046094: step 543, loss 0.143153, acc 1\n",
            "2019-04-11T10:03:12.277418: step 544, loss 0.526315, acc 0.875\n",
            "2019-04-11T10:03:12.495485: step 545, loss 0.256619, acc 0.96875\n",
            "2019-04-11T10:03:12.716696: step 546, loss 0.322872, acc 0.875\n",
            "2019-04-11T10:03:12.925602: step 547, loss 0.494464, acc 0.875\n",
            "2019-04-11T10:03:13.143317: step 548, loss 0.475613, acc 0.78125\n",
            "2019-04-11T10:03:13.351412: step 549, loss 0.281538, acc 0.90625\n",
            "2019-04-11T10:03:13.570063: step 550, loss 0.248507, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:15.805639: step 550, loss 0.255692, acc 0.9306\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-550\n",
            "\n",
            "2019-04-11T10:03:16.105316: step 551, loss 0.269456, acc 0.90625\n",
            "2019-04-11T10:03:16.322811: step 552, loss 0.144449, acc 0.96875\n",
            "2019-04-11T10:03:16.540972: step 553, loss 0.268609, acc 0.9375\n",
            "2019-04-11T10:03:16.760796: step 554, loss 0.419532, acc 0.875\n",
            "2019-04-11T10:03:16.977821: step 555, loss 0.17814, acc 0.9375\n",
            "2019-04-11T10:03:17.195739: step 556, loss 0.346762, acc 0.9375\n",
            "2019-04-11T10:03:17.417211: step 557, loss 0.123442, acc 0.96875\n",
            "2019-04-11T10:03:17.635376: step 558, loss 0.251311, acc 0.90625\n",
            "2019-04-11T10:03:17.854361: step 559, loss 0.297086, acc 0.90625\n",
            "2019-04-11T10:03:18.073178: step 560, loss 0.205214, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:20.310283: step 560, loss 0.261268, acc 0.9288\n",
            "\n",
            "2019-04-11T10:03:20.516780: step 561, loss 0.49965, acc 0.84375\n",
            "2019-04-11T10:03:20.738052: step 562, loss 0.0853612, acc 1\n",
            "2019-04-11T10:03:20.944395: step 563, loss 0.40419, acc 0.875\n",
            "2019-04-11T10:03:21.156901: step 564, loss 0.274622, acc 0.875\n",
            "2019-04-11T10:03:21.371476: step 565, loss 0.189805, acc 0.96875\n",
            "2019-04-11T10:03:21.591355: step 566, loss 0.247598, acc 0.9375\n",
            "2019-04-11T10:03:21.807207: step 567, loss 0.137273, acc 0.96875\n",
            "2019-04-11T10:03:22.028711: step 568, loss 0.258928, acc 0.875\n",
            "2019-04-11T10:03:22.247137: step 569, loss 0.248112, acc 0.96875\n",
            "2019-04-11T10:03:22.463165: step 570, loss 0.627342, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:24.703425: step 570, loss 0.263455, acc 0.9238\n",
            "\n",
            "2019-04-11T10:03:24.923795: step 571, loss 0.483904, acc 0.84375\n",
            "2019-04-11T10:03:25.152313: step 572, loss 0.1685, acc 0.96875\n",
            "2019-04-11T10:03:25.369431: step 573, loss 0.0742982, acc 1\n",
            "2019-04-11T10:03:25.584845: step 574, loss 0.290056, acc 0.90625\n",
            "2019-04-11T10:03:25.804625: step 575, loss 0.162133, acc 0.9375\n",
            "2019-04-11T10:03:26.023372: step 576, loss 0.275957, acc 0.9375\n",
            "2019-04-11T10:03:26.237026: step 577, loss 0.254178, acc 0.90625\n",
            "2019-04-11T10:03:26.456792: step 578, loss 0.200579, acc 0.96875\n",
            "2019-04-11T10:03:26.669795: step 579, loss 0.255698, acc 0.84375\n",
            "2019-04-11T10:03:26.892573: step 580, loss 0.147619, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:29.129222: step 580, loss 0.270127, acc 0.9226\n",
            "\n",
            "2019-04-11T10:03:29.342855: step 581, loss 0.140187, acc 0.96875\n",
            "2019-04-11T10:03:29.560058: step 582, loss 0.244675, acc 0.9375\n",
            "2019-04-11T10:03:29.777128: step 583, loss 0.316217, acc 0.90625\n",
            "2019-04-11T10:03:29.995815: step 584, loss 0.701759, acc 0.84375\n",
            "2019-04-11T10:03:30.211291: step 585, loss 0.157029, acc 0.9375\n",
            "2019-04-11T10:03:30.423409: step 586, loss 0.378921, acc 0.9375\n",
            "2019-04-11T10:03:30.640667: step 587, loss 0.446481, acc 0.78125\n",
            "2019-04-11T10:03:30.851177: step 588, loss 0.292266, acc 0.875\n",
            "2019-04-11T10:03:31.072297: step 589, loss 0.0645341, acc 1\n",
            "2019-04-11T10:03:31.289738: step 590, loss 0.297408, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:33.533060: step 590, loss 0.255577, acc 0.9278\n",
            "\n",
            "2019-04-11T10:03:33.749236: step 591, loss 0.321926, acc 0.90625\n",
            "2019-04-11T10:03:33.956820: step 592, loss 0.146528, acc 0.96875\n",
            "2019-04-11T10:03:34.166080: step 593, loss 0.235451, acc 0.9375\n",
            "2019-04-11T10:03:34.382367: step 594, loss 0.496053, acc 0.8125\n",
            "2019-04-11T10:03:34.592523: step 595, loss 0.528192, acc 0.84375\n",
            "2019-04-11T10:03:34.810574: step 596, loss 0.339023, acc 0.90625\n",
            "2019-04-11T10:03:35.026175: step 597, loss 0.141112, acc 1\n",
            "2019-04-11T10:03:35.240656: step 598, loss 0.121308, acc 0.96875\n",
            "2019-04-11T10:03:35.454156: step 599, loss 0.139367, acc 0.96875\n",
            "2019-04-11T10:03:35.665963: step 600, loss 0.414227, acc 0.8125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:37.903637: step 600, loss 0.276675, acc 0.9174\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-600\n",
            "\n",
            "2019-04-11T10:03:38.205683: step 601, loss 0.537161, acc 0.9375\n",
            "2019-04-11T10:03:38.416033: step 602, loss 0.613297, acc 0.875\n",
            "2019-04-11T10:03:38.630378: step 603, loss 0.142076, acc 0.9375\n",
            "2019-04-11T10:03:38.841982: step 604, loss 0.179338, acc 0.9375\n",
            "2019-04-11T10:03:39.047924: step 605, loss 0.261138, acc 0.9375\n",
            "2019-04-11T10:03:39.268091: step 606, loss 0.289919, acc 0.9375\n",
            "2019-04-11T10:03:39.485571: step 607, loss 0.306945, acc 0.90625\n",
            "2019-04-11T10:03:39.703129: step 608, loss 0.259502, acc 0.90625\n",
            "2019-04-11T10:03:39.921852: step 609, loss 0.139166, acc 0.96875\n",
            "2019-04-11T10:03:40.138073: step 610, loss 0.124406, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:42.395101: step 610, loss 0.238638, acc 0.9332\n",
            "\n",
            "2019-04-11T10:03:42.613323: step 611, loss 0.45832, acc 0.875\n",
            "2019-04-11T10:03:42.834653: step 612, loss 0.201759, acc 0.9375\n",
            "2019-04-11T10:03:43.042824: step 613, loss 0.209382, acc 0.96875\n",
            "2019-04-11T10:03:43.262263: step 614, loss 0.0896519, acc 1\n",
            "2019-04-11T10:03:43.479166: step 615, loss 0.206103, acc 0.9375\n",
            "2019-04-11T10:03:43.697365: step 616, loss 0.334149, acc 0.90625\n",
            "2019-04-11T10:03:43.913393: step 617, loss 0.33735, acc 0.9375\n",
            "2019-04-11T10:03:44.129244: step 618, loss 0.489079, acc 0.875\n",
            "2019-04-11T10:03:44.339029: step 619, loss 0.210614, acc 0.96875\n",
            "2019-04-11T10:03:44.555714: step 620, loss 0.335266, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:46.790775: step 620, loss 0.243315, acc 0.9316\n",
            "\n",
            "2019-04-11T10:03:47.008546: step 621, loss 0.257138, acc 0.9375\n",
            "2019-04-11T10:03:47.227057: step 622, loss 0.268485, acc 0.90625\n",
            "2019-04-11T10:03:47.441900: step 623, loss 0.426616, acc 0.875\n",
            "2019-04-11T10:03:47.648352: step 624, loss 0.139059, acc 0.96875\n",
            "2019-04-11T10:03:47.857900: step 625, loss 0.160564, acc 0.9375\n",
            "2019-04-11T10:03:48.074896: step 626, loss 0.213711, acc 0.9375\n",
            "2019-04-11T10:03:48.281445: step 627, loss 0.272304, acc 0.90625\n",
            "2019-04-11T10:03:48.503463: step 628, loss 0.351912, acc 0.875\n",
            "2019-04-11T10:03:48.713016: step 629, loss 0.176123, acc 0.90625\n",
            "2019-04-11T10:03:48.932014: step 630, loss 0.205858, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:51.172730: step 630, loss 0.229352, acc 0.938\n",
            "\n",
            "2019-04-11T10:03:51.380702: step 631, loss 0.24244, acc 0.9375\n",
            "2019-04-11T10:03:51.600495: step 632, loss 0.273393, acc 0.90625\n",
            "2019-04-11T10:03:51.817596: step 633, loss 0.296388, acc 0.9375\n",
            "2019-04-11T10:03:52.028853: step 634, loss 0.285025, acc 0.96875\n",
            "2019-04-11T10:03:52.247165: step 635, loss 0.225542, acc 0.90625\n",
            "2019-04-11T10:03:52.461973: step 636, loss 0.228358, acc 0.9375\n",
            "2019-04-11T10:03:52.682107: step 637, loss 0.3472, acc 0.9375\n",
            "2019-04-11T10:03:52.907582: step 638, loss 0.217519, acc 0.96875\n",
            "2019-04-11T10:03:53.131895: step 639, loss 0.174541, acc 0.96875\n",
            "2019-04-11T10:03:53.349800: step 640, loss 0.0895963, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:03:55.619158: step 640, loss 0.241507, acc 0.9334\n",
            "\n",
            "2019-04-11T10:03:55.835811: step 641, loss 0.198117, acc 0.9375\n",
            "2019-04-11T10:03:56.060965: step 642, loss 0.0853661, acc 1\n",
            "2019-04-11T10:03:56.285682: step 643, loss 0.359588, acc 0.875\n",
            "2019-04-11T10:03:56.508093: step 644, loss 0.365787, acc 0.90625\n",
            "2019-04-11T10:03:56.732133: step 645, loss 0.702872, acc 0.84375\n",
            "2019-04-11T10:03:56.954268: step 646, loss 0.338389, acc 0.90625\n",
            "2019-04-11T10:03:57.168701: step 647, loss 0.221546, acc 0.9375\n",
            "2019-04-11T10:03:57.397693: step 648, loss 0.361014, acc 0.90625\n",
            "2019-04-11T10:03:57.617260: step 649, loss 0.157142, acc 0.9375\n",
            "2019-04-11T10:03:57.846947: step 650, loss 0.214497, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:00.153613: step 650, loss 0.250858, acc 0.9298\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-650\n",
            "\n",
            "2019-04-11T10:04:00.473181: step 651, loss 0.166193, acc 0.96875\n",
            "2019-04-11T10:04:00.688715: step 652, loss 0.253067, acc 0.90625\n",
            "2019-04-11T10:04:00.910325: step 653, loss 0.118198, acc 1\n",
            "2019-04-11T10:04:01.124365: step 654, loss 0.132095, acc 0.96875\n",
            "2019-04-11T10:04:01.339915: step 655, loss 0.108952, acc 1\n",
            "2019-04-11T10:04:01.549194: step 656, loss 0.332701, acc 0.9375\n",
            "2019-04-11T10:04:01.760248: step 657, loss 0.460993, acc 0.875\n",
            "2019-04-11T10:04:01.980362: step 658, loss 0.172992, acc 0.90625\n",
            "2019-04-11T10:04:02.195404: step 659, loss 0.43691, acc 0.84375\n",
            "2019-04-11T10:04:02.414323: step 660, loss 0.154427, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:04.664195: step 660, loss 0.232688, acc 0.9336\n",
            "\n",
            "2019-04-11T10:04:04.886590: step 661, loss 0.132068, acc 0.9375\n",
            "2019-04-11T10:04:05.105002: step 662, loss 0.21793, acc 0.90625\n",
            "2019-04-11T10:04:05.321945: step 663, loss 0.170783, acc 0.96875\n",
            "2019-04-11T10:04:05.532739: step 664, loss 0.216595, acc 0.9375\n",
            "2019-04-11T10:04:05.747708: step 665, loss 0.168278, acc 0.9375\n",
            "2019-04-11T10:04:05.969054: step 666, loss 0.189028, acc 0.9375\n",
            "2019-04-11T10:04:06.180548: step 667, loss 0.402386, acc 0.90625\n",
            "2019-04-11T10:04:06.396348: step 668, loss 0.317848, acc 0.84375\n",
            "2019-04-11T10:04:06.610526: step 669, loss 0.34445, acc 0.90625\n",
            "2019-04-11T10:04:06.832185: step 670, loss 0.249328, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:09.065728: step 670, loss 0.236393, acc 0.9316\n",
            "\n",
            "2019-04-11T10:04:09.284216: step 671, loss 0.238435, acc 0.84375\n",
            "2019-04-11T10:04:09.501127: step 672, loss 0.200862, acc 0.96875\n",
            "2019-04-11T10:04:09.717924: step 673, loss 0.322327, acc 0.9375\n",
            "2019-04-11T10:04:09.935335: step 674, loss 0.178878, acc 1\n",
            "2019-04-11T10:04:10.153492: step 675, loss 0.517083, acc 0.875\n",
            "2019-04-11T10:04:10.366525: step 676, loss 0.267036, acc 0.90625\n",
            "2019-04-11T10:04:10.582390: step 677, loss 0.188286, acc 0.9375\n",
            "2019-04-11T10:04:10.800276: step 678, loss 0.139476, acc 0.9375\n",
            "2019-04-11T10:04:11.019125: step 679, loss 0.116945, acc 0.96875\n",
            "2019-04-11T10:04:11.238095: step 680, loss 0.178099, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:13.504169: step 680, loss 0.235968, acc 0.9304\n",
            "\n",
            "2019-04-11T10:04:13.717126: step 681, loss 0.276658, acc 0.875\n",
            "2019-04-11T10:04:13.940112: step 682, loss 0.134077, acc 1\n",
            "2019-04-11T10:04:14.158973: step 683, loss 0.147526, acc 0.9375\n",
            "2019-04-11T10:04:14.375566: step 684, loss 0.178685, acc 0.96875\n",
            "2019-04-11T10:04:14.594380: step 685, loss 0.277009, acc 0.9375\n",
            "2019-04-11T10:04:14.814822: step 686, loss 0.239416, acc 0.9375\n",
            "2019-04-11T10:04:15.037707: step 687, loss 0.494095, acc 0.84375\n",
            "2019-04-11T10:04:15.253521: step 688, loss 0.238364, acc 0.875\n",
            "2019-04-11T10:04:15.470815: step 689, loss 0.0510923, acc 1\n",
            "2019-04-11T10:04:15.690851: step 690, loss 0.149647, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:17.934923: step 690, loss 0.232693, acc 0.933\n",
            "\n",
            "2019-04-11T10:04:18.149876: step 691, loss 0.210269, acc 0.90625\n",
            "2019-04-11T10:04:18.365806: step 692, loss 0.294601, acc 0.9375\n",
            "2019-04-11T10:04:18.582265: step 693, loss 0.32196, acc 0.90625\n",
            "2019-04-11T10:04:18.791545: step 694, loss 0.176593, acc 0.9375\n",
            "2019-04-11T10:04:19.009169: step 695, loss 0.133147, acc 1\n",
            "2019-04-11T10:04:19.221610: step 696, loss 0.372548, acc 0.9375\n",
            "2019-04-11T10:04:19.431439: step 697, loss 0.20273, acc 0.90625\n",
            "2019-04-11T10:04:19.646593: step 698, loss 0.23081, acc 0.90625\n",
            "2019-04-11T10:04:19.863385: step 699, loss 0.145064, acc 1\n",
            "2019-04-11T10:04:20.088526: step 700, loss 0.0936904, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:22.323795: step 700, loss 0.223429, acc 0.9384\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-700\n",
            "\n",
            "2019-04-11T10:04:22.621332: step 701, loss 0.184678, acc 0.9375\n",
            "2019-04-11T10:04:22.839299: step 702, loss 0.0907076, acc 1\n",
            "2019-04-11T10:04:23.057799: step 703, loss 0.629685, acc 0.8125\n",
            "2019-04-11T10:04:23.278562: step 704, loss 0.0912082, acc 1\n",
            "2019-04-11T10:04:23.496175: step 705, loss 0.147249, acc 0.96875\n",
            "2019-04-11T10:04:23.717323: step 706, loss 0.102259, acc 0.9375\n",
            "2019-04-11T10:04:23.933007: step 707, loss 0.38779, acc 0.9375\n",
            "2019-04-11T10:04:24.151842: step 708, loss 0.219295, acc 0.96875\n",
            "2019-04-11T10:04:24.367106: step 709, loss 0.154573, acc 0.96875\n",
            "2019-04-11T10:04:24.583089: step 710, loss 0.391898, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:26.820924: step 710, loss 0.229462, acc 0.9334\n",
            "\n",
            "2019-04-11T10:04:27.043626: step 711, loss 0.171105, acc 0.9375\n",
            "2019-04-11T10:04:27.247868: step 712, loss 0.327657, acc 0.90625\n",
            "2019-04-11T10:04:27.466698: step 713, loss 0.207613, acc 0.875\n",
            "2019-04-11T10:04:27.681839: step 714, loss 0.317906, acc 0.9375\n",
            "2019-04-11T10:04:27.900916: step 715, loss 0.087358, acc 1\n",
            "2019-04-11T10:04:28.120779: step 716, loss 0.0987924, acc 1\n",
            "2019-04-11T10:04:28.338350: step 717, loss 0.16023, acc 0.96875\n",
            "2019-04-11T10:04:28.548712: step 718, loss 0.215614, acc 0.9375\n",
            "2019-04-11T10:04:28.765859: step 719, loss 0.361816, acc 0.90625\n",
            "2019-04-11T10:04:28.974034: step 720, loss 0.482469, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:31.208264: step 720, loss 0.226575, acc 0.9344\n",
            "\n",
            "2019-04-11T10:04:31.425052: step 721, loss 0.347211, acc 0.9375\n",
            "2019-04-11T10:04:31.643943: step 722, loss 0.144419, acc 0.9375\n",
            "2019-04-11T10:04:31.863400: step 723, loss 0.0981425, acc 0.96875\n",
            "2019-04-11T10:04:32.074430: step 724, loss 0.331019, acc 0.90625\n",
            "2019-04-11T10:04:32.288198: step 725, loss 0.435221, acc 0.875\n",
            "2019-04-11T10:04:32.504634: step 726, loss 0.282539, acc 0.875\n",
            "2019-04-11T10:04:32.721693: step 727, loss 0.298638, acc 0.90625\n",
            "2019-04-11T10:04:32.934316: step 728, loss 0.211184, acc 0.9375\n",
            "2019-04-11T10:04:33.151907: step 729, loss 0.289881, acc 0.90625\n",
            "2019-04-11T10:04:33.373067: step 730, loss 0.189869, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:35.610311: step 730, loss 0.215909, acc 0.9372\n",
            "\n",
            "2019-04-11T10:04:35.829051: step 731, loss 0.19706, acc 0.875\n",
            "2019-04-11T10:04:36.036184: step 732, loss 0.293308, acc 0.9375\n",
            "2019-04-11T10:04:36.247469: step 733, loss 0.186778, acc 0.96875\n",
            "2019-04-11T10:04:36.463370: step 734, loss 0.0713826, acc 1\n",
            "2019-04-11T10:04:36.688723: step 735, loss 0.350225, acc 0.84375\n",
            "2019-04-11T10:04:36.905194: step 736, loss 0.165289, acc 0.9375\n",
            "2019-04-11T10:04:37.123738: step 737, loss 0.173911, acc 0.9375\n",
            "2019-04-11T10:04:37.342996: step 738, loss 0.191432, acc 0.96875\n",
            "2019-04-11T10:04:37.552124: step 739, loss 0.336939, acc 0.875\n",
            "2019-04-11T10:04:37.762326: step 740, loss 0.132256, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:40.001361: step 740, loss 0.223569, acc 0.9364\n",
            "\n",
            "2019-04-11T10:04:40.221470: step 741, loss 0.0954186, acc 1\n",
            "2019-04-11T10:04:40.438251: step 742, loss 0.24803, acc 0.9375\n",
            "2019-04-11T10:04:40.654446: step 743, loss 0.191115, acc 0.9375\n",
            "2019-04-11T10:04:40.877477: step 744, loss 0.227626, acc 0.9375\n",
            "2019-04-11T10:04:41.097329: step 745, loss 0.319605, acc 0.8125\n",
            "2019-04-11T10:04:41.321505: step 746, loss 0.191658, acc 0.96875\n",
            "2019-04-11T10:04:41.558707: step 747, loss 0.220919, acc 0.9375\n",
            "2019-04-11T10:04:41.784277: step 748, loss 0.30927, acc 0.9375\n",
            "2019-04-11T10:04:42.009341: step 749, loss 0.221574, acc 0.90625\n",
            "2019-04-11T10:04:42.233973: step 750, loss 0.230424, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:44.561796: step 750, loss 0.220646, acc 0.9386\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-750\n",
            "\n",
            "2019-04-11T10:04:44.895266: step 751, loss 0.206699, acc 0.96875\n",
            "2019-04-11T10:04:45.122002: step 752, loss 0.266634, acc 0.9375\n",
            "2019-04-11T10:04:45.347539: step 753, loss 0.0714135, acc 1\n",
            "2019-04-11T10:04:45.572014: step 754, loss 0.0698818, acc 1\n",
            "2019-04-11T10:04:45.790963: step 755, loss 0.197953, acc 0.9375\n",
            "2019-04-11T10:04:46.020971: step 756, loss 0.143219, acc 0.96875\n",
            "2019-04-11T10:04:46.244363: step 757, loss 0.175024, acc 0.9375\n",
            "2019-04-11T10:04:46.465918: step 758, loss 0.306504, acc 0.84375\n",
            "2019-04-11T10:04:46.696218: step 759, loss 0.303584, acc 0.9375\n",
            "2019-04-11T10:04:46.926699: step 760, loss 0.162519, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:49.242538: step 760, loss 0.224283, acc 0.9336\n",
            "\n",
            "2019-04-11T10:04:49.473321: step 761, loss 0.381794, acc 0.875\n",
            "2019-04-11T10:04:49.702386: step 762, loss 0.148695, acc 0.96875\n",
            "2019-04-11T10:04:49.931177: step 763, loss 0.206835, acc 0.9375\n",
            "2019-04-11T10:04:50.158027: step 764, loss 0.213165, acc 0.90625\n",
            "2019-04-11T10:04:50.388993: step 765, loss 0.181295, acc 0.96875\n",
            "2019-04-11T10:04:50.613669: step 766, loss 0.547157, acc 0.875\n",
            "2019-04-11T10:04:50.833952: step 767, loss 0.164389, acc 0.9375\n",
            "2019-04-11T10:04:51.066821: step 768, loss 0.100541, acc 0.96875\n",
            "2019-04-11T10:04:51.291529: step 769, loss 0.0976624, acc 1\n",
            "2019-04-11T10:04:51.515392: step 770, loss 0.177123, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:53.747305: step 770, loss 0.218752, acc 0.9372\n",
            "\n",
            "2019-04-11T10:04:53.961652: step 771, loss 0.558202, acc 0.84375\n",
            "2019-04-11T10:04:54.177220: step 772, loss 0.20872, acc 0.9375\n",
            "2019-04-11T10:04:54.395257: step 773, loss 0.494738, acc 0.84375\n",
            "2019-04-11T10:04:54.616124: step 774, loss 0.204017, acc 0.9375\n",
            "2019-04-11T10:04:54.838822: step 775, loss 0.118658, acc 0.96875\n",
            "2019-04-11T10:04:55.059020: step 776, loss 0.365634, acc 0.90625\n",
            "2019-04-11T10:04:55.274782: step 777, loss 0.354997, acc 0.875\n",
            "2019-04-11T10:04:55.489261: step 778, loss 0.149501, acc 0.96875\n",
            "2019-04-11T10:04:55.696973: step 779, loss 0.214057, acc 0.96875\n",
            "2019-04-11T10:04:55.913214: step 780, loss 0.0395763, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:04:58.152208: step 780, loss 0.235417, acc 0.9326\n",
            "\n",
            "2019-04-11T10:04:58.372104: step 781, loss 0.271628, acc 0.9375\n",
            "2019-04-11T10:04:58.587408: step 782, loss 0.0844463, acc 1\n",
            "2019-04-11T10:04:58.806545: step 783, loss 0.0905531, acc 0.96875\n",
            "2019-04-11T10:04:59.024672: step 784, loss 0.333235, acc 0.90625\n",
            "2019-04-11T10:04:59.242969: step 785, loss 0.302192, acc 0.84375\n",
            "2019-04-11T10:04:59.465337: step 786, loss 0.107833, acc 0.96875\n",
            "2019-04-11T10:04:59.683706: step 787, loss 0.222153, acc 0.90625\n",
            "2019-04-11T10:04:59.888121: step 788, loss 0.249012, acc 0.9375\n",
            "2019-04-11T10:05:00.106705: step 789, loss 0.307945, acc 0.90625\n",
            "2019-04-11T10:05:00.324963: step 790, loss 0.224703, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:02.575646: step 790, loss 0.209831, acc 0.9436\n",
            "\n",
            "2019-04-11T10:05:02.798222: step 791, loss 0.241133, acc 0.90625\n",
            "2019-04-11T10:05:03.018583: step 792, loss 0.217225, acc 0.90625\n",
            "2019-04-11T10:05:03.239938: step 793, loss 0.163716, acc 0.96875\n",
            "2019-04-11T10:05:03.456818: step 794, loss 0.174703, acc 0.9375\n",
            "2019-04-11T10:05:03.671128: step 795, loss 0.124345, acc 0.96875\n",
            "2019-04-11T10:05:03.894968: step 796, loss 0.0723884, acc 1\n",
            "2019-04-11T10:05:04.106332: step 797, loss 0.141246, acc 0.96875\n",
            "2019-04-11T10:05:04.324022: step 798, loss 0.270793, acc 0.90625\n",
            "2019-04-11T10:05:04.543518: step 799, loss 0.159265, acc 0.90625\n",
            "2019-04-11T10:05:04.761046: step 800, loss 0.190959, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:07.010797: step 800, loss 0.209307, acc 0.9388\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-800\n",
            "\n",
            "2019-04-11T10:05:07.323691: step 801, loss 0.356894, acc 0.84375\n",
            "2019-04-11T10:05:07.569882: step 802, loss 0.089379, acc 1\n",
            "2019-04-11T10:05:07.781703: step 803, loss 0.334786, acc 0.875\n",
            "2019-04-11T10:05:07.998976: step 804, loss 0.132934, acc 0.96875\n",
            "2019-04-11T10:05:08.220126: step 805, loss 0.0990966, acc 0.96875\n",
            "2019-04-11T10:05:08.434939: step 806, loss 0.123296, acc 0.96875\n",
            "2019-04-11T10:05:08.652939: step 807, loss 0.0472302, acc 1\n",
            "2019-04-11T10:05:08.869737: step 808, loss 0.168835, acc 0.9375\n",
            "2019-04-11T10:05:09.081006: step 809, loss 0.422489, acc 0.84375\n",
            "2019-04-11T10:05:09.297942: step 810, loss 0.40677, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:11.552975: step 810, loss 0.224909, acc 0.936\n",
            "\n",
            "2019-04-11T10:05:11.772374: step 811, loss 0.282695, acc 0.90625\n",
            "2019-04-11T10:05:11.987059: step 812, loss 0.210696, acc 0.9375\n",
            "2019-04-11T10:05:12.203808: step 813, loss 0.180033, acc 0.9375\n",
            "2019-04-11T10:05:12.422791: step 814, loss 0.257272, acc 0.9375\n",
            "2019-04-11T10:05:12.640299: step 815, loss 0.224131, acc 0.90625\n",
            "2019-04-11T10:05:12.870359: step 816, loss 0.622686, acc 0.84375\n",
            "2019-04-11T10:05:13.096292: step 817, loss 0.0852215, acc 0.96875\n",
            "2019-04-11T10:05:13.321582: step 818, loss 0.274803, acc 0.9375\n",
            "2019-04-11T10:05:13.539216: step 819, loss 0.379796, acc 0.84375\n",
            "2019-04-11T10:05:13.749028: step 820, loss 0.230877, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:15.994084: step 820, loss 0.253313, acc 0.923\n",
            "\n",
            "2019-04-11T10:05:16.211895: step 821, loss 0.0995977, acc 1\n",
            "2019-04-11T10:05:16.430969: step 822, loss 0.0985418, acc 0.9375\n",
            "2019-04-11T10:05:16.649615: step 823, loss 0.246093, acc 0.875\n",
            "2019-04-11T10:05:16.871611: step 824, loss 0.44945, acc 0.84375\n",
            "2019-04-11T10:05:17.082576: step 825, loss 0.320674, acc 0.875\n",
            "2019-04-11T10:05:17.297594: step 826, loss 0.277119, acc 0.9375\n",
            "2019-04-11T10:05:17.517967: step 827, loss 0.218645, acc 0.96875\n",
            "2019-04-11T10:05:17.735248: step 828, loss 0.250503, acc 0.90625\n",
            "2019-04-11T10:05:17.955201: step 829, loss 0.257568, acc 0.90625\n",
            "2019-04-11T10:05:18.175523: step 830, loss 0.0944322, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:20.412568: step 830, loss 0.225704, acc 0.9382\n",
            "\n",
            "2019-04-11T10:05:20.631615: step 831, loss 0.118981, acc 0.96875\n",
            "2019-04-11T10:05:20.853322: step 832, loss 0.145532, acc 0.9375\n",
            "2019-04-11T10:05:21.074222: step 833, loss 0.22778, acc 0.875\n",
            "2019-04-11T10:05:21.292417: step 834, loss 0.32439, acc 0.875\n",
            "2019-04-11T10:05:21.502830: step 835, loss 0.109983, acc 0.96875\n",
            "2019-04-11T10:05:21.725985: step 836, loss 0.378375, acc 0.875\n",
            "2019-04-11T10:05:21.944935: step 837, loss 0.189247, acc 0.9375\n",
            "2019-04-11T10:05:22.164836: step 838, loss 0.102613, acc 0.96875\n",
            "2019-04-11T10:05:22.372394: step 839, loss 0.35926, acc 0.875\n",
            "2019-04-11T10:05:22.588203: step 840, loss 0.291394, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:24.833311: step 840, loss 0.211221, acc 0.9374\n",
            "\n",
            "2019-04-11T10:05:25.050976: step 841, loss 0.359937, acc 0.9375\n",
            "2019-04-11T10:05:25.256438: step 842, loss 0.567875, acc 0.875\n",
            "2019-04-11T10:05:25.480879: step 843, loss 0.122508, acc 0.96875\n",
            "2019-04-11T10:05:25.694193: step 844, loss 0.469881, acc 0.84375\n",
            "2019-04-11T10:05:25.911095: step 845, loss 0.119828, acc 1\n",
            "2019-04-11T10:05:26.128023: step 846, loss 0.25035, acc 0.9375\n",
            "2019-04-11T10:05:26.348624: step 847, loss 0.482182, acc 0.8125\n",
            "2019-04-11T10:05:26.571406: step 848, loss 0.397056, acc 0.90625\n",
            "2019-04-11T10:05:26.791343: step 849, loss 0.330266, acc 0.84375\n",
            "2019-04-11T10:05:27.015686: step 850, loss 0.188131, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:29.252690: step 850, loss 0.217558, acc 0.9364\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-850\n",
            "\n",
            "2019-04-11T10:05:29.561597: step 851, loss 0.554181, acc 0.90625\n",
            "2019-04-11T10:05:29.778436: step 852, loss 0.325342, acc 0.84375\n",
            "2019-04-11T10:05:29.994712: step 853, loss 0.283992, acc 0.90625\n",
            "2019-04-11T10:05:30.213965: step 854, loss 0.296213, acc 0.90625\n",
            "2019-04-11T10:05:30.431705: step 855, loss 0.332695, acc 0.9375\n",
            "2019-04-11T10:05:30.648202: step 856, loss 0.589253, acc 0.875\n",
            "2019-04-11T10:05:30.872032: step 857, loss 0.286237, acc 0.9375\n",
            "2019-04-11T10:05:31.085926: step 858, loss 0.371376, acc 0.90625\n",
            "2019-04-11T10:05:31.294130: step 859, loss 0.112773, acc 0.96875\n",
            "2019-04-11T10:05:31.515183: step 860, loss 0.248245, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:33.750609: step 860, loss 0.23242, acc 0.9294\n",
            "\n",
            "2019-04-11T10:05:33.969089: step 861, loss 0.243376, acc 0.9375\n",
            "2019-04-11T10:05:34.184515: step 862, loss 0.222757, acc 0.9375\n",
            "2019-04-11T10:05:34.395551: step 863, loss 0.105377, acc 0.96875\n",
            "2019-04-11T10:05:34.609338: step 864, loss 0.278018, acc 0.9375\n",
            "2019-04-11T10:05:34.831323: step 865, loss 0.256971, acc 0.90625\n",
            "2019-04-11T10:05:35.051709: step 866, loss 0.0799213, acc 1\n",
            "2019-04-11T10:05:35.271476: step 867, loss 0.229751, acc 0.96875\n",
            "2019-04-11T10:05:35.488905: step 868, loss 0.252442, acc 0.96875\n",
            "2019-04-11T10:05:35.701428: step 869, loss 0.437084, acc 0.875\n",
            "2019-04-11T10:05:35.916630: step 870, loss 0.139413, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:38.153159: step 870, loss 0.213882, acc 0.9368\n",
            "\n",
            "2019-04-11T10:05:38.369274: step 871, loss 0.337033, acc 0.90625\n",
            "2019-04-11T10:05:38.587825: step 872, loss 0.157152, acc 0.9375\n",
            "2019-04-11T10:05:38.807416: step 873, loss 0.111439, acc 0.96875\n",
            "2019-04-11T10:05:39.028551: step 874, loss 0.329881, acc 0.9375\n",
            "2019-04-11T10:05:39.238342: step 875, loss 0.235817, acc 0.9375\n",
            "2019-04-11T10:05:39.456047: step 876, loss 0.170055, acc 0.9375\n",
            "2019-04-11T10:05:39.668808: step 877, loss 0.34572, acc 0.875\n",
            "2019-04-11T10:05:39.882962: step 878, loss 0.201203, acc 0.9375\n",
            "2019-04-11T10:05:40.098875: step 879, loss 0.0563793, acc 1\n",
            "2019-04-11T10:05:40.316561: step 880, loss 0.15256, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:42.591854: step 880, loss 0.212424, acc 0.9418\n",
            "\n",
            "2019-04-11T10:05:42.813802: step 881, loss 0.135433, acc 0.9375\n",
            "2019-04-11T10:05:43.045424: step 882, loss 0.0717251, acc 1\n",
            "2019-04-11T10:05:43.270368: step 883, loss 0.109368, acc 0.96875\n",
            "2019-04-11T10:05:43.484463: step 884, loss 0.0756174, acc 1\n",
            "2019-04-11T10:05:43.705041: step 885, loss 0.133199, acc 0.96875\n",
            "2019-04-11T10:05:43.923831: step 886, loss 0.173301, acc 0.9375\n",
            "2019-04-11T10:05:44.146307: step 887, loss 0.0975199, acc 0.96875\n",
            "2019-04-11T10:05:44.364816: step 888, loss 0.0768631, acc 0.96875\n",
            "2019-04-11T10:05:44.581317: step 889, loss 0.255883, acc 0.9375\n",
            "2019-04-11T10:05:44.802506: step 890, loss 0.125444, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:47.042863: step 890, loss 0.215252, acc 0.9396\n",
            "\n",
            "2019-04-11T10:05:47.256267: step 891, loss 0.0801441, acc 1\n",
            "2019-04-11T10:05:47.475501: step 892, loss 0.247181, acc 0.9375\n",
            "2019-04-11T10:05:47.685514: step 893, loss 0.358524, acc 0.90625\n",
            "2019-04-11T10:05:47.901473: step 894, loss 0.394051, acc 0.8125\n",
            "2019-04-11T10:05:48.117280: step 895, loss 0.282006, acc 0.9375\n",
            "2019-04-11T10:05:48.337302: step 896, loss 0.235486, acc 0.90625\n",
            "2019-04-11T10:05:48.558085: step 897, loss 0.197713, acc 0.96875\n",
            "2019-04-11T10:05:48.780168: step 898, loss 0.343247, acc 0.90625\n",
            "2019-04-11T10:05:48.998554: step 899, loss 0.178188, acc 0.90625\n",
            "2019-04-11T10:05:49.219517: step 900, loss 0.347242, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:51.455957: step 900, loss 0.21793, acc 0.9346\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-900\n",
            "\n",
            "2019-04-11T10:05:51.758438: step 901, loss 0.307027, acc 0.875\n",
            "2019-04-11T10:05:51.976343: step 902, loss 0.359116, acc 0.90625\n",
            "2019-04-11T10:05:52.196295: step 903, loss 0.2599, acc 0.9375\n",
            "2019-04-11T10:05:52.414805: step 904, loss 0.21193, acc 0.96875\n",
            "2019-04-11T10:05:52.624630: step 905, loss 0.403986, acc 0.90625\n",
            "2019-04-11T10:05:52.846901: step 906, loss 0.156807, acc 0.90625\n",
            "2019-04-11T10:05:53.060617: step 907, loss 0.115062, acc 1\n",
            "2019-04-11T10:05:53.279714: step 908, loss 0.10355, acc 1\n",
            "2019-04-11T10:05:53.502976: step 909, loss 0.319773, acc 0.875\n",
            "2019-04-11T10:05:53.721786: step 910, loss 0.458791, acc 0.78125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:05:55.958104: step 910, loss 0.205692, acc 0.9434\n",
            "\n",
            "2019-04-11T10:05:56.177394: step 911, loss 0.356217, acc 0.90625\n",
            "2019-04-11T10:05:56.397686: step 912, loss 0.169943, acc 0.96875\n",
            "2019-04-11T10:05:56.616086: step 913, loss 0.226362, acc 0.9375\n",
            "2019-04-11T10:05:56.834685: step 914, loss 0.19203, acc 0.90625\n",
            "2019-04-11T10:05:57.047930: step 915, loss 0.135743, acc 0.96875\n",
            "2019-04-11T10:05:57.267169: step 916, loss 0.177448, acc 0.96875\n",
            "2019-04-11T10:05:57.480606: step 917, loss 0.433812, acc 0.84375\n",
            "2019-04-11T10:05:57.700198: step 918, loss 0.153345, acc 0.96875\n",
            "2019-04-11T10:05:57.911159: step 919, loss 0.0931588, acc 1\n",
            "2019-04-11T10:05:58.127264: step 920, loss 0.402827, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:00.369218: step 920, loss 0.212768, acc 0.9402\n",
            "\n",
            "2019-04-11T10:06:00.589914: step 921, loss 0.109639, acc 0.96875\n",
            "2019-04-11T10:06:00.812935: step 922, loss 0.3225, acc 0.90625\n",
            "2019-04-11T10:06:01.030630: step 923, loss 0.377475, acc 0.9375\n",
            "2019-04-11T10:06:01.238107: step 924, loss 0.042263, acc 1\n",
            "2019-04-11T10:06:01.459531: step 925, loss 0.193591, acc 0.9375\n",
            "2019-04-11T10:06:01.672315: step 926, loss 0.159272, acc 0.9375\n",
            "2019-04-11T10:06:01.898563: step 927, loss 0.325648, acc 0.84375\n",
            "2019-04-11T10:06:02.119344: step 928, loss 0.251973, acc 0.9375\n",
            "2019-04-11T10:06:02.335326: step 929, loss 0.0427439, acc 1\n",
            "2019-04-11T10:06:02.554554: step 930, loss 0.25346, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:04.792844: step 930, loss 0.202059, acc 0.9454\n",
            "\n",
            "2019-04-11T10:06:05.013540: step 931, loss 0.147907, acc 0.96875\n",
            "2019-04-11T10:06:05.234927: step 932, loss 0.165602, acc 0.9375\n",
            "2019-04-11T10:06:05.452304: step 933, loss 0.189689, acc 0.9375\n",
            "2019-04-11T10:06:05.673796: step 934, loss 0.248993, acc 0.90625\n",
            "2019-04-11T10:06:05.894693: step 935, loss 0.144682, acc 0.9375\n",
            "2019-04-11T10:06:06.117550: step 936, loss 0.493953, acc 0.9375\n",
            "2019-04-11T10:06:06.333812: step 937, loss 0.139426, acc 0.96875\n",
            "2019-04-11T10:06:06.545519: step 938, loss 0.286217, acc 0.90625\n",
            "2019-04-11T10:06:06.763793: step 939, loss 0.218391, acc 0.875\n",
            "2019-04-11T10:06:06.986685: step 940, loss 0.0529381, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:09.226302: step 940, loss 0.187677, acc 0.9494\n",
            "\n",
            "2019-04-11T10:06:09.443722: step 941, loss 0.221218, acc 0.9375\n",
            "2019-04-11T10:06:09.660388: step 942, loss 0.332757, acc 0.90625\n",
            "2019-04-11T10:06:09.881623: step 943, loss 0.147666, acc 0.9375\n",
            "2019-04-11T10:06:10.099502: step 944, loss 0.232145, acc 0.90625\n",
            "2019-04-11T10:06:10.320177: step 945, loss 0.194352, acc 0.96875\n",
            "2019-04-11T10:06:10.537370: step 946, loss 0.316988, acc 0.9375\n",
            "2019-04-11T10:06:10.758635: step 947, loss 0.192253, acc 0.9375\n",
            "2019-04-11T10:06:10.978105: step 948, loss 0.0605346, acc 1\n",
            "2019-04-11T10:06:11.200748: step 949, loss 0.261512, acc 0.96875\n",
            "2019-04-11T10:06:11.412400: step 950, loss 0.318734, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:13.662987: step 950, loss 0.1994, acc 0.9456\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-950\n",
            "\n",
            "2019-04-11T10:06:13.985143: step 951, loss 0.219903, acc 0.96875\n",
            "2019-04-11T10:06:14.199635: step 952, loss 0.32606, acc 0.875\n",
            "2019-04-11T10:06:14.413249: step 953, loss 0.203353, acc 0.90625\n",
            "2019-04-11T10:06:14.635950: step 954, loss 0.117872, acc 0.96875\n",
            "2019-04-11T10:06:14.861794: step 955, loss 0.187923, acc 0.9375\n",
            "2019-04-11T10:06:15.081649: step 956, loss 0.23609, acc 0.875\n",
            "2019-04-11T10:06:15.304607: step 957, loss 0.037174, acc 1\n",
            "2019-04-11T10:06:15.521339: step 958, loss 0.405741, acc 0.90625\n",
            "2019-04-11T10:06:15.741068: step 959, loss 0.334885, acc 0.875\n",
            "2019-04-11T10:06:15.958792: step 960, loss 0.205426, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:18.197432: step 960, loss 0.190442, acc 0.9494\n",
            "\n",
            "2019-04-11T10:06:18.420289: step 961, loss 0.196374, acc 0.9375\n",
            "2019-04-11T10:06:18.640932: step 962, loss 0.100112, acc 0.96875\n",
            "2019-04-11T10:06:18.864591: step 963, loss 0.125741, acc 1\n",
            "2019-04-11T10:06:19.088082: step 964, loss 0.264558, acc 0.875\n",
            "2019-04-11T10:06:19.304509: step 965, loss 0.228752, acc 0.96875\n",
            "2019-04-11T10:06:19.523355: step 966, loss 0.0954915, acc 1\n",
            "2019-04-11T10:06:19.741373: step 967, loss 0.189717, acc 0.9375\n",
            "2019-04-11T10:06:19.959068: step 968, loss 0.145291, acc 0.9375\n",
            "2019-04-11T10:06:20.179426: step 969, loss 0.355273, acc 0.9375\n",
            "2019-04-11T10:06:20.403168: step 970, loss 0.117682, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:22.637211: step 970, loss 0.190364, acc 0.9462\n",
            "\n",
            "2019-04-11T10:06:22.861121: step 971, loss 0.130263, acc 0.96875\n",
            "2019-04-11T10:06:23.077549: step 972, loss 0.331408, acc 0.90625\n",
            "2019-04-11T10:06:23.293417: step 973, loss 0.142981, acc 0.96875\n",
            "2019-04-11T10:06:23.515821: step 974, loss 0.347959, acc 0.9375\n",
            "2019-04-11T10:06:23.728938: step 975, loss 0.21976, acc 0.96875\n",
            "2019-04-11T10:06:23.940842: step 976, loss 0.0935367, acc 0.96875\n",
            "2019-04-11T10:06:24.163517: step 977, loss 0.10647, acc 0.9375\n",
            "2019-04-11T10:06:24.380356: step 978, loss 0.510337, acc 0.875\n",
            "2019-04-11T10:06:24.597072: step 979, loss 0.249613, acc 0.90625\n",
            "2019-04-11T10:06:24.818645: step 980, loss 0.148696, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:27.058239: step 980, loss 0.203702, acc 0.9396\n",
            "\n",
            "2019-04-11T10:06:27.274426: step 981, loss 0.0494678, acc 1\n",
            "2019-04-11T10:06:27.491934: step 982, loss 0.471547, acc 0.875\n",
            "2019-04-11T10:06:27.712030: step 983, loss 0.170153, acc 0.96875\n",
            "2019-04-11T10:06:27.929675: step 984, loss 0.149964, acc 0.96875\n",
            "2019-04-11T10:06:28.149202: step 985, loss 0.181844, acc 0.9375\n",
            "2019-04-11T10:06:28.361702: step 986, loss 0.571931, acc 0.84375\n",
            "2019-04-11T10:06:28.583141: step 987, loss 0.383933, acc 0.875\n",
            "2019-04-11T10:06:28.792292: step 988, loss 0.3745, acc 0.875\n",
            "2019-04-11T10:06:29.003185: step 989, loss 0.334583, acc 0.84375\n",
            "2019-04-11T10:06:29.225315: step 990, loss 0.30274, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:31.489420: step 990, loss 0.191565, acc 0.949\n",
            "\n",
            "2019-04-11T10:06:31.709456: step 991, loss 0.15522, acc 0.96875\n",
            "2019-04-11T10:06:31.930100: step 992, loss 0.340896, acc 0.90625\n",
            "2019-04-11T10:06:32.139027: step 993, loss 0.0722288, acc 1\n",
            "2019-04-11T10:06:32.361653: step 994, loss 0.180152, acc 0.90625\n",
            "2019-04-11T10:06:32.577734: step 995, loss 0.476237, acc 0.875\n",
            "2019-04-11T10:06:32.794006: step 996, loss 0.0884291, acc 0.96875\n",
            "2019-04-11T10:06:33.011657: step 997, loss 0.214017, acc 0.90625\n",
            "2019-04-11T10:06:33.219420: step 998, loss 0.228973, acc 0.90625\n",
            "2019-04-11T10:06:33.439410: step 999, loss 0.281676, acc 0.9375\n",
            "2019-04-11T10:06:33.663538: step 1000, loss 0.242332, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:35.907959: step 1000, loss 0.201126, acc 0.9432\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1000\n",
            "\n",
            "2019-04-11T10:06:36.213637: step 1001, loss 0.309929, acc 0.96875\n",
            "2019-04-11T10:06:36.436008: step 1002, loss 0.460846, acc 0.90625\n",
            "2019-04-11T10:06:36.656020: step 1003, loss 0.220487, acc 0.90625\n",
            "2019-04-11T10:06:36.881217: step 1004, loss 0.0960817, acc 1\n",
            "2019-04-11T10:06:37.097268: step 1005, loss 0.434094, acc 0.875\n",
            "2019-04-11T10:06:37.317356: step 1006, loss 0.0709579, acc 1\n",
            "2019-04-11T10:06:37.537306: step 1007, loss 0.0995579, acc 0.96875\n",
            "2019-04-11T10:06:37.752050: step 1008, loss 0.134241, acc 0.9375\n",
            "2019-04-11T10:06:37.962604: step 1009, loss 0.166719, acc 0.9375\n",
            "2019-04-11T10:06:38.184523: step 1010, loss 0.0623806, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:40.422309: step 1010, loss 0.198519, acc 0.9456\n",
            "\n",
            "2019-04-11T10:06:40.638362: step 1011, loss 0.170254, acc 0.9375\n",
            "2019-04-11T10:06:40.860019: step 1012, loss 0.372274, acc 0.875\n",
            "2019-04-11T10:06:41.079123: step 1013, loss 0.174362, acc 0.96875\n",
            "2019-04-11T10:06:41.297576: step 1014, loss 0.173879, acc 0.96875\n",
            "2019-04-11T10:06:41.508031: step 1015, loss 0.484962, acc 0.90625\n",
            "2019-04-11T10:06:41.725459: step 1016, loss 0.364308, acc 0.9375\n",
            "2019-04-11T10:06:41.945240: step 1017, loss 0.212483, acc 0.90625\n",
            "2019-04-11T10:06:42.154991: step 1018, loss 0.115261, acc 0.96875\n",
            "2019-04-11T10:06:42.376409: step 1019, loss 0.42704, acc 0.90625\n",
            "2019-04-11T10:06:42.600069: step 1020, loss 0.140893, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:44.853398: step 1020, loss 0.187633, acc 0.9468\n",
            "\n",
            "2019-04-11T10:06:45.074874: step 1021, loss 0.0662463, acc 0.96875\n",
            "2019-04-11T10:06:45.295266: step 1022, loss 0.150001, acc 0.96875\n",
            "2019-04-11T10:06:45.506258: step 1023, loss 0.230643, acc 0.9375\n",
            "2019-04-11T10:06:45.724906: step 1024, loss 0.313311, acc 0.875\n",
            "2019-04-11T10:06:45.939627: step 1025, loss 0.128646, acc 0.96875\n",
            "2019-04-11T10:06:46.160267: step 1026, loss 0.178078, acc 0.96875\n",
            "2019-04-11T10:06:46.369975: step 1027, loss 0.10581, acc 0.96875\n",
            "2019-04-11T10:06:46.589655: step 1028, loss 0.410463, acc 0.90625\n",
            "2019-04-11T10:06:46.806830: step 1029, loss 0.146434, acc 0.96875\n",
            "2019-04-11T10:06:47.028939: step 1030, loss 0.210521, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:49.273582: step 1030, loss 0.191382, acc 0.9512\n",
            "\n",
            "2019-04-11T10:06:49.495503: step 1031, loss 0.182345, acc 0.96875\n",
            "2019-04-11T10:06:49.713154: step 1032, loss 0.216755, acc 0.9375\n",
            "2019-04-11T10:06:49.930272: step 1033, loss 0.296231, acc 0.90625\n",
            "2019-04-11T10:06:50.143381: step 1034, loss 0.205361, acc 0.96875\n",
            "2019-04-11T10:06:50.363377: step 1035, loss 0.278002, acc 0.9375\n",
            "2019-04-11T10:06:50.580963: step 1036, loss 0.145745, acc 0.9375\n",
            "2019-04-11T10:06:50.799664: step 1037, loss 0.21941, acc 0.90625\n",
            "2019-04-11T10:06:51.020721: step 1038, loss 0.0564367, acc 1\n",
            "2019-04-11T10:06:51.234953: step 1039, loss 0.21441, acc 0.96875\n",
            "2019-04-11T10:06:51.450988: step 1040, loss 0.211155, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:53.689693: step 1040, loss 0.197, acc 0.946\n",
            "\n",
            "2019-04-11T10:06:53.907005: step 1041, loss 0.202884, acc 0.90625\n",
            "2019-04-11T10:06:54.125417: step 1042, loss 0.12491, acc 0.96875\n",
            "2019-04-11T10:06:54.347657: step 1043, loss 0.345033, acc 0.90625\n",
            "2019-04-11T10:06:54.566319: step 1044, loss 0.417797, acc 0.9375\n",
            "2019-04-11T10:06:54.785134: step 1045, loss 0.184754, acc 0.90625\n",
            "2019-04-11T10:06:55.008044: step 1046, loss 0.0842722, acc 0.96875\n",
            "2019-04-11T10:06:55.225050: step 1047, loss 0.253814, acc 0.9375\n",
            "2019-04-11T10:06:55.444401: step 1048, loss 0.217155, acc 0.90625\n",
            "2019-04-11T10:06:55.660978: step 1049, loss 0.297286, acc 0.90625\n",
            "2019-04-11T10:06:55.878722: step 1050, loss 0.052967, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:06:58.118887: step 1050, loss 0.186083, acc 0.9478\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1050\n",
            "\n",
            "2019-04-11T10:06:58.428298: step 1051, loss 0.158914, acc 0.9375\n",
            "2019-04-11T10:06:58.644330: step 1052, loss 0.384638, acc 0.9375\n",
            "2019-04-11T10:06:58.869943: step 1053, loss 0.0741382, acc 1\n",
            "2019-04-11T10:06:59.092287: step 1054, loss 0.091888, acc 0.96875\n",
            "2019-04-11T10:06:59.309825: step 1055, loss 0.17154, acc 0.96875\n",
            "2019-04-11T10:06:59.532020: step 1056, loss 0.16043, acc 0.96875\n",
            "2019-04-11T10:06:59.742587: step 1057, loss 0.214203, acc 0.90625\n",
            "2019-04-11T10:06:59.959541: step 1058, loss 0.277454, acc 0.9375\n",
            "2019-04-11T10:07:00.208353: step 1059, loss 0.136522, acc 0.96875\n",
            "2019-04-11T10:07:00.429093: step 1060, loss 0.125737, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:02.700629: step 1060, loss 0.181333, acc 0.9504\n",
            "\n",
            "2019-04-11T10:07:02.920664: step 1061, loss 0.244473, acc 0.90625\n",
            "2019-04-11T10:07:03.139459: step 1062, loss 0.186124, acc 0.9375\n",
            "2019-04-11T10:07:03.360011: step 1063, loss 0.241014, acc 0.9375\n",
            "2019-04-11T10:07:03.580889: step 1064, loss 0.120432, acc 0.96875\n",
            "2019-04-11T10:07:03.805439: step 1065, loss 0.43008, acc 0.875\n",
            "2019-04-11T10:07:04.022305: step 1066, loss 0.740055, acc 0.875\n",
            "2019-04-11T10:07:04.240397: step 1067, loss 0.0833436, acc 1\n",
            "2019-04-11T10:07:04.461106: step 1068, loss 0.133709, acc 0.9375\n",
            "2019-04-11T10:07:04.682665: step 1069, loss 0.0454771, acc 1\n",
            "2019-04-11T10:07:04.908102: step 1070, loss 0.110754, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:07.153180: step 1070, loss 0.198184, acc 0.9452\n",
            "\n",
            "2019-04-11T10:07:07.373035: step 1071, loss 0.140562, acc 0.96875\n",
            "2019-04-11T10:07:07.588828: step 1072, loss 0.678508, acc 0.875\n",
            "2019-04-11T10:07:07.810240: step 1073, loss 0.227386, acc 0.90625\n",
            "2019-04-11T10:07:08.024828: step 1074, loss 0.319192, acc 0.9375\n",
            "2019-04-11T10:07:08.245926: step 1075, loss 0.109235, acc 0.9375\n",
            "2019-04-11T10:07:08.457727: step 1076, loss 0.205955, acc 0.9375\n",
            "2019-04-11T10:07:08.675960: step 1077, loss 0.406971, acc 0.875\n",
            "2019-04-11T10:07:08.895867: step 1078, loss 0.266964, acc 0.90625\n",
            "2019-04-11T10:07:09.108504: step 1079, loss 0.302604, acc 0.90625\n",
            "2019-04-11T10:07:09.324123: step 1080, loss 0.414301, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:11.566877: step 1080, loss 0.182286, acc 0.9502\n",
            "\n",
            "2019-04-11T10:07:11.784006: step 1081, loss 0.120331, acc 0.96875\n",
            "2019-04-11T10:07:12.005958: step 1082, loss 0.171695, acc 0.9375\n",
            "2019-04-11T10:07:12.228236: step 1083, loss 0.158871, acc 0.96875\n",
            "2019-04-11T10:07:12.443779: step 1084, loss 0.164428, acc 0.90625\n",
            "2019-04-11T10:07:12.665208: step 1085, loss 0.376053, acc 0.84375\n",
            "2019-04-11T10:07:12.886287: step 1086, loss 0.208222, acc 0.9375\n",
            "2019-04-11T10:07:13.109268: step 1087, loss 0.2537, acc 0.96875\n",
            "2019-04-11T10:07:13.331447: step 1088, loss 0.163852, acc 0.96875\n",
            "2019-04-11T10:07:13.561394: step 1089, loss 0.174292, acc 0.90625\n",
            "2019-04-11T10:07:13.783260: step 1090, loss 0.254101, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:16.043439: step 1090, loss 0.188723, acc 0.948\n",
            "\n",
            "2019-04-11T10:07:16.261707: step 1091, loss 0.0380849, acc 1\n",
            "2019-04-11T10:07:16.483698: step 1092, loss 0.171883, acc 0.90625\n",
            "2019-04-11T10:07:16.705619: step 1093, loss 0.188044, acc 0.9375\n",
            "2019-04-11T10:07:16.922468: step 1094, loss 0.444859, acc 0.875\n",
            "2019-04-11T10:07:17.143639: step 1095, loss 0.193661, acc 0.90625\n",
            "2019-04-11T10:07:17.363913: step 1096, loss 0.0819849, acc 1\n",
            "2019-04-11T10:07:17.583373: step 1097, loss 0.469746, acc 0.9375\n",
            "2019-04-11T10:07:17.800381: step 1098, loss 0.387483, acc 0.875\n",
            "2019-04-11T10:07:18.020349: step 1099, loss 0.177471, acc 0.96875\n",
            "2019-04-11T10:07:18.236096: step 1100, loss 0.261257, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:20.472307: step 1100, loss 0.19597, acc 0.9472\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1100\n",
            "\n",
            "2019-04-11T10:07:20.788525: step 1101, loss 0.188384, acc 0.96875\n",
            "2019-04-11T10:07:21.008979: step 1102, loss 0.233096, acc 0.90625\n",
            "2019-04-11T10:07:21.227934: step 1103, loss 0.265816, acc 0.90625\n",
            "2019-04-11T10:07:21.440711: step 1104, loss 0.162585, acc 0.96875\n",
            "2019-04-11T10:07:21.661242: step 1105, loss 0.152111, acc 0.96875\n",
            "2019-04-11T10:07:21.874688: step 1106, loss 0.169951, acc 0.9375\n",
            "2019-04-11T10:07:22.087069: step 1107, loss 0.103322, acc 0.96875\n",
            "2019-04-11T10:07:22.305544: step 1108, loss 0.202241, acc 0.90625\n",
            "2019-04-11T10:07:22.514030: step 1109, loss 0.344477, acc 0.875\n",
            "2019-04-11T10:07:22.739430: step 1110, loss 0.177841, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:24.984851: step 1110, loss 0.183552, acc 0.9484\n",
            "\n",
            "2019-04-11T10:07:25.202898: step 1111, loss 0.27131, acc 0.90625\n",
            "2019-04-11T10:07:25.418047: step 1112, loss 0.219061, acc 0.875\n",
            "2019-04-11T10:07:25.635135: step 1113, loss 0.5426, acc 0.84375\n",
            "2019-04-11T10:07:25.862851: step 1114, loss 0.0969094, acc 0.96875\n",
            "2019-04-11T10:07:26.081955: step 1115, loss 0.263045, acc 0.9375\n",
            "2019-04-11T10:07:26.299856: step 1116, loss 0.186319, acc 0.96875\n",
            "2019-04-11T10:07:26.525849: step 1117, loss 0.39471, acc 0.9375\n",
            "2019-04-11T10:07:26.744707: step 1118, loss 0.0752926, acc 1\n",
            "2019-04-11T10:07:26.952505: step 1119, loss 0.0416449, acc 1\n",
            "2019-04-11T10:07:27.178797: step 1120, loss 0.103163, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:29.412948: step 1120, loss 0.181446, acc 0.9514\n",
            "\n",
            "2019-04-11T10:07:29.634111: step 1121, loss 0.249293, acc 0.875\n",
            "2019-04-11T10:07:29.852363: step 1122, loss 0.134803, acc 0.96875\n",
            "2019-04-11T10:07:30.073087: step 1123, loss 0.147099, acc 0.9375\n",
            "2019-04-11T10:07:30.288293: step 1124, loss 0.0382749, acc 1\n",
            "2019-04-11T10:07:30.507681: step 1125, loss 0.044806, acc 1\n",
            "2019-04-11T10:07:30.727735: step 1126, loss 0.127688, acc 0.90625\n",
            "2019-04-11T10:07:30.940346: step 1127, loss 0.165377, acc 0.9375\n",
            "2019-04-11T10:07:31.160196: step 1128, loss 0.0794105, acc 0.96875\n",
            "2019-04-11T10:07:31.380859: step 1129, loss 0.302444, acc 0.9375\n",
            "2019-04-11T10:07:31.600126: step 1130, loss 0.226918, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:33.838148: step 1130, loss 0.175745, acc 0.9494\n",
            "\n",
            "2019-04-11T10:07:34.059417: step 1131, loss 0.167278, acc 0.9375\n",
            "2019-04-11T10:07:34.276785: step 1132, loss 0.192993, acc 0.90625\n",
            "2019-04-11T10:07:34.498840: step 1133, loss 0.221149, acc 0.9375\n",
            "2019-04-11T10:07:34.714080: step 1134, loss 0.147584, acc 0.96875\n",
            "2019-04-11T10:07:34.940908: step 1135, loss 0.0665191, acc 1\n",
            "2019-04-11T10:07:35.157803: step 1136, loss 0.204747, acc 0.90625\n",
            "2019-04-11T10:07:35.368951: step 1137, loss 0.184784, acc 0.96875\n",
            "2019-04-11T10:07:35.591069: step 1138, loss 0.12768, acc 0.96875\n",
            "2019-04-11T10:07:35.812332: step 1139, loss 0.17372, acc 0.9375\n",
            "2019-04-11T10:07:36.045531: step 1140, loss 0.299552, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:38.283215: step 1140, loss 0.174705, acc 0.9532\n",
            "\n",
            "2019-04-11T10:07:38.500495: step 1141, loss 0.221429, acc 0.90625\n",
            "2019-04-11T10:07:38.720077: step 1142, loss 0.121368, acc 0.9375\n",
            "2019-04-11T10:07:38.940831: step 1143, loss 0.177174, acc 0.96875\n",
            "2019-04-11T10:07:39.161716: step 1144, loss 0.106227, acc 0.96875\n",
            "2019-04-11T10:07:39.383277: step 1145, loss 0.0997557, acc 0.96875\n",
            "2019-04-11T10:07:39.605037: step 1146, loss 0.392314, acc 0.84375\n",
            "2019-04-11T10:07:39.823984: step 1147, loss 0.343832, acc 0.90625\n",
            "2019-04-11T10:07:40.044935: step 1148, loss 0.550389, acc 0.875\n",
            "2019-04-11T10:07:40.263109: step 1149, loss 0.21734, acc 0.90625\n",
            "2019-04-11T10:07:40.482558: step 1150, loss 0.0391138, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:42.721348: step 1150, loss 0.169659, acc 0.9546\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1150\n",
            "\n",
            "2019-04-11T10:07:43.030162: step 1151, loss 0.0816904, acc 0.96875\n",
            "2019-04-11T10:07:43.256146: step 1152, loss 0.0998007, acc 1\n",
            "2019-04-11T10:07:43.473446: step 1153, loss 0.442795, acc 0.90625\n",
            "2019-04-11T10:07:43.700201: step 1154, loss 0.0392541, acc 1\n",
            "2019-04-11T10:07:43.924843: step 1155, loss 0.179314, acc 0.9375\n",
            "2019-04-11T10:07:44.144832: step 1156, loss 0.160278, acc 0.9375\n",
            "2019-04-11T10:07:44.361663: step 1157, loss 0.201199, acc 0.9375\n",
            "2019-04-11T10:07:44.582226: step 1158, loss 0.0724154, acc 1\n",
            "2019-04-11T10:07:44.802921: step 1159, loss 0.141468, acc 0.90625\n",
            "2019-04-11T10:07:45.021920: step 1160, loss 0.305807, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:47.259413: step 1160, loss 0.182749, acc 0.949\n",
            "\n",
            "2019-04-11T10:07:47.481166: step 1161, loss 0.247361, acc 0.9375\n",
            "2019-04-11T10:07:47.704160: step 1162, loss 0.251878, acc 0.9375\n",
            "2019-04-11T10:07:47.926424: step 1163, loss 0.164924, acc 0.96875\n",
            "2019-04-11T10:07:48.142468: step 1164, loss 0.294979, acc 0.90625\n",
            "2019-04-11T10:07:48.364565: step 1165, loss 0.06512, acc 1\n",
            "2019-04-11T10:07:48.586898: step 1166, loss 0.0666269, acc 1\n",
            "2019-04-11T10:07:48.806524: step 1167, loss 0.230527, acc 0.90625\n",
            "2019-04-11T10:07:49.028423: step 1168, loss 0.645989, acc 0.8125\n",
            "2019-04-11T10:07:49.246817: step 1169, loss 0.116482, acc 1\n",
            "2019-04-11T10:07:49.466590: step 1170, loss 0.202809, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:51.714501: step 1170, loss 0.180125, acc 0.948\n",
            "\n",
            "2019-04-11T10:07:51.935087: step 1171, loss 0.302499, acc 0.90625\n",
            "2019-04-11T10:07:52.148173: step 1172, loss 0.130663, acc 0.9375\n",
            "2019-04-11T10:07:52.366887: step 1173, loss 0.128232, acc 0.96875\n",
            "2019-04-11T10:07:52.580004: step 1174, loss 0.226451, acc 0.9375\n",
            "2019-04-11T10:07:52.806009: step 1175, loss 0.160844, acc 0.9375\n",
            "2019-04-11T10:07:53.032475: step 1176, loss 0.0701836, acc 1\n",
            "2019-04-11T10:07:53.251109: step 1177, loss 0.175826, acc 0.9375\n",
            "2019-04-11T10:07:53.472385: step 1178, loss 0.160029, acc 0.96875\n",
            "2019-04-11T10:07:53.688329: step 1179, loss 0.494612, acc 0.875\n",
            "2019-04-11T10:07:53.908348: step 1180, loss 0.166982, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:07:56.149275: step 1180, loss 0.213926, acc 0.9336\n",
            "\n",
            "2019-04-11T10:07:56.357871: step 1181, loss 0.455564, acc 0.875\n",
            "2019-04-11T10:07:56.583708: step 1182, loss 0.0927215, acc 0.96875\n",
            "2019-04-11T10:07:56.807736: step 1183, loss 0.0518234, acc 1\n",
            "2019-04-11T10:07:57.030028: step 1184, loss 0.0656318, acc 1\n",
            "2019-04-11T10:07:57.240473: step 1185, loss 0.230456, acc 0.9375\n",
            "2019-04-11T10:07:57.463829: step 1186, loss 0.172546, acc 0.9375\n",
            "2019-04-11T10:07:57.688500: step 1187, loss 0.0404348, acc 1\n",
            "2019-04-11T10:07:57.914822: step 1188, loss 0.0986769, acc 0.96875\n",
            "2019-04-11T10:07:58.132667: step 1189, loss 0.0625776, acc 1\n",
            "2019-04-11T10:07:58.349613: step 1190, loss 0.164873, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:00.592207: step 1190, loss 0.18879, acc 0.9496\n",
            "\n",
            "2019-04-11T10:08:00.809411: step 1191, loss 0.154495, acc 0.96875\n",
            "2019-04-11T10:08:01.033153: step 1192, loss 0.325085, acc 0.9375\n",
            "2019-04-11T10:08:01.254443: step 1193, loss 0.296266, acc 0.875\n",
            "2019-04-11T10:08:01.477199: step 1194, loss 0.159217, acc 0.96875\n",
            "2019-04-11T10:08:01.690522: step 1195, loss 0.140366, acc 0.96875\n",
            "2019-04-11T10:08:01.915607: step 1196, loss 0.104701, acc 0.96875\n",
            "2019-04-11T10:08:02.126474: step 1197, loss 0.0751054, acc 1\n",
            "2019-04-11T10:08:02.352672: step 1198, loss 0.0933179, acc 0.96875\n",
            "2019-04-11T10:08:02.563661: step 1199, loss 0.0744509, acc 1\n",
            "2019-04-11T10:08:02.784906: step 1200, loss 0.289228, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:05.023049: step 1200, loss 0.178354, acc 0.9526\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1200\n",
            "\n",
            "2019-04-11T10:08:05.328229: step 1201, loss 0.208559, acc 0.96875\n",
            "2019-04-11T10:08:05.547519: step 1202, loss 0.073925, acc 0.96875\n",
            "2019-04-11T10:08:05.754638: step 1203, loss 0.449687, acc 0.90625\n",
            "2019-04-11T10:08:05.969099: step 1204, loss 0.0697844, acc 1\n",
            "2019-04-11T10:08:06.181962: step 1205, loss 0.190143, acc 0.9375\n",
            "2019-04-11T10:08:06.401508: step 1206, loss 0.108666, acc 0.96875\n",
            "2019-04-11T10:08:06.622967: step 1207, loss 0.25186, acc 0.96875\n",
            "2019-04-11T10:08:06.848738: step 1208, loss 0.1411, acc 0.96875\n",
            "2019-04-11T10:08:07.073794: step 1209, loss 0.347872, acc 0.96875\n",
            "2019-04-11T10:08:07.301566: step 1210, loss 0.187483, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:09.540400: step 1210, loss 0.165056, acc 0.9526\n",
            "\n",
            "2019-04-11T10:08:09.756826: step 1211, loss 0.184575, acc 0.96875\n",
            "2019-04-11T10:08:09.975534: step 1212, loss 0.466567, acc 0.90625\n",
            "2019-04-11T10:08:10.194246: step 1213, loss 0.20003, acc 0.9375\n",
            "2019-04-11T10:08:10.415317: step 1214, loss 0.319472, acc 0.90625\n",
            "2019-04-11T10:08:10.638867: step 1215, loss 0.108102, acc 0.96875\n",
            "2019-04-11T10:08:10.862610: step 1216, loss 0.232222, acc 0.90625\n",
            "2019-04-11T10:08:11.081206: step 1217, loss 0.310388, acc 0.875\n",
            "2019-04-11T10:08:11.303811: step 1218, loss 0.189786, acc 0.9375\n",
            "2019-04-11T10:08:11.521896: step 1219, loss 0.395965, acc 0.875\n",
            "2019-04-11T10:08:11.752285: step 1220, loss 0.683917, acc 0.78125\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:13.986364: step 1220, loss 0.181244, acc 0.9494\n",
            "\n",
            "2019-04-11T10:08:14.211372: step 1221, loss 0.0719722, acc 1\n",
            "2019-04-11T10:08:14.439682: step 1222, loss 0.0927099, acc 1\n",
            "2019-04-11T10:08:14.665195: step 1223, loss 0.294905, acc 0.96875\n",
            "2019-04-11T10:08:14.887951: step 1224, loss 0.421827, acc 0.875\n",
            "2019-04-11T10:08:15.122667: step 1225, loss 0.0569213, acc 0.96875\n",
            "2019-04-11T10:08:15.347647: step 1226, loss 0.182148, acc 0.96875\n",
            "2019-04-11T10:08:15.567587: step 1227, loss 0.0866375, acc 0.96875\n",
            "2019-04-11T10:08:15.787637: step 1228, loss 0.246581, acc 0.96875\n",
            "2019-04-11T10:08:16.010114: step 1229, loss 0.0835635, acc 0.96875\n",
            "2019-04-11T10:08:16.232928: step 1230, loss 0.0869717, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:18.475459: step 1230, loss 0.176932, acc 0.9474\n",
            "\n",
            "2019-04-11T10:08:18.700630: step 1231, loss 0.20113, acc 0.96875\n",
            "2019-04-11T10:08:18.926342: step 1232, loss 0.195169, acc 0.90625\n",
            "2019-04-11T10:08:19.141081: step 1233, loss 0.363223, acc 0.9375\n",
            "2019-04-11T10:08:19.358474: step 1234, loss 0.150486, acc 0.96875\n",
            "2019-04-11T10:08:19.577661: step 1235, loss 0.12673, acc 0.9375\n",
            "2019-04-11T10:08:19.800261: step 1236, loss 0.215945, acc 0.875\n",
            "2019-04-11T10:08:20.020024: step 1237, loss 0.249795, acc 0.90625\n",
            "2019-04-11T10:08:20.237410: step 1238, loss 0.295178, acc 0.9375\n",
            "2019-04-11T10:08:20.461896: step 1239, loss 0.106235, acc 0.96875\n",
            "2019-04-11T10:08:20.682268: step 1240, loss 0.310691, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:22.925208: step 1240, loss 0.16148, acc 0.9548\n",
            "\n",
            "2019-04-11T10:08:23.149699: step 1241, loss 0.0808422, acc 0.96875\n",
            "2019-04-11T10:08:23.369563: step 1242, loss 0.194663, acc 0.9375\n",
            "2019-04-11T10:08:23.591602: step 1243, loss 0.0841362, acc 1\n",
            "2019-04-11T10:08:23.803471: step 1244, loss 0.0830205, acc 1\n",
            "2019-04-11T10:08:24.025910: step 1245, loss 0.061885, acc 1\n",
            "2019-04-11T10:08:24.237373: step 1246, loss 0.390079, acc 0.90625\n",
            "2019-04-11T10:08:24.456788: step 1247, loss 0.284741, acc 0.90625\n",
            "2019-04-11T10:08:24.680703: step 1248, loss 0.0610984, acc 1\n",
            "2019-04-11T10:08:24.900700: step 1249, loss 0.149748, acc 0.90625\n",
            "2019-04-11T10:08:25.119211: step 1250, loss 0.0872672, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:27.367669: step 1250, loss 0.180576, acc 0.9492\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1250\n",
            "\n",
            "2019-04-11T10:08:27.670403: step 1251, loss 0.13596, acc 0.96875\n",
            "2019-04-11T10:08:27.894199: step 1252, loss 0.135447, acc 0.96875\n",
            "2019-04-11T10:08:28.111568: step 1253, loss 0.161077, acc 0.9375\n",
            "2019-04-11T10:08:28.329788: step 1254, loss 0.138402, acc 0.96875\n",
            "2019-04-11T10:08:28.541904: step 1255, loss 0.133888, acc 0.9375\n",
            "2019-04-11T10:08:28.759124: step 1256, loss 0.212839, acc 0.90625\n",
            "2019-04-11T10:08:28.983011: step 1257, loss 0.325914, acc 0.875\n",
            "2019-04-11T10:08:29.203002: step 1258, loss 0.326403, acc 0.9375\n",
            "2019-04-11T10:08:29.425161: step 1259, loss 0.214911, acc 0.9375\n",
            "2019-04-11T10:08:29.641129: step 1260, loss 0.240999, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:31.877434: step 1260, loss 0.179285, acc 0.9474\n",
            "\n",
            "2019-04-11T10:08:32.098642: step 1261, loss 0.0390971, acc 1\n",
            "2019-04-11T10:08:32.314448: step 1262, loss 0.133769, acc 0.96875\n",
            "2019-04-11T10:08:32.535842: step 1263, loss 0.240407, acc 0.9375\n",
            "2019-04-11T10:08:32.759903: step 1264, loss 0.303867, acc 0.9375\n",
            "2019-04-11T10:08:32.982890: step 1265, loss 0.212882, acc 0.90625\n",
            "2019-04-11T10:08:33.205018: step 1266, loss 0.111274, acc 1\n",
            "2019-04-11T10:08:33.425161: step 1267, loss 0.34641, acc 0.90625\n",
            "2019-04-11T10:08:33.647010: step 1268, loss 0.0456376, acc 1\n",
            "2019-04-11T10:08:33.867029: step 1269, loss 0.153294, acc 0.96875\n",
            "2019-04-11T10:08:34.087406: step 1270, loss 0.227616, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:36.320305: step 1270, loss 0.165599, acc 0.9544\n",
            "\n",
            "2019-04-11T10:08:36.538627: step 1271, loss 0.228563, acc 0.96875\n",
            "2019-04-11T10:08:36.762787: step 1272, loss 0.104854, acc 0.96875\n",
            "2019-04-11T10:08:36.987496: step 1273, loss 0.08718, acc 0.96875\n",
            "2019-04-11T10:08:37.207280: step 1274, loss 0.201709, acc 0.9375\n",
            "2019-04-11T10:08:37.426886: step 1275, loss 0.35039, acc 0.90625\n",
            "2019-04-11T10:08:37.647809: step 1276, loss 0.330952, acc 0.9375\n",
            "2019-04-11T10:08:37.868513: step 1277, loss 0.162023, acc 0.96875\n",
            "2019-04-11T10:08:38.086914: step 1278, loss 0.181356, acc 0.96875\n",
            "2019-04-11T10:08:38.302741: step 1279, loss 0.441106, acc 0.875\n",
            "2019-04-11T10:08:38.522750: step 1280, loss 0.394577, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:40.761328: step 1280, loss 0.164636, acc 0.9522\n",
            "\n",
            "2019-04-11T10:08:40.985908: step 1281, loss 0.0588433, acc 1\n",
            "2019-04-11T10:08:41.207581: step 1282, loss 0.111223, acc 0.96875\n",
            "2019-04-11T10:08:41.428040: step 1283, loss 0.111872, acc 0.96875\n",
            "2019-04-11T10:08:41.642544: step 1284, loss 0.0845733, acc 1\n",
            "2019-04-11T10:08:41.855502: step 1285, loss 0.37945, acc 0.90625\n",
            "2019-04-11T10:08:42.073224: step 1286, loss 0.169239, acc 0.9375\n",
            "2019-04-11T10:08:42.295292: step 1287, loss 0.193812, acc 0.9375\n",
            "2019-04-11T10:08:42.515279: step 1288, loss 0.151048, acc 0.9375\n",
            "2019-04-11T10:08:42.739353: step 1289, loss 0.0843446, acc 0.96875\n",
            "2019-04-11T10:08:42.964114: step 1290, loss 0.421779, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:45.222130: step 1290, loss 0.198033, acc 0.9424\n",
            "\n",
            "2019-04-11T10:08:45.437788: step 1291, loss 0.668932, acc 0.90625\n",
            "2019-04-11T10:08:45.656944: step 1292, loss 0.0872897, acc 1\n",
            "2019-04-11T10:08:45.874820: step 1293, loss 0.286224, acc 0.96875\n",
            "2019-04-11T10:08:46.099892: step 1294, loss 0.12775, acc 0.9375\n",
            "2019-04-11T10:08:46.308892: step 1295, loss 0.045176, acc 1\n",
            "2019-04-11T10:08:46.529734: step 1296, loss 0.182693, acc 0.96875\n",
            "2019-04-11T10:08:46.744712: step 1297, loss 0.106013, acc 0.96875\n",
            "2019-04-11T10:08:46.966236: step 1298, loss 0.250523, acc 0.875\n",
            "2019-04-11T10:08:47.189329: step 1299, loss 0.0981327, acc 0.96875\n",
            "2019-04-11T10:08:47.406892: step 1300, loss 0.120384, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:49.642068: step 1300, loss 0.158332, acc 0.958\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1300\n",
            "\n",
            "2019-04-11T10:08:49.949092: step 1301, loss 0.0531219, acc 1\n",
            "2019-04-11T10:08:50.164286: step 1302, loss 0.25994, acc 0.875\n",
            "2019-04-11T10:08:50.382634: step 1303, loss 0.21152, acc 0.96875\n",
            "2019-04-11T10:08:50.602307: step 1304, loss 0.105769, acc 1\n",
            "2019-04-11T10:08:50.829644: step 1305, loss 0.262038, acc 0.875\n",
            "2019-04-11T10:08:51.049164: step 1306, loss 0.227622, acc 0.90625\n",
            "2019-04-11T10:08:51.268571: step 1307, loss 0.229028, acc 0.96875\n",
            "2019-04-11T10:08:51.487528: step 1308, loss 0.162923, acc 0.9375\n",
            "2019-04-11T10:08:51.698313: step 1309, loss 0.160162, acc 0.9375\n",
            "2019-04-11T10:08:51.919572: step 1310, loss 0.103521, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:54.197682: step 1310, loss 0.177319, acc 0.9528\n",
            "\n",
            "2019-04-11T10:08:54.406750: step 1311, loss 0.171595, acc 0.96875\n",
            "2019-04-11T10:08:54.626918: step 1312, loss 0.317254, acc 0.875\n",
            "2019-04-11T10:08:54.845363: step 1313, loss 0.124993, acc 0.96875\n",
            "2019-04-11T10:08:55.064399: step 1314, loss 0.256052, acc 0.9375\n",
            "2019-04-11T10:08:55.285844: step 1315, loss 0.231909, acc 0.90625\n",
            "2019-04-11T10:08:55.497936: step 1316, loss 0.168257, acc 0.9375\n",
            "2019-04-11T10:08:55.718265: step 1317, loss 0.250044, acc 0.9375\n",
            "2019-04-11T10:08:55.938748: step 1318, loss 0.159759, acc 0.9375\n",
            "2019-04-11T10:08:56.156575: step 1319, loss 0.232975, acc 0.875\n",
            "2019-04-11T10:08:56.376226: step 1320, loss 0.150623, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:08:58.619303: step 1320, loss 0.171223, acc 0.9556\n",
            "\n",
            "2019-04-11T10:08:58.842706: step 1321, loss 0.141523, acc 0.9375\n",
            "2019-04-11T10:08:59.065710: step 1322, loss 0.13905, acc 0.9375\n",
            "2019-04-11T10:08:59.282112: step 1323, loss 0.21208, acc 0.96875\n",
            "2019-04-11T10:08:59.504312: step 1324, loss 0.150959, acc 0.9375\n",
            "2019-04-11T10:08:59.720193: step 1325, loss 0.164575, acc 0.9375\n",
            "2019-04-11T10:08:59.938284: step 1326, loss 0.287931, acc 0.9375\n",
            "2019-04-11T10:09:00.156248: step 1327, loss 0.164477, acc 0.9375\n",
            "2019-04-11T10:09:00.374713: step 1328, loss 0.0590789, acc 1\n",
            "2019-04-11T10:09:00.594887: step 1329, loss 0.170261, acc 0.96875\n",
            "2019-04-11T10:09:00.813546: step 1330, loss 0.200764, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:03.124985: step 1330, loss 0.167785, acc 0.9516\n",
            "\n",
            "2019-04-11T10:09:03.350132: step 1331, loss 0.214952, acc 0.9375\n",
            "2019-04-11T10:09:03.582590: step 1332, loss 0.0840138, acc 1\n",
            "2019-04-11T10:09:03.818146: step 1333, loss 0.404778, acc 0.90625\n",
            "2019-04-11T10:09:04.047006: step 1334, loss 0.0711374, acc 1\n",
            "2019-04-11T10:09:04.274912: step 1335, loss 0.104321, acc 0.96875\n",
            "2019-04-11T10:09:04.501255: step 1336, loss 0.101353, acc 0.96875\n",
            "2019-04-11T10:09:04.734592: step 1337, loss 0.114529, acc 0.9375\n",
            "2019-04-11T10:09:04.966065: step 1338, loss 0.0980459, acc 0.96875\n",
            "2019-04-11T10:09:05.194052: step 1339, loss 0.201945, acc 0.96875\n",
            "2019-04-11T10:09:05.421688: step 1340, loss 0.0855925, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:07.687302: step 1340, loss 0.173795, acc 0.9512\n",
            "\n",
            "2019-04-11T10:09:07.915907: step 1341, loss 0.0771251, acc 1\n",
            "2019-04-11T10:09:08.139966: step 1342, loss 0.104802, acc 0.96875\n",
            "2019-04-11T10:09:08.365477: step 1343, loss 0.128797, acc 0.96875\n",
            "2019-04-11T10:09:08.588623: step 1344, loss 0.179395, acc 0.96875\n",
            "2019-04-11T10:09:08.815602: step 1345, loss 0.190287, acc 0.9375\n",
            "2019-04-11T10:09:09.035531: step 1346, loss 0.0352301, acc 1\n",
            "2019-04-11T10:09:09.245207: step 1347, loss 0.0648607, acc 1\n",
            "2019-04-11T10:09:09.471466: step 1348, loss 0.149757, acc 0.9375\n",
            "2019-04-11T10:09:09.693730: step 1349, loss 0.223288, acc 0.96875\n",
            "2019-04-11T10:09:09.915109: step 1350, loss 0.366096, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:12.158413: step 1350, loss 0.164955, acc 0.9518\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1350\n",
            "\n",
            "2019-04-11T10:09:12.469171: step 1351, loss 0.060682, acc 1\n",
            "2019-04-11T10:09:12.693616: step 1352, loss 0.0710168, acc 0.96875\n",
            "2019-04-11T10:09:12.918143: step 1353, loss 0.190635, acc 0.9375\n",
            "2019-04-11T10:09:13.139628: step 1354, loss 0.0755531, acc 1\n",
            "2019-04-11T10:09:13.356324: step 1355, loss 0.173115, acc 0.90625\n",
            "2019-04-11T10:09:13.579324: step 1356, loss 0.0926674, acc 0.96875\n",
            "2019-04-11T10:09:13.799517: step 1357, loss 0.0245467, acc 1\n",
            "2019-04-11T10:09:14.021589: step 1358, loss 0.25144, acc 0.90625\n",
            "2019-04-11T10:09:14.242394: step 1359, loss 0.0456906, acc 1\n",
            "2019-04-11T10:09:14.463094: step 1360, loss 0.0488615, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:16.741394: step 1360, loss 0.157536, acc 0.9548\n",
            "\n",
            "2019-04-11T10:09:16.969090: step 1361, loss 0.144347, acc 0.96875\n",
            "2019-04-11T10:09:17.187577: step 1362, loss 0.135425, acc 0.9375\n",
            "2019-04-11T10:09:17.406465: step 1363, loss 0.274897, acc 0.90625\n",
            "2019-04-11T10:09:17.626575: step 1364, loss 0.0425441, acc 1\n",
            "2019-04-11T10:09:17.837740: step 1365, loss 0.0393703, acc 1\n",
            "2019-04-11T10:09:18.061261: step 1366, loss 0.499608, acc 0.90625\n",
            "2019-04-11T10:09:18.274237: step 1367, loss 0.227007, acc 0.9375\n",
            "2019-04-11T10:09:18.493055: step 1368, loss 0.14046, acc 0.90625\n",
            "2019-04-11T10:09:18.717556: step 1369, loss 0.0475733, acc 1\n",
            "2019-04-11T10:09:18.954249: step 1370, loss 0.0997192, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:21.197917: step 1370, loss 0.156279, acc 0.959\n",
            "\n",
            "2019-04-11T10:09:21.414567: step 1371, loss 0.049586, acc 0.96875\n",
            "2019-04-11T10:09:21.633593: step 1372, loss 0.0409504, acc 1\n",
            "2019-04-11T10:09:21.851862: step 1373, loss 0.0835136, acc 0.96875\n",
            "2019-04-11T10:09:22.069358: step 1374, loss 0.107391, acc 1\n",
            "2019-04-11T10:09:22.286026: step 1375, loss 0.11754, acc 0.96875\n",
            "2019-04-11T10:09:22.501255: step 1376, loss 0.264467, acc 0.90625\n",
            "2019-04-11T10:09:22.724795: step 1377, loss 0.0663548, acc 1\n",
            "2019-04-11T10:09:22.945429: step 1378, loss 0.0770845, acc 1\n",
            "2019-04-11T10:09:23.166435: step 1379, loss 0.16717, acc 0.96875\n",
            "2019-04-11T10:09:23.378151: step 1380, loss 0.159302, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:25.620981: step 1380, loss 0.15586, acc 0.9576\n",
            "\n",
            "2019-04-11T10:09:25.843126: step 1381, loss 0.376996, acc 0.90625\n",
            "2019-04-11T10:09:26.061856: step 1382, loss 0.065945, acc 1\n",
            "2019-04-11T10:09:26.279070: step 1383, loss 0.0966789, acc 0.96875\n",
            "2019-04-11T10:09:26.497270: step 1384, loss 0.129015, acc 0.96875\n",
            "2019-04-11T10:09:26.721137: step 1385, loss 0.460878, acc 0.90625\n",
            "2019-04-11T10:09:26.945859: step 1386, loss 0.0558094, acc 1\n",
            "2019-04-11T10:09:27.165594: step 1387, loss 0.0666976, acc 1\n",
            "2019-04-11T10:09:27.385417: step 1388, loss 0.158577, acc 0.9375\n",
            "2019-04-11T10:09:27.608690: step 1389, loss 0.208416, acc 0.90625\n",
            "2019-04-11T10:09:27.828990: step 1390, loss 0.198286, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:30.065445: step 1390, loss 0.157509, acc 0.9558\n",
            "\n",
            "2019-04-11T10:09:30.286227: step 1391, loss 0.126371, acc 0.96875\n",
            "2019-04-11T10:09:30.506386: step 1392, loss 0.126443, acc 0.96875\n",
            "2019-04-11T10:09:30.731577: step 1393, loss 0.0535319, acc 0.96875\n",
            "2019-04-11T10:09:30.951296: step 1394, loss 0.0185003, acc 1\n",
            "2019-04-11T10:09:31.172250: step 1395, loss 0.17564, acc 0.96875\n",
            "2019-04-11T10:09:31.390225: step 1396, loss 0.161894, acc 0.90625\n",
            "2019-04-11T10:09:31.611688: step 1397, loss 0.134649, acc 0.9375\n",
            "2019-04-11T10:09:31.833544: step 1398, loss 0.160938, acc 0.9375\n",
            "2019-04-11T10:09:32.051180: step 1399, loss 0.237984, acc 0.875\n",
            "2019-04-11T10:09:32.267129: step 1400, loss 0.0874123, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:34.518644: step 1400, loss 0.166682, acc 0.9542\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1400\n",
            "\n",
            "2019-04-11T10:09:34.830657: step 1401, loss 0.0745469, acc 0.96875\n",
            "2019-04-11T10:09:35.051860: step 1402, loss 0.191882, acc 0.9375\n",
            "2019-04-11T10:09:35.274664: step 1403, loss 0.079209, acc 1\n",
            "2019-04-11T10:09:35.496128: step 1404, loss 0.228991, acc 0.9375\n",
            "2019-04-11T10:09:35.721854: step 1405, loss 0.148025, acc 0.96875\n",
            "2019-04-11T10:09:35.942801: step 1406, loss 0.145544, acc 0.96875\n",
            "2019-04-11T10:09:36.169461: step 1407, loss 0.142057, acc 0.96875\n",
            "2019-04-11T10:09:36.392533: step 1408, loss 0.166108, acc 0.96875\n",
            "2019-04-11T10:09:36.608239: step 1409, loss 0.244, acc 0.9375\n",
            "2019-04-11T10:09:36.837700: step 1410, loss 0.166883, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:39.076269: step 1410, loss 0.1562, acc 0.9576\n",
            "\n",
            "2019-04-11T10:09:39.292329: step 1411, loss 0.427045, acc 0.8125\n",
            "2019-04-11T10:09:39.514696: step 1412, loss 0.292857, acc 0.9375\n",
            "2019-04-11T10:09:39.736059: step 1413, loss 0.128042, acc 0.96875\n",
            "2019-04-11T10:09:39.955715: step 1414, loss 0.0586716, acc 1\n",
            "2019-04-11T10:09:40.175088: step 1415, loss 0.123585, acc 0.96875\n",
            "2019-04-11T10:09:40.394593: step 1416, loss 0.0493688, acc 1\n",
            "2019-04-11T10:09:40.613670: step 1417, loss 0.327915, acc 0.875\n",
            "2019-04-11T10:09:40.841516: step 1418, loss 0.206739, acc 0.9375\n",
            "2019-04-11T10:09:41.066118: step 1419, loss 0.116786, acc 0.96875\n",
            "2019-04-11T10:09:41.284865: step 1420, loss 0.086445, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:43.524407: step 1420, loss 0.159017, acc 0.9566\n",
            "\n",
            "2019-04-11T10:09:43.740970: step 1421, loss 0.399113, acc 0.875\n",
            "2019-04-11T10:09:43.963776: step 1422, loss 0.172012, acc 0.9375\n",
            "2019-04-11T10:09:44.193277: step 1423, loss 0.0818518, acc 0.96875\n",
            "2019-04-11T10:09:44.421929: step 1424, loss 0.146603, acc 0.9375\n",
            "2019-04-11T10:09:44.645016: step 1425, loss 0.0931186, acc 1\n",
            "2019-04-11T10:09:44.871698: step 1426, loss 0.0302198, acc 1\n",
            "2019-04-11T10:09:45.092462: step 1427, loss 0.284523, acc 0.9375\n",
            "2019-04-11T10:09:45.314421: step 1428, loss 0.203862, acc 0.90625\n",
            "2019-04-11T10:09:45.536893: step 1429, loss 0.232659, acc 0.90625\n",
            "2019-04-11T10:09:45.755977: step 1430, loss 0.195999, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:47.993673: step 1430, loss 0.151933, acc 0.9588\n",
            "\n",
            "2019-04-11T10:09:48.213127: step 1431, loss 0.311668, acc 0.9375\n",
            "2019-04-11T10:09:48.430036: step 1432, loss 0.187757, acc 0.9375\n",
            "2019-04-11T10:09:48.647399: step 1433, loss 0.186561, acc 0.90625\n",
            "2019-04-11T10:09:48.869541: step 1434, loss 0.0565064, acc 1\n",
            "2019-04-11T10:09:49.094628: step 1435, loss 0.0877794, acc 0.96875\n",
            "2019-04-11T10:09:49.315517: step 1436, loss 0.101734, acc 0.96875\n",
            "2019-04-11T10:09:49.543044: step 1437, loss 0.0931618, acc 1\n",
            "2019-04-11T10:09:49.763942: step 1438, loss 0.120721, acc 0.96875\n",
            "2019-04-11T10:09:49.992201: step 1439, loss 0.187916, acc 0.90625\n",
            "2019-04-11T10:09:50.214432: step 1440, loss 0.300189, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:52.458916: step 1440, loss 0.161464, acc 0.954\n",
            "\n",
            "2019-04-11T10:09:52.695934: step 1441, loss 0.103625, acc 0.96875\n",
            "2019-04-11T10:09:52.929471: step 1442, loss 0.0796265, acc 1\n",
            "2019-04-11T10:09:53.158394: step 1443, loss 0.074295, acc 0.96875\n",
            "2019-04-11T10:09:53.397056: step 1444, loss 0.15056, acc 0.96875\n",
            "2019-04-11T10:09:53.633988: step 1445, loss 0.348354, acc 0.9375\n",
            "2019-04-11T10:09:53.873111: step 1446, loss 0.393846, acc 0.9375\n",
            "2019-04-11T10:09:54.112996: step 1447, loss 0.194772, acc 0.9375\n",
            "2019-04-11T10:09:54.342932: step 1448, loss 0.397183, acc 0.90625\n",
            "2019-04-11T10:09:54.567605: step 1449, loss 0.185073, acc 0.96875\n",
            "2019-04-11T10:09:54.828967: step 1450, loss 0.237135, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:09:57.158790: step 1450, loss 0.158995, acc 0.9552\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1450\n",
            "\n",
            "2019-04-11T10:09:57.501256: step 1451, loss 0.105157, acc 1\n",
            "2019-04-11T10:09:57.740506: step 1452, loss 0.208391, acc 0.96875\n",
            "2019-04-11T10:09:57.973907: step 1453, loss 0.266382, acc 0.9375\n",
            "2019-04-11T10:09:58.211903: step 1454, loss 0.151052, acc 0.9375\n",
            "2019-04-11T10:09:58.450861: step 1455, loss 0.13313, acc 0.96875\n",
            "2019-04-11T10:09:58.684366: step 1456, loss 0.215846, acc 0.96875\n",
            "2019-04-11T10:09:58.913568: step 1457, loss 0.100874, acc 0.96875\n",
            "2019-04-11T10:09:59.151910: step 1458, loss 0.0996271, acc 0.96875\n",
            "2019-04-11T10:09:59.389973: step 1459, loss 0.14969, acc 0.96875\n",
            "2019-04-11T10:09:59.626348: step 1460, loss 0.422393, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:01.959582: step 1460, loss 0.171882, acc 0.9488\n",
            "\n",
            "2019-04-11T10:10:02.196663: step 1461, loss 0.100426, acc 0.96875\n",
            "2019-04-11T10:10:02.428584: step 1462, loss 0.253764, acc 0.90625\n",
            "2019-04-11T10:10:02.662352: step 1463, loss 0.152758, acc 0.9375\n",
            "2019-04-11T10:10:02.894979: step 1464, loss 0.114475, acc 0.9375\n",
            "2019-04-11T10:10:03.111752: step 1465, loss 0.188837, acc 0.9375\n",
            "2019-04-11T10:10:03.335006: step 1466, loss 0.187945, acc 0.9375\n",
            "2019-04-11T10:10:03.558018: step 1467, loss 0.428299, acc 0.9375\n",
            "2019-04-11T10:10:03.782304: step 1468, loss 0.241691, acc 0.90625\n",
            "2019-04-11T10:10:04.004906: step 1469, loss 0.176349, acc 0.90625\n",
            "2019-04-11T10:10:04.220289: step 1470, loss 0.381765, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:06.454447: step 1470, loss 0.154552, acc 0.9548\n",
            "\n",
            "2019-04-11T10:10:06.678130: step 1471, loss 0.057294, acc 1\n",
            "2019-04-11T10:10:06.900534: step 1472, loss 0.191614, acc 0.9375\n",
            "2019-04-11T10:10:07.117946: step 1473, loss 0.211519, acc 0.9375\n",
            "2019-04-11T10:10:07.336891: step 1474, loss 0.231714, acc 0.90625\n",
            "2019-04-11T10:10:07.558890: step 1475, loss 0.141698, acc 0.96875\n",
            "2019-04-11T10:10:07.773138: step 1476, loss 0.328418, acc 0.90625\n",
            "2019-04-11T10:10:07.996503: step 1477, loss 0.0981644, acc 1\n",
            "2019-04-11T10:10:08.218275: step 1478, loss 0.204662, acc 0.90625\n",
            "2019-04-11T10:10:08.440193: step 1479, loss 0.173888, acc 0.90625\n",
            "2019-04-11T10:10:08.655745: step 1480, loss 0.286874, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:10.897435: step 1480, loss 0.161434, acc 0.9566\n",
            "\n",
            "2019-04-11T10:10:11.115254: step 1481, loss 0.408732, acc 0.875\n",
            "2019-04-11T10:10:11.336489: step 1482, loss 0.102025, acc 0.9375\n",
            "2019-04-11T10:10:11.560887: step 1483, loss 0.240096, acc 0.875\n",
            "2019-04-11T10:10:11.785709: step 1484, loss 0.228905, acc 0.96875\n",
            "2019-04-11T10:10:12.012070: step 1485, loss 0.289021, acc 0.90625\n",
            "2019-04-11T10:10:12.235112: step 1486, loss 0.11038, acc 0.96875\n",
            "2019-04-11T10:10:12.448950: step 1487, loss 0.280059, acc 0.96875\n",
            "2019-04-11T10:10:12.672550: step 1488, loss 0.120814, acc 0.96875\n",
            "2019-04-11T10:10:12.898819: step 1489, loss 0.0885678, acc 0.96875\n",
            "2019-04-11T10:10:13.119419: step 1490, loss 0.226615, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:15.364075: step 1490, loss 0.167975, acc 0.9494\n",
            "\n",
            "2019-04-11T10:10:15.584942: step 1491, loss 0.202159, acc 0.90625\n",
            "2019-04-11T10:10:15.804054: step 1492, loss 0.219244, acc 0.90625\n",
            "2019-04-11T10:10:16.024151: step 1493, loss 0.28178, acc 0.90625\n",
            "2019-04-11T10:10:16.244751: step 1494, loss 0.293704, acc 0.9375\n",
            "2019-04-11T10:10:16.482425: step 1495, loss 0.359072, acc 0.875\n",
            "2019-04-11T10:10:16.702790: step 1496, loss 0.0530358, acc 1\n",
            "2019-04-11T10:10:16.922541: step 1497, loss 0.0275974, acc 1\n",
            "2019-04-11T10:10:17.146465: step 1498, loss 0.297508, acc 0.9375\n",
            "2019-04-11T10:10:17.365956: step 1499, loss 0.207863, acc 0.9375\n",
            "2019-04-11T10:10:17.590333: step 1500, loss 0.176916, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:19.826432: step 1500, loss 0.159128, acc 0.9534\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1500\n",
            "\n",
            "2019-04-11T10:10:20.135575: step 1501, loss 0.0534027, acc 1\n",
            "2019-04-11T10:10:20.384436: step 1502, loss 0.145983, acc 0.9375\n",
            "2019-04-11T10:10:20.610035: step 1503, loss 0.462622, acc 0.9375\n",
            "2019-04-11T10:10:20.853190: step 1504, loss 0.298831, acc 0.90625\n",
            "2019-04-11T10:10:21.081705: step 1505, loss 0.118296, acc 0.96875\n",
            "2019-04-11T10:10:21.307455: step 1506, loss 0.0992915, acc 0.96875\n",
            "2019-04-11T10:10:21.526276: step 1507, loss 0.0464366, acc 1\n",
            "2019-04-11T10:10:21.749364: step 1508, loss 0.158976, acc 0.90625\n",
            "2019-04-11T10:10:21.968822: step 1509, loss 0.130475, acc 0.96875\n",
            "2019-04-11T10:10:22.190394: step 1510, loss 0.134432, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:24.427811: step 1510, loss 0.170383, acc 0.9542\n",
            "\n",
            "2019-04-11T10:10:24.655123: step 1511, loss 0.24751, acc 0.9375\n",
            "2019-04-11T10:10:24.877774: step 1512, loss 0.274669, acc 0.96875\n",
            "2019-04-11T10:10:25.097883: step 1513, loss 0.0392319, acc 1\n",
            "2019-04-11T10:10:25.319785: step 1514, loss 0.272658, acc 0.9375\n",
            "2019-04-11T10:10:25.543030: step 1515, loss 0.148839, acc 0.96875\n",
            "2019-04-11T10:10:25.765482: step 1516, loss 0.123906, acc 0.9375\n",
            "2019-04-11T10:10:25.987546: step 1517, loss 0.118598, acc 0.96875\n",
            "2019-04-11T10:10:26.208135: step 1518, loss 0.0905384, acc 1\n",
            "2019-04-11T10:10:26.429874: step 1519, loss 0.0866733, acc 0.96875\n",
            "2019-04-11T10:10:26.646546: step 1520, loss 0.424005, acc 0.875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:28.886338: step 1520, loss 0.1578, acc 0.9548\n",
            "\n",
            "2019-04-11T10:10:29.095018: step 1521, loss 0.483693, acc 0.84375\n",
            "2019-04-11T10:10:29.319708: step 1522, loss 0.222228, acc 0.90625\n",
            "2019-04-11T10:10:29.540772: step 1523, loss 0.0945053, acc 0.96875\n",
            "2019-04-11T10:10:29.764971: step 1524, loss 0.0380052, acc 1\n",
            "2019-04-11T10:10:29.987892: step 1525, loss 0.112792, acc 0.9375\n",
            "2019-04-11T10:10:30.213377: step 1526, loss 0.152386, acc 0.96875\n",
            "2019-04-11T10:10:30.437546: step 1527, loss 0.305707, acc 0.90625\n",
            "2019-04-11T10:10:30.659720: step 1528, loss 0.0449538, acc 1\n",
            "2019-04-11T10:10:30.885201: step 1529, loss 0.100771, acc 0.96875\n",
            "2019-04-11T10:10:31.108854: step 1530, loss 0.0535282, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:33.352448: step 1530, loss 0.146477, acc 0.9594\n",
            "\n",
            "2019-04-11T10:10:33.570251: step 1531, loss 0.0297339, acc 1\n",
            "2019-04-11T10:10:33.794313: step 1532, loss 0.0380108, acc 1\n",
            "2019-04-11T10:10:34.019604: step 1533, loss 0.323489, acc 0.9375\n",
            "2019-04-11T10:10:34.238887: step 1534, loss 0.0469801, acc 1\n",
            "2019-04-11T10:10:34.466395: step 1535, loss 0.0483487, acc 1\n",
            "2019-04-11T10:10:34.689577: step 1536, loss 0.103586, acc 1\n",
            "2019-04-11T10:10:34.910801: step 1537, loss 0.0902364, acc 0.96875\n",
            "2019-04-11T10:10:35.135824: step 1538, loss 0.152159, acc 0.9375\n",
            "2019-04-11T10:10:35.361308: step 1539, loss 0.0961939, acc 0.96875\n",
            "2019-04-11T10:10:35.587119: step 1540, loss 0.144079, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:37.830612: step 1540, loss 0.158525, acc 0.9546\n",
            "\n",
            "2019-04-11T10:10:38.056921: step 1541, loss 0.0526515, acc 1\n",
            "2019-04-11T10:10:38.283479: step 1542, loss 0.130485, acc 0.9375\n",
            "2019-04-11T10:10:38.507539: step 1543, loss 0.15361, acc 0.9375\n",
            "2019-04-11T10:10:38.728168: step 1544, loss 0.0639391, acc 1\n",
            "2019-04-11T10:10:38.954320: step 1545, loss 0.17938, acc 0.90625\n",
            "2019-04-11T10:10:39.181268: step 1546, loss 0.0912254, acc 0.96875\n",
            "2019-04-11T10:10:39.398160: step 1547, loss 0.199667, acc 0.9375\n",
            "2019-04-11T10:10:39.617014: step 1548, loss 0.336249, acc 0.9375\n",
            "2019-04-11T10:10:39.838676: step 1549, loss 0.322851, acc 0.90625\n",
            "2019-04-11T10:10:40.053690: step 1550, loss 0.195479, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:42.290724: step 1550, loss 0.150541, acc 0.9588\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1550\n",
            "\n",
            "2019-04-11T10:10:42.598751: step 1551, loss 0.0456365, acc 1\n",
            "2019-04-11T10:10:42.822621: step 1552, loss 0.17945, acc 0.9375\n",
            "2019-04-11T10:10:43.037808: step 1553, loss 0.161306, acc 0.96875\n",
            "2019-04-11T10:10:43.260424: step 1554, loss 0.296612, acc 0.9375\n",
            "2019-04-11T10:10:43.474834: step 1555, loss 0.278884, acc 0.90625\n",
            "2019-04-11T10:10:43.690974: step 1556, loss 0.265685, acc 0.84375\n",
            "2019-04-11T10:10:43.915289: step 1557, loss 0.184193, acc 0.9375\n",
            "2019-04-11T10:10:44.137808: step 1558, loss 0.297291, acc 0.9375\n",
            "2019-04-11T10:10:44.360290: step 1559, loss 0.216748, acc 0.9375\n",
            "2019-04-11T10:10:44.583567: step 1560, loss 0.158378, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:46.841089: step 1560, loss 0.162909, acc 0.9528\n",
            "\n",
            "2019-04-11T10:10:47.064027: step 1561, loss 0.0483606, acc 1\n",
            "2019-04-11T10:10:47.283364: step 1562, loss 0.158737, acc 0.96875\n",
            "2019-04-11T10:10:47.502904: step 1563, loss 0.071809, acc 1\n",
            "2019-04-11T10:10:47.730517: step 1564, loss 0.0299123, acc 1\n",
            "2019-04-11T10:10:47.955909: step 1565, loss 0.191318, acc 0.96875\n",
            "2019-04-11T10:10:48.180420: step 1566, loss 0.076136, acc 0.96875\n",
            "2019-04-11T10:10:48.406846: step 1567, loss 0.196381, acc 0.9375\n",
            "2019-04-11T10:10:48.629745: step 1568, loss 0.363653, acc 0.875\n",
            "2019-04-11T10:10:48.850580: step 1569, loss 0.0826878, acc 0.9375\n",
            "2019-04-11T10:10:49.061167: step 1570, loss 0.119514, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:51.302323: step 1570, loss 0.154011, acc 0.956\n",
            "\n",
            "2019-04-11T10:10:51.522691: step 1571, loss 0.124689, acc 0.96875\n",
            "2019-04-11T10:10:51.745212: step 1572, loss 0.214827, acc 0.9375\n",
            "2019-04-11T10:10:51.969923: step 1573, loss 0.102974, acc 0.9375\n",
            "2019-04-11T10:10:52.186035: step 1574, loss 0.112256, acc 0.96875\n",
            "2019-04-11T10:10:52.404889: step 1575, loss 0.196554, acc 0.9375\n",
            "2019-04-11T10:10:52.628004: step 1576, loss 0.252231, acc 0.90625\n",
            "2019-04-11T10:10:52.852832: step 1577, loss 0.0551793, acc 1\n",
            "2019-04-11T10:10:53.077560: step 1578, loss 0.0860487, acc 0.96875\n",
            "2019-04-11T10:10:53.298864: step 1579, loss 0.236523, acc 0.9375\n",
            "2019-04-11T10:10:53.518103: step 1580, loss 0.172197, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:10:55.754357: step 1580, loss 0.152385, acc 0.9584\n",
            "\n",
            "2019-04-11T10:10:55.970960: step 1581, loss 0.0485231, acc 1\n",
            "2019-04-11T10:10:56.205747: step 1582, loss 0.120738, acc 0.9375\n",
            "2019-04-11T10:10:56.430231: step 1583, loss 0.283718, acc 0.9375\n",
            "2019-04-11T10:10:56.649679: step 1584, loss 0.326529, acc 0.9375\n",
            "2019-04-11T10:10:56.873150: step 1585, loss 0.0496833, acc 1\n",
            "2019-04-11T10:10:57.093144: step 1586, loss 0.482235, acc 0.90625\n",
            "2019-04-11T10:10:57.304422: step 1587, loss 0.0772944, acc 1\n",
            "2019-04-11T10:10:57.525947: step 1588, loss 0.116167, acc 0.96875\n",
            "2019-04-11T10:10:57.739320: step 1589, loss 0.202906, acc 0.90625\n",
            "2019-04-11T10:10:57.963685: step 1590, loss 0.112809, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:00.209050: step 1590, loss 0.149366, acc 0.9562\n",
            "\n",
            "2019-04-11T10:11:00.427001: step 1591, loss 0.129829, acc 0.96875\n",
            "2019-04-11T10:11:00.647204: step 1592, loss 0.392509, acc 0.78125\n",
            "2019-04-11T10:11:00.872686: step 1593, loss 0.0974889, acc 1\n",
            "2019-04-11T10:11:01.096963: step 1594, loss 0.157774, acc 0.9375\n",
            "2019-04-11T10:11:01.313033: step 1595, loss 0.439416, acc 0.90625\n",
            "2019-04-11T10:11:01.534679: step 1596, loss 0.10574, acc 0.96875\n",
            "2019-04-11T10:11:01.762881: step 1597, loss 0.161341, acc 0.9375\n",
            "2019-04-11T10:11:01.987584: step 1598, loss 0.0727627, acc 0.96875\n",
            "2019-04-11T10:11:02.210058: step 1599, loss 0.0244437, acc 1\n",
            "2019-04-11T10:11:02.420470: step 1600, loss 0.141433, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:04.661457: step 1600, loss 0.1535, acc 0.9564\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1600\n",
            "\n",
            "2019-04-11T10:11:04.970623: step 1601, loss 0.191036, acc 0.90625\n",
            "2019-04-11T10:11:05.196908: step 1602, loss 0.124074, acc 0.9375\n",
            "2019-04-11T10:11:05.416745: step 1603, loss 0.231579, acc 0.9375\n",
            "2019-04-11T10:11:05.637207: step 1604, loss 0.057895, acc 1\n",
            "2019-04-11T10:11:05.851172: step 1605, loss 0.08356, acc 1\n",
            "2019-04-11T10:11:06.075467: step 1606, loss 0.202826, acc 0.90625\n",
            "2019-04-11T10:11:06.295451: step 1607, loss 0.1212, acc 0.96875\n",
            "2019-04-11T10:11:06.518527: step 1608, loss 0.217798, acc 0.90625\n",
            "2019-04-11T10:11:06.743827: step 1609, loss 0.303666, acc 0.9375\n",
            "2019-04-11T10:11:06.971561: step 1610, loss 0.173629, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:09.214800: step 1610, loss 0.147486, acc 0.9578\n",
            "\n",
            "2019-04-11T10:11:09.433703: step 1611, loss 0.0449966, acc 1\n",
            "2019-04-11T10:11:09.650416: step 1612, loss 0.0943094, acc 1\n",
            "2019-04-11T10:11:09.869251: step 1613, loss 0.149031, acc 0.96875\n",
            "2019-04-11T10:11:10.090838: step 1614, loss 0.0776656, acc 1\n",
            "2019-04-11T10:11:10.310562: step 1615, loss 0.397901, acc 0.875\n",
            "2019-04-11T10:11:10.537590: step 1616, loss 0.372749, acc 0.875\n",
            "2019-04-11T10:11:10.759735: step 1617, loss 0.127176, acc 0.96875\n",
            "2019-04-11T10:11:10.987783: step 1618, loss 0.110897, acc 0.96875\n",
            "2019-04-11T10:11:11.210906: step 1619, loss 0.0187296, acc 1\n",
            "2019-04-11T10:11:11.432066: step 1620, loss 0.157112, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:13.676098: step 1620, loss 0.172901, acc 0.9498\n",
            "\n",
            "2019-04-11T10:11:13.894332: step 1621, loss 0.444626, acc 0.84375\n",
            "2019-04-11T10:11:14.110943: step 1622, loss 0.0571221, acc 1\n",
            "2019-04-11T10:11:14.327069: step 1623, loss 0.256172, acc 0.90625\n",
            "2019-04-11T10:11:14.546610: step 1624, loss 0.117346, acc 0.9375\n",
            "2019-04-11T10:11:14.767604: step 1625, loss 0.183447, acc 0.96875\n",
            "2019-04-11T10:11:14.993141: step 1626, loss 0.260211, acc 0.9375\n",
            "2019-04-11T10:11:15.212856: step 1627, loss 0.0622945, acc 1\n",
            "2019-04-11T10:11:15.435163: step 1628, loss 0.0583152, acc 1\n",
            "2019-04-11T10:11:15.662137: step 1629, loss 0.122845, acc 0.96875\n",
            "2019-04-11T10:11:15.880883: step 1630, loss 0.126061, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:18.213287: step 1630, loss 0.145505, acc 0.9612\n",
            "\n",
            "2019-04-11T10:11:18.431403: step 1631, loss 0.150518, acc 0.96875\n",
            "2019-04-11T10:11:18.651631: step 1632, loss 0.236887, acc 0.9375\n",
            "2019-04-11T10:11:18.876865: step 1633, loss 0.123363, acc 0.96875\n",
            "2019-04-11T10:11:19.098783: step 1634, loss 0.242419, acc 0.875\n",
            "2019-04-11T10:11:19.317655: step 1635, loss 0.281203, acc 0.84375\n",
            "2019-04-11T10:11:19.539968: step 1636, loss 0.274344, acc 0.9375\n",
            "2019-04-11T10:11:19.765212: step 1637, loss 0.0864702, acc 0.96875\n",
            "2019-04-11T10:11:19.979118: step 1638, loss 0.164911, acc 0.90625\n",
            "2019-04-11T10:11:20.199782: step 1639, loss 0.369722, acc 0.875\n",
            "2019-04-11T10:11:20.426363: step 1640, loss 0.106191, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:22.678148: step 1640, loss 0.152958, acc 0.956\n",
            "\n",
            "2019-04-11T10:11:22.896894: step 1641, loss 0.236263, acc 0.90625\n",
            "2019-04-11T10:11:23.121510: step 1642, loss 0.132926, acc 0.96875\n",
            "2019-04-11T10:11:23.343189: step 1643, loss 0.080445, acc 1\n",
            "2019-04-11T10:11:23.560889: step 1644, loss 0.444852, acc 0.84375\n",
            "2019-04-11T10:11:23.780332: step 1645, loss 0.0638214, acc 1\n",
            "2019-04-11T10:11:24.004572: step 1646, loss 0.0670374, acc 1\n",
            "2019-04-11T10:11:24.229177: step 1647, loss 0.303975, acc 0.90625\n",
            "2019-04-11T10:11:24.446805: step 1648, loss 0.339799, acc 0.90625\n",
            "2019-04-11T10:11:24.669286: step 1649, loss 0.0871935, acc 0.96875\n",
            "2019-04-11T10:11:24.891950: step 1650, loss 0.227269, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:27.144402: step 1650, loss 0.167161, acc 0.9494\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1650\n",
            "\n",
            "2019-04-11T10:11:27.449303: step 1651, loss 0.310769, acc 0.875\n",
            "2019-04-11T10:11:27.675062: step 1652, loss 0.0916631, acc 0.9375\n",
            "2019-04-11T10:11:27.897647: step 1653, loss 0.067646, acc 0.96875\n",
            "2019-04-11T10:11:28.122462: step 1654, loss 0.12026, acc 0.9375\n",
            "2019-04-11T10:11:28.347970: step 1655, loss 0.0775173, acc 0.96875\n",
            "2019-04-11T10:11:28.571189: step 1656, loss 0.035127, acc 1\n",
            "2019-04-11T10:11:28.796940: step 1657, loss 0.254051, acc 0.96875\n",
            "2019-04-11T10:11:29.021312: step 1658, loss 0.130681, acc 0.96875\n",
            "2019-04-11T10:11:29.245669: step 1659, loss 0.284931, acc 0.9375\n",
            "2019-04-11T10:11:29.475470: step 1660, loss 0.117011, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:31.712876: step 1660, loss 0.145612, acc 0.9584\n",
            "\n",
            "2019-04-11T10:11:31.936529: step 1661, loss 0.171478, acc 0.96875\n",
            "2019-04-11T10:11:32.160399: step 1662, loss 0.164086, acc 0.9375\n",
            "2019-04-11T10:11:32.386007: step 1663, loss 0.135995, acc 0.96875\n",
            "2019-04-11T10:11:32.610260: step 1664, loss 0.217869, acc 0.90625\n",
            "2019-04-11T10:11:32.838976: step 1665, loss 0.0801578, acc 1\n",
            "2019-04-11T10:11:33.064232: step 1666, loss 0.241455, acc 0.875\n",
            "2019-04-11T10:11:33.290566: step 1667, loss 0.436627, acc 0.84375\n",
            "2019-04-11T10:11:33.510136: step 1668, loss 0.0803529, acc 1\n",
            "2019-04-11T10:11:33.736644: step 1669, loss 0.262654, acc 0.96875\n",
            "2019-04-11T10:11:33.954220: step 1670, loss 0.095306, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:36.189553: step 1670, loss 0.158156, acc 0.9536\n",
            "\n",
            "2019-04-11T10:11:36.412373: step 1671, loss 0.202959, acc 0.96875\n",
            "2019-04-11T10:11:36.636636: step 1672, loss 0.362987, acc 0.90625\n",
            "2019-04-11T10:11:36.862342: step 1673, loss 0.113476, acc 0.96875\n",
            "2019-04-11T10:11:37.087426: step 1674, loss 0.289227, acc 0.9375\n",
            "2019-04-11T10:11:37.303557: step 1675, loss 0.0861052, acc 1\n",
            "2019-04-11T10:11:37.517694: step 1676, loss 0.131719, acc 0.96875\n",
            "2019-04-11T10:11:37.741530: step 1677, loss 0.058338, acc 1\n",
            "2019-04-11T10:11:37.962140: step 1678, loss 0.0945312, acc 0.96875\n",
            "2019-04-11T10:11:38.187453: step 1679, loss 0.0798848, acc 0.96875\n",
            "2019-04-11T10:11:38.417438: step 1680, loss 0.16995, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:40.658875: step 1680, loss 0.134981, acc 0.962\n",
            "\n",
            "2019-04-11T10:11:40.884865: step 1681, loss 0.141626, acc 0.96875\n",
            "2019-04-11T10:11:41.106005: step 1682, loss 0.167464, acc 0.9375\n",
            "2019-04-11T10:11:41.326204: step 1683, loss 0.0299724, acc 1\n",
            "2019-04-11T10:11:41.553004: step 1684, loss 0.226827, acc 0.90625\n",
            "2019-04-11T10:11:41.774129: step 1685, loss 0.215202, acc 0.9375\n",
            "2019-04-11T10:11:41.999833: step 1686, loss 0.0448539, acc 1\n",
            "2019-04-11T10:11:42.224320: step 1687, loss 0.0624744, acc 1\n",
            "2019-04-11T10:11:42.442536: step 1688, loss 0.127887, acc 0.96875\n",
            "2019-04-11T10:11:42.667952: step 1689, loss 0.0376225, acc 1\n",
            "2019-04-11T10:11:42.894368: step 1690, loss 0.0690461, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:45.145224: step 1690, loss 0.145674, acc 0.9588\n",
            "\n",
            "2019-04-11T10:11:45.368197: step 1691, loss 0.102089, acc 1\n",
            "2019-04-11T10:11:45.585943: step 1692, loss 0.0746597, acc 1\n",
            "2019-04-11T10:11:45.811106: step 1693, loss 0.161345, acc 0.9375\n",
            "2019-04-11T10:11:46.034976: step 1694, loss 0.237162, acc 0.9375\n",
            "2019-04-11T10:11:46.255899: step 1695, loss 0.152251, acc 0.90625\n",
            "2019-04-11T10:11:46.480537: step 1696, loss 0.0973871, acc 0.96875\n",
            "2019-04-11T10:11:46.708071: step 1697, loss 0.25099, acc 0.90625\n",
            "2019-04-11T10:11:46.930959: step 1698, loss 0.0686892, acc 0.96875\n",
            "2019-04-11T10:11:47.145153: step 1699, loss 0.0831399, acc 0.96875\n",
            "2019-04-11T10:11:47.367322: step 1700, loss 0.0608682, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:49.605713: step 1700, loss 0.152276, acc 0.9552\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1700\n",
            "\n",
            "2019-04-11T10:11:49.920487: step 1701, loss 0.126294, acc 0.96875\n",
            "2019-04-11T10:11:50.140650: step 1702, loss 0.110135, acc 0.96875\n",
            "2019-04-11T10:11:50.361778: step 1703, loss 0.458566, acc 0.90625\n",
            "2019-04-11T10:11:50.586202: step 1704, loss 0.0512226, acc 1\n",
            "2019-04-11T10:11:50.812369: step 1705, loss 0.123115, acc 0.96875\n",
            "2019-04-11T10:11:51.034185: step 1706, loss 0.134353, acc 0.9375\n",
            "2019-04-11T10:11:51.255060: step 1707, loss 0.140232, acc 0.9375\n",
            "2019-04-11T10:11:51.477543: step 1708, loss 0.063695, acc 1\n",
            "2019-04-11T10:11:51.689738: step 1709, loss 0.177612, acc 0.9375\n",
            "2019-04-11T10:11:51.912049: step 1710, loss 0.0616556, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:54.181521: step 1710, loss 0.141968, acc 0.9592\n",
            "\n",
            "2019-04-11T10:11:54.414962: step 1711, loss 0.183875, acc 0.96875\n",
            "2019-04-11T10:11:54.627091: step 1712, loss 0.639708, acc 0.8125\n",
            "2019-04-11T10:11:54.846566: step 1713, loss 0.112994, acc 0.9375\n",
            "2019-04-11T10:11:55.076747: step 1714, loss 0.122103, acc 0.9375\n",
            "2019-04-11T10:11:55.300020: step 1715, loss 0.452226, acc 0.84375\n",
            "2019-04-11T10:11:55.526535: step 1716, loss 0.29851, acc 0.875\n",
            "2019-04-11T10:11:55.749392: step 1717, loss 0.118078, acc 0.96875\n",
            "2019-04-11T10:11:55.975479: step 1718, loss 0.0437652, acc 1\n",
            "2019-04-11T10:11:56.274655: step 1719, loss 0.321181, acc 0.9375\n",
            "2019-04-11T10:11:56.496986: step 1720, loss 0.0745595, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:11:58.752740: step 1720, loss 0.14521, acc 0.9578\n",
            "\n",
            "2019-04-11T10:11:58.976277: step 1721, loss 0.0694672, acc 0.96875\n",
            "2019-04-11T10:11:59.198940: step 1722, loss 0.11164, acc 0.9375\n",
            "2019-04-11T10:11:59.423497: step 1723, loss 0.0715879, acc 0.96875\n",
            "2019-04-11T10:11:59.636926: step 1724, loss 0.172509, acc 0.96875\n",
            "2019-04-11T10:11:59.860168: step 1725, loss 0.138881, acc 0.9375\n",
            "2019-04-11T10:12:00.082924: step 1726, loss 0.304973, acc 0.90625\n",
            "2019-04-11T10:12:00.306093: step 1727, loss 0.0378593, acc 1\n",
            "2019-04-11T10:12:00.528456: step 1728, loss 0.0608008, acc 1\n",
            "2019-04-11T10:12:00.747259: step 1729, loss 0.073422, acc 1\n",
            "2019-04-11T10:12:00.971292: step 1730, loss 0.0288931, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:03.221481: step 1730, loss 0.137159, acc 0.9646\n",
            "\n",
            "2019-04-11T10:12:03.443387: step 1731, loss 0.0345389, acc 1\n",
            "2019-04-11T10:12:03.664386: step 1732, loss 0.127986, acc 0.96875\n",
            "2019-04-11T10:12:03.885678: step 1733, loss 0.0437066, acc 1\n",
            "2019-04-11T10:12:04.108973: step 1734, loss 0.163535, acc 0.96875\n",
            "2019-04-11T10:12:04.330856: step 1735, loss 0.11872, acc 0.96875\n",
            "2019-04-11T10:12:04.546883: step 1736, loss 0.140802, acc 0.9375\n",
            "2019-04-11T10:12:04.770574: step 1737, loss 0.126341, acc 0.96875\n",
            "2019-04-11T10:12:04.996088: step 1738, loss 0.100206, acc 0.96875\n",
            "2019-04-11T10:12:05.221744: step 1739, loss 0.11035, acc 0.96875\n",
            "2019-04-11T10:12:05.446950: step 1740, loss 0.0719183, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:07.693286: step 1740, loss 0.134019, acc 0.9638\n",
            "\n",
            "2019-04-11T10:12:07.911787: step 1741, loss 0.198409, acc 0.96875\n",
            "2019-04-11T10:12:08.133898: step 1742, loss 0.0604147, acc 0.96875\n",
            "2019-04-11T10:12:08.354587: step 1743, loss 0.180198, acc 0.9375\n",
            "2019-04-11T10:12:08.577904: step 1744, loss 0.104159, acc 0.9375\n",
            "2019-04-11T10:12:08.798975: step 1745, loss 0.220759, acc 0.9375\n",
            "2019-04-11T10:12:09.021934: step 1746, loss 0.0980436, acc 0.9375\n",
            "2019-04-11T10:12:09.245870: step 1747, loss 0.106655, acc 0.96875\n",
            "2019-04-11T10:12:09.466986: step 1748, loss 0.102125, acc 0.96875\n",
            "2019-04-11T10:12:09.689947: step 1749, loss 0.0884794, acc 0.96875\n",
            "2019-04-11T10:12:09.917422: step 1750, loss 0.130005, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:12.166416: step 1750, loss 0.138198, acc 0.9612\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1750\n",
            "\n",
            "2019-04-11T10:12:12.477969: step 1751, loss 0.331041, acc 0.96875\n",
            "2019-04-11T10:12:12.702438: step 1752, loss 0.0823776, acc 0.96875\n",
            "2019-04-11T10:12:12.926643: step 1753, loss 0.19657, acc 0.96875\n",
            "2019-04-11T10:12:13.142168: step 1754, loss 0.0345487, acc 1\n",
            "2019-04-11T10:12:13.365510: step 1755, loss 0.167151, acc 0.9375\n",
            "2019-04-11T10:12:13.587871: step 1756, loss 0.10717, acc 1\n",
            "2019-04-11T10:12:13.807835: step 1757, loss 0.251805, acc 0.90625\n",
            "2019-04-11T10:12:14.022217: step 1758, loss 0.154993, acc 0.9375\n",
            "2019-04-11T10:12:14.241917: step 1759, loss 0.145816, acc 0.96875\n",
            "2019-04-11T10:12:14.464550: step 1760, loss 0.108043, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:16.714284: step 1760, loss 0.132424, acc 0.9638\n",
            "\n",
            "2019-04-11T10:12:16.940912: step 1761, loss 0.19851, acc 0.9375\n",
            "2019-04-11T10:12:17.156172: step 1762, loss 0.126644, acc 0.96875\n",
            "2019-04-11T10:12:17.380168: step 1763, loss 0.408327, acc 0.9375\n",
            "2019-04-11T10:12:17.629656: step 1764, loss 0.0824329, acc 0.96875\n",
            "2019-04-11T10:12:17.852154: step 1765, loss 0.212453, acc 0.9375\n",
            "2019-04-11T10:12:18.071389: step 1766, loss 0.164973, acc 0.875\n",
            "2019-04-11T10:12:18.294251: step 1767, loss 0.207136, acc 0.90625\n",
            "2019-04-11T10:12:18.511357: step 1768, loss 0.256259, acc 0.90625\n",
            "2019-04-11T10:12:18.733577: step 1769, loss 0.0346601, acc 1\n",
            "2019-04-11T10:12:18.954055: step 1770, loss 0.25476, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:21.190844: step 1770, loss 0.137254, acc 0.9638\n",
            "\n",
            "2019-04-11T10:12:21.411900: step 1771, loss 0.123116, acc 0.96875\n",
            "2019-04-11T10:12:21.633720: step 1772, loss 0.103629, acc 0.96875\n",
            "2019-04-11T10:12:21.853967: step 1773, loss 0.0451543, acc 1\n",
            "2019-04-11T10:12:22.078050: step 1774, loss 0.0909253, acc 0.96875\n",
            "2019-04-11T10:12:22.297644: step 1775, loss 0.0680275, acc 1\n",
            "2019-04-11T10:12:22.518610: step 1776, loss 0.276147, acc 0.9375\n",
            "2019-04-11T10:12:22.733987: step 1777, loss 0.0943659, acc 0.96875\n",
            "2019-04-11T10:12:22.960402: step 1778, loss 0.0576398, acc 1\n",
            "2019-04-11T10:12:23.177288: step 1779, loss 0.281676, acc 0.90625\n",
            "2019-04-11T10:12:23.399886: step 1780, loss 0.327507, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:25.641825: step 1780, loss 0.156801, acc 0.9578\n",
            "\n",
            "2019-04-11T10:12:25.858793: step 1781, loss 0.169612, acc 0.96875\n",
            "2019-04-11T10:12:26.076856: step 1782, loss 0.14968, acc 0.9375\n",
            "2019-04-11T10:12:26.298990: step 1783, loss 0.167772, acc 0.96875\n",
            "2019-04-11T10:12:26.517650: step 1784, loss 0.127701, acc 0.96875\n",
            "2019-04-11T10:12:26.745285: step 1785, loss 0.100766, acc 0.96875\n",
            "2019-04-11T10:12:26.972011: step 1786, loss 0.0497676, acc 1\n",
            "2019-04-11T10:12:27.196634: step 1787, loss 0.0988325, acc 0.96875\n",
            "2019-04-11T10:12:27.414926: step 1788, loss 0.0267776, acc 1\n",
            "2019-04-11T10:12:27.633930: step 1789, loss 0.0748372, acc 1\n",
            "2019-04-11T10:12:27.853291: step 1790, loss 0.140642, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:30.102226: step 1790, loss 0.168758, acc 0.9462\n",
            "\n",
            "2019-04-11T10:12:30.323070: step 1791, loss 0.0634602, acc 1\n",
            "2019-04-11T10:12:30.542230: step 1792, loss 0.0436579, acc 1\n",
            "2019-04-11T10:12:30.766705: step 1793, loss 0.415446, acc 0.875\n",
            "2019-04-11T10:12:30.984874: step 1794, loss 0.209529, acc 0.90625\n",
            "2019-04-11T10:12:31.205161: step 1795, loss 0.17816, acc 0.9375\n",
            "2019-04-11T10:12:31.424260: step 1796, loss 0.262562, acc 0.90625\n",
            "2019-04-11T10:12:31.645114: step 1797, loss 0.409108, acc 0.90625\n",
            "2019-04-11T10:12:31.861817: step 1798, loss 0.14396, acc 0.90625\n",
            "2019-04-11T10:12:32.084257: step 1799, loss 0.171418, acc 0.96875\n",
            "2019-04-11T10:12:32.302647: step 1800, loss 0.0403097, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:34.549484: step 1800, loss 0.136118, acc 0.9638\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1800\n",
            "\n",
            "2019-04-11T10:12:34.870826: step 1801, loss 0.0612908, acc 0.96875\n",
            "2019-04-11T10:12:35.093312: step 1802, loss 0.234753, acc 0.875\n",
            "2019-04-11T10:12:35.312522: step 1803, loss 0.114925, acc 0.90625\n",
            "2019-04-11T10:12:35.533492: step 1804, loss 0.301719, acc 0.875\n",
            "2019-04-11T10:12:35.757694: step 1805, loss 0.257383, acc 0.9375\n",
            "2019-04-11T10:12:35.986068: step 1806, loss 0.228827, acc 0.9375\n",
            "2019-04-11T10:12:36.208867: step 1807, loss 0.118066, acc 0.96875\n",
            "2019-04-11T10:12:36.433192: step 1808, loss 0.0547388, acc 0.96875\n",
            "2019-04-11T10:12:36.658910: step 1809, loss 0.213677, acc 0.96875\n",
            "2019-04-11T10:12:36.881619: step 1810, loss 0.271291, acc 0.84375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:39.155225: step 1810, loss 0.146763, acc 0.954\n",
            "\n",
            "2019-04-11T10:12:39.372750: step 1811, loss 0.117544, acc 1\n",
            "2019-04-11T10:12:39.593147: step 1812, loss 0.0980247, acc 0.96875\n",
            "2019-04-11T10:12:39.815798: step 1813, loss 0.209969, acc 0.9375\n",
            "2019-04-11T10:12:40.036656: step 1814, loss 0.0952859, acc 0.96875\n",
            "2019-04-11T10:12:40.258092: step 1815, loss 0.141649, acc 0.96875\n",
            "2019-04-11T10:12:40.474601: step 1816, loss 0.189993, acc 0.9375\n",
            "2019-04-11T10:12:40.698047: step 1817, loss 0.415756, acc 0.9375\n",
            "2019-04-11T10:12:40.925249: step 1818, loss 0.0411147, acc 1\n",
            "2019-04-11T10:12:41.150134: step 1819, loss 0.0403396, acc 1\n",
            "2019-04-11T10:12:41.364512: step 1820, loss 0.25994, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:43.607750: step 1820, loss 0.139697, acc 0.9642\n",
            "\n",
            "2019-04-11T10:12:43.821144: step 1821, loss 0.0937039, acc 0.96875\n",
            "2019-04-11T10:12:44.040109: step 1822, loss 0.124205, acc 0.96875\n",
            "2019-04-11T10:12:44.262859: step 1823, loss 0.201775, acc 0.90625\n",
            "2019-04-11T10:12:44.490068: step 1824, loss 0.0732374, acc 0.96875\n",
            "2019-04-11T10:12:44.714243: step 1825, loss 0.0455078, acc 0.96875\n",
            "2019-04-11T10:12:44.932714: step 1826, loss 0.183259, acc 0.96875\n",
            "2019-04-11T10:12:45.148504: step 1827, loss 0.0406566, acc 1\n",
            "2019-04-11T10:12:45.369338: step 1828, loss 0.514354, acc 0.875\n",
            "2019-04-11T10:12:45.596625: step 1829, loss 0.178376, acc 0.9375\n",
            "2019-04-11T10:12:45.813146: step 1830, loss 0.0614758, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:48.074597: step 1830, loss 0.134922, acc 0.9644\n",
            "\n",
            "2019-04-11T10:12:48.289871: step 1831, loss 0.0179485, acc 1\n",
            "2019-04-11T10:12:48.507930: step 1832, loss 0.0418857, acc 1\n",
            "2019-04-11T10:12:48.725817: step 1833, loss 0.149047, acc 0.9375\n",
            "2019-04-11T10:12:48.947825: step 1834, loss 0.0510708, acc 1\n",
            "2019-04-11T10:12:49.165323: step 1835, loss 0.03847, acc 1\n",
            "2019-04-11T10:12:49.385698: step 1836, loss 0.184573, acc 0.96875\n",
            "2019-04-11T10:12:49.604703: step 1837, loss 0.234182, acc 0.90625\n",
            "2019-04-11T10:12:49.826904: step 1838, loss 0.155745, acc 0.96875\n",
            "2019-04-11T10:12:50.046233: step 1839, loss 0.348314, acc 0.90625\n",
            "2019-04-11T10:12:50.268219: step 1840, loss 0.308144, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:52.515150: step 1840, loss 0.14156, acc 0.9584\n",
            "\n",
            "2019-04-11T10:12:52.738210: step 1841, loss 0.0829829, acc 1\n",
            "2019-04-11T10:12:52.959066: step 1842, loss 0.018589, acc 1\n",
            "2019-04-11T10:12:53.176383: step 1843, loss 0.108032, acc 0.96875\n",
            "2019-04-11T10:12:53.392102: step 1844, loss 0.213578, acc 0.9375\n",
            "2019-04-11T10:12:53.610785: step 1845, loss 0.0337707, acc 1\n",
            "2019-04-11T10:12:53.839233: step 1846, loss 0.286258, acc 0.96875\n",
            "2019-04-11T10:12:54.061085: step 1847, loss 0.272558, acc 0.90625\n",
            "2019-04-11T10:12:54.284786: step 1848, loss 0.0572301, acc 0.96875\n",
            "2019-04-11T10:12:54.503874: step 1849, loss 0.373849, acc 0.875\n",
            "2019-04-11T10:12:54.717924: step 1850, loss 0.160198, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:12:56.964394: step 1850, loss 0.128261, acc 0.9648\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1850\n",
            "\n",
            "2019-04-11T10:12:57.282132: step 1851, loss 0.0873257, acc 0.96875\n",
            "2019-04-11T10:12:57.502244: step 1852, loss 0.219367, acc 0.90625\n",
            "2019-04-11T10:12:57.722931: step 1853, loss 0.127743, acc 0.9375\n",
            "2019-04-11T10:12:57.942976: step 1854, loss 0.0477362, acc 1\n",
            "2019-04-11T10:12:58.157590: step 1855, loss 0.0564914, acc 1\n",
            "2019-04-11T10:12:58.377414: step 1856, loss 0.118279, acc 0.96875\n",
            "2019-04-11T10:12:58.598462: step 1857, loss 0.021892, acc 1\n",
            "2019-04-11T10:12:58.832697: step 1858, loss 0.0362063, acc 1\n",
            "2019-04-11T10:12:59.054448: step 1859, loss 0.321394, acc 0.90625\n",
            "2019-04-11T10:12:59.279415: step 1860, loss 0.151978, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:01.529908: step 1860, loss 0.13814, acc 0.961\n",
            "\n",
            "2019-04-11T10:13:01.754346: step 1861, loss 0.0887991, acc 0.96875\n",
            "2019-04-11T10:13:01.976577: step 1862, loss 0.062794, acc 0.96875\n",
            "2019-04-11T10:13:02.201672: step 1863, loss 0.110482, acc 0.96875\n",
            "2019-04-11T10:13:02.416831: step 1864, loss 0.327064, acc 0.90625\n",
            "2019-04-11T10:13:02.643808: step 1865, loss 0.0307369, acc 1\n",
            "2019-04-11T10:13:02.869235: step 1866, loss 0.108445, acc 0.96875\n",
            "2019-04-11T10:13:03.095260: step 1867, loss 0.0493153, acc 1\n",
            "2019-04-11T10:13:03.317260: step 1868, loss 0.311191, acc 0.875\n",
            "2019-04-11T10:13:03.535694: step 1869, loss 0.199138, acc 0.90625\n",
            "2019-04-11T10:13:03.759238: step 1870, loss 0.184267, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:06.000472: step 1870, loss 0.13046, acc 0.9614\n",
            "\n",
            "2019-04-11T10:13:06.224994: step 1871, loss 0.223363, acc 0.90625\n",
            "2019-04-11T10:13:06.448110: step 1872, loss 0.387589, acc 0.90625\n",
            "2019-04-11T10:13:06.671462: step 1873, loss 0.0707816, acc 1\n",
            "2019-04-11T10:13:06.896469: step 1874, loss 0.0992899, acc 0.9375\n",
            "2019-04-11T10:13:07.116162: step 1875, loss 0.0917963, acc 0.96875\n",
            "2019-04-11T10:13:07.331914: step 1876, loss 0.226137, acc 0.90625\n",
            "2019-04-11T10:13:07.555510: step 1877, loss 0.236796, acc 0.9375\n",
            "2019-04-11T10:13:07.777724: step 1878, loss 0.255532, acc 0.875\n",
            "2019-04-11T10:13:08.002101: step 1879, loss 0.234542, acc 0.9375\n",
            "2019-04-11T10:13:08.225073: step 1880, loss 0.0873562, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:10.466462: step 1880, loss 0.133837, acc 0.9618\n",
            "\n",
            "2019-04-11T10:13:10.704237: step 1881, loss 0.136047, acc 0.9375\n",
            "2019-04-11T10:13:10.929202: step 1882, loss 0.0714665, acc 0.96875\n",
            "2019-04-11T10:13:11.142475: step 1883, loss 0.0155139, acc 1\n",
            "2019-04-11T10:13:11.366908: step 1884, loss 0.0882733, acc 0.96875\n",
            "2019-04-11T10:13:11.588414: step 1885, loss 0.131801, acc 0.9375\n",
            "2019-04-11T10:13:11.812279: step 1886, loss 0.122252, acc 0.96875\n",
            "2019-04-11T10:13:12.036091: step 1887, loss 0.123698, acc 0.96875\n",
            "2019-04-11T10:13:12.257322: step 1888, loss 0.164729, acc 0.96875\n",
            "2019-04-11T10:13:12.481058: step 1889, loss 0.341934, acc 0.875\n",
            "2019-04-11T10:13:12.697925: step 1890, loss 0.027717, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:14.948235: step 1890, loss 0.134439, acc 0.9624\n",
            "\n",
            "2019-04-11T10:13:15.172129: step 1891, loss 0.0405817, acc 1\n",
            "2019-04-11T10:13:15.390799: step 1892, loss 0.0706277, acc 1\n",
            "2019-04-11T10:13:15.613258: step 1893, loss 0.0385828, acc 1\n",
            "2019-04-11T10:13:15.837969: step 1894, loss 0.0709772, acc 1\n",
            "2019-04-11T10:13:16.058356: step 1895, loss 0.0714492, acc 1\n",
            "2019-04-11T10:13:16.274558: step 1896, loss 0.244447, acc 0.9375\n",
            "2019-04-11T10:13:16.505881: step 1897, loss 0.470261, acc 0.875\n",
            "2019-04-11T10:13:16.726301: step 1898, loss 0.227271, acc 0.96875\n",
            "2019-04-11T10:13:16.940782: step 1899, loss 0.0409622, acc 1\n",
            "2019-04-11T10:13:17.163365: step 1900, loss 0.0635032, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:19.431946: step 1900, loss 0.135329, acc 0.9618\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1900\n",
            "\n",
            "2019-04-11T10:13:19.745053: step 1901, loss 0.0670821, acc 1\n",
            "2019-04-11T10:13:19.967665: step 1902, loss 0.126679, acc 0.96875\n",
            "2019-04-11T10:13:20.190287: step 1903, loss 0.28163, acc 0.90625\n",
            "2019-04-11T10:13:20.414268: step 1904, loss 0.0793028, acc 1\n",
            "2019-04-11T10:13:20.631663: step 1905, loss 0.140013, acc 0.9375\n",
            "2019-04-11T10:13:20.849551: step 1906, loss 0.0774027, acc 0.96875\n",
            "2019-04-11T10:13:21.069171: step 1907, loss 0.0243279, acc 1\n",
            "2019-04-11T10:13:21.295725: step 1908, loss 0.0889907, acc 0.96875\n",
            "2019-04-11T10:13:21.523085: step 1909, loss 0.108993, acc 0.90625\n",
            "2019-04-11T10:13:21.745640: step 1910, loss 0.0515737, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:23.992174: step 1910, loss 0.120738, acc 0.9678\n",
            "\n",
            "2019-04-11T10:13:24.215389: step 1911, loss 0.115741, acc 0.9375\n",
            "2019-04-11T10:13:24.435141: step 1912, loss 0.0228671, acc 1\n",
            "2019-04-11T10:13:24.651148: step 1913, loss 0.0858813, acc 1\n",
            "2019-04-11T10:13:24.874898: step 1914, loss 0.0345819, acc 1\n",
            "2019-04-11T10:13:25.093107: step 1915, loss 0.101628, acc 0.9375\n",
            "2019-04-11T10:13:25.307684: step 1916, loss 0.131763, acc 0.9375\n",
            "2019-04-11T10:13:25.529344: step 1917, loss 0.173182, acc 0.9375\n",
            "2019-04-11T10:13:25.754438: step 1918, loss 0.0859625, acc 0.96875\n",
            "2019-04-11T10:13:25.976786: step 1919, loss 0.0714016, acc 1\n",
            "2019-04-11T10:13:26.203681: step 1920, loss 0.302948, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:28.441899: step 1920, loss 0.128652, acc 0.9634\n",
            "\n",
            "2019-04-11T10:13:28.665089: step 1921, loss 0.0332141, acc 1\n",
            "2019-04-11T10:13:28.883961: step 1922, loss 0.241573, acc 0.9375\n",
            "2019-04-11T10:13:29.102392: step 1923, loss 0.188083, acc 0.90625\n",
            "2019-04-11T10:13:29.324942: step 1924, loss 0.0714582, acc 1\n",
            "2019-04-11T10:13:29.547728: step 1925, loss 0.107233, acc 0.96875\n",
            "2019-04-11T10:13:29.765647: step 1926, loss 0.335261, acc 0.875\n",
            "2019-04-11T10:13:29.988416: step 1927, loss 0.260453, acc 0.90625\n",
            "2019-04-11T10:13:30.214414: step 1928, loss 0.183161, acc 0.9375\n",
            "2019-04-11T10:13:30.435716: step 1929, loss 0.223497, acc 0.875\n",
            "2019-04-11T10:13:30.656703: step 1930, loss 0.0428287, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:32.900845: step 1930, loss 0.122333, acc 0.967\n",
            "\n",
            "2019-04-11T10:13:33.125982: step 1931, loss 0.120493, acc 0.9375\n",
            "2019-04-11T10:13:33.339429: step 1932, loss 0.108875, acc 0.96875\n",
            "2019-04-11T10:13:33.564201: step 1933, loss 0.254793, acc 0.90625\n",
            "2019-04-11T10:13:33.789634: step 1934, loss 0.0893597, acc 0.96875\n",
            "2019-04-11T10:13:34.007791: step 1935, loss 0.0675703, acc 1\n",
            "2019-04-11T10:13:34.229859: step 1936, loss 0.0440448, acc 1\n",
            "2019-04-11T10:13:34.450802: step 1937, loss 0.111803, acc 0.96875\n",
            "2019-04-11T10:13:34.676942: step 1938, loss 0.123474, acc 0.9375\n",
            "2019-04-11T10:13:34.904142: step 1939, loss 0.238043, acc 0.96875\n",
            "2019-04-11T10:13:35.127210: step 1940, loss 0.23964, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:37.374305: step 1940, loss 0.12849, acc 0.9652\n",
            "\n",
            "2019-04-11T10:13:37.598309: step 1941, loss 0.133441, acc 0.96875\n",
            "2019-04-11T10:13:37.814385: step 1942, loss 0.123256, acc 0.9375\n",
            "2019-04-11T10:13:38.033849: step 1943, loss 0.139247, acc 0.9375\n",
            "2019-04-11T10:13:38.256466: step 1944, loss 0.110705, acc 0.96875\n",
            "2019-04-11T10:13:38.479130: step 1945, loss 0.154093, acc 0.96875\n",
            "2019-04-11T10:13:38.698617: step 1946, loss 0.0243836, acc 1\n",
            "2019-04-11T10:13:38.924502: step 1947, loss 0.117241, acc 0.96875\n",
            "2019-04-11T10:13:39.141845: step 1948, loss 0.0379938, acc 1\n",
            "2019-04-11T10:13:39.365930: step 1949, loss 0.279697, acc 0.90625\n",
            "2019-04-11T10:13:39.585176: step 1950, loss 0.0894724, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:41.839001: step 1950, loss 0.121164, acc 0.968\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-1950\n",
            "\n",
            "2019-04-11T10:13:42.143664: step 1951, loss 0.0297813, acc 1\n",
            "2019-04-11T10:13:42.367198: step 1952, loss 0.0730265, acc 0.96875\n",
            "2019-04-11T10:13:42.585556: step 1953, loss 0.0787574, acc 1\n",
            "2019-04-11T10:13:42.809935: step 1954, loss 0.115356, acc 0.9375\n",
            "2019-04-11T10:13:43.031531: step 1955, loss 0.126383, acc 0.96875\n",
            "2019-04-11T10:13:43.256030: step 1956, loss 0.293405, acc 0.9375\n",
            "2019-04-11T10:13:43.470086: step 1957, loss 0.0461467, acc 1\n",
            "2019-04-11T10:13:43.694216: step 1958, loss 0.230172, acc 0.90625\n",
            "2019-04-11T10:13:43.916607: step 1959, loss 0.0708233, acc 1\n",
            "2019-04-11T10:13:44.130855: step 1960, loss 0.0967782, acc 1\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:46.372123: step 1960, loss 0.128322, acc 0.9654\n",
            "\n",
            "2019-04-11T10:13:46.602806: step 1961, loss 0.0673007, acc 0.96875\n",
            "2019-04-11T10:13:46.839411: step 1962, loss 0.256264, acc 0.9375\n",
            "2019-04-11T10:13:47.063960: step 1963, loss 0.420897, acc 0.875\n",
            "2019-04-11T10:13:47.282217: step 1964, loss 0.0317675, acc 1\n",
            "2019-04-11T10:13:47.499789: step 1965, loss 0.0485967, acc 1\n",
            "2019-04-11T10:13:47.716499: step 1966, loss 0.319667, acc 0.90625\n",
            "2019-04-11T10:13:47.942721: step 1967, loss 0.0814248, acc 0.96875\n",
            "2019-04-11T10:13:48.163304: step 1968, loss 0.135918, acc 0.9375\n",
            "2019-04-11T10:13:48.388339: step 1969, loss 0.115718, acc 0.96875\n",
            "2019-04-11T10:13:48.610948: step 1970, loss 0.201352, acc 0.96875\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:50.856470: step 1970, loss 0.122768, acc 0.9674\n",
            "\n",
            "2019-04-11T10:13:51.082093: step 1971, loss 0.227797, acc 0.96875\n",
            "2019-04-11T10:13:51.306852: step 1972, loss 0.0959725, acc 0.96875\n",
            "2019-04-11T10:13:51.529677: step 1973, loss 0.209497, acc 0.90625\n",
            "2019-04-11T10:13:51.749921: step 1974, loss 0.443355, acc 0.9375\n",
            "2019-04-11T10:13:51.971154: step 1975, loss 0.113409, acc 0.96875\n",
            "2019-04-11T10:13:52.194591: step 1976, loss 0.106312, acc 0.96875\n",
            "2019-04-11T10:13:52.414397: step 1977, loss 0.087442, acc 0.96875\n",
            "2019-04-11T10:13:52.638718: step 1978, loss 0.0489392, acc 1\n",
            "2019-04-11T10:13:52.860956: step 1979, loss 0.138991, acc 0.9375\n",
            "2019-04-11T10:13:53.083998: step 1980, loss 0.189836, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:55.325941: step 1980, loss 0.138286, acc 0.961\n",
            "\n",
            "2019-04-11T10:13:55.548363: step 1981, loss 0.127932, acc 0.9375\n",
            "2019-04-11T10:13:55.772062: step 1982, loss 0.157562, acc 0.90625\n",
            "2019-04-11T10:13:55.991095: step 1983, loss 0.152, acc 0.90625\n",
            "2019-04-11T10:13:56.211889: step 1984, loss 0.0470895, acc 1\n",
            "2019-04-11T10:13:56.435595: step 1985, loss 0.175878, acc 0.96875\n",
            "2019-04-11T10:13:56.663636: step 1986, loss 0.171828, acc 0.9375\n",
            "2019-04-11T10:13:56.890035: step 1987, loss 0.151966, acc 0.90625\n",
            "2019-04-11T10:13:57.113535: step 1988, loss 0.334437, acc 0.90625\n",
            "2019-04-11T10:13:57.333810: step 1989, loss 0.0565157, acc 0.96875\n",
            "2019-04-11T10:13:57.554796: step 1990, loss 0.212987, acc 0.9375\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:13:59.806092: step 1990, loss 0.1309, acc 0.9646\n",
            "\n",
            "2019-04-11T10:14:00.021146: step 1991, loss 0.179098, acc 0.96875\n",
            "2019-04-11T10:14:00.246635: step 1992, loss 0.205592, acc 0.90625\n",
            "2019-04-11T10:14:00.466470: step 1993, loss 0.315787, acc 0.875\n",
            "2019-04-11T10:14:00.695016: step 1994, loss 0.0947688, acc 0.96875\n",
            "2019-04-11T10:14:00.915946: step 1995, loss 0.149692, acc 0.9375\n",
            "2019-04-11T10:14:01.137752: step 1996, loss 0.373617, acc 0.9375\n",
            "2019-04-11T10:14:01.355217: step 1997, loss 0.126813, acc 0.96875\n",
            "2019-04-11T10:14:01.575790: step 1998, loss 0.153136, acc 0.96875\n",
            "2019-04-11T10:14:01.794958: step 1999, loss 0.0652376, acc 1\n",
            "2019-04-11T10:14:02.017330: step 2000, loss 0.299498, acc 0.90625\n",
            "\n",
            "Evaluation:\n",
            "2019-04-11T10:14:04.261987: step 2000, loss 0.137627, acc 0.9606\n",
            "\n",
            "Saved model checkpoint to /content/runs/1554976753/checkpoints/model-2000\n",
            "\n",
            "test accuracy 0.9621\n",
            "Tensor(\"loss:0\", shape=(), dtype=string)\n",
            "Tensor(\"accuracy:0\", shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IUcR04T5zJa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}